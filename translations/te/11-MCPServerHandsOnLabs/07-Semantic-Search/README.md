<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "772b18b1ca4fb34af68e12eb2f2defda",
  "translation_date": "2025-12-11T14:02:23+00:00",
  "source_file": "11-MCPServerHandsOnLabs/07-Semantic-Search/README.md",
  "language_code": "te"
}
-->
# ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞á‡∞Ç‡∞ü‡∞ø‡∞ó‡±ç‡∞∞‡±á‡∞∑‡∞®‡±ç

## üéØ ‡∞à ‡∞≤‡±ç‡∞Ø‡∞æ‡∞¨‡±ç ‡∞è‡∞Æ‡∞ø ‡∞ï‡∞µ‡∞∞‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø

‡∞à ‡∞≤‡±ç‡∞Ø‡∞æ‡∞¨‡±ç Azure OpenAI ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å PostgreSQL ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï pgvector ‡∞é‡∞ï‡±ç‡∞∏‡±ç‚Äå‡∞ü‡±Ü‡∞®‡±ç‡∞∑‡∞®‡±ç ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞∏‡∞æ‡∞Æ‡∞∞‡±ç‡∞•‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å ‡∞Ö‡∞Æ‡∞≤‡±Å ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∏‡∞Æ‡∞ó‡±ç‡∞∞ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ó‡∞¶‡∞∞‡±ç‡∞∂‡∞ï‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞Ö‡∞Ç‡∞¶‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø. ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞∏‡∞π‡∞ú ‡∞≠‡∞æ‡∞∑‡∞æ ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡∞®‡±Å ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±Å‡∞®‡±á ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡∞æ‡∞∞‡±Ç‡∞™‡±ç‡∞Ø‡∞§ ‡∞Ü‡∞ß‡∞æ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞≤‡∞®‡±Å ‡∞Ö‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡±á AI-‡∞∂‡∞ï‡±ç‡∞§‡∞ø‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞® ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç‚Äå‡∞®‡±Å ‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡∞æ‡∞∞‡±Å.

## ‡∞Ö‡∞µ‡∞≤‡±ã‡∞ï‡∞®‡∞Ç

‡∞∏‡∞æ‡∞Ç‡∞™‡±ç‡∞∞‡∞¶‡∞æ‡∞Ø ‡∞ï‡±Ä‡∞µ‡∞∞‡±ç‡∞°‡±ç-‡∞Ü‡∞ß‡∞æ‡∞∞‡∞ø‡∞§ ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞§‡∞∞‡∞ö‡±Å‡∞ó‡∞æ ‡∞µ‡∞ø‡∞®‡∞ø‡∞Ø‡±ã‡∞ó‡∞¶‡∞æ‡∞∞‡±Å‡∞≤ ‡∞â‡∞¶‡±ç‡∞¶‡±á‡∞∂‡±ç‡∞Ø‡∞Ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞Ö‡∞∞‡±ç‡∞•‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞™‡∞ü‡±ç‡∞ü‡±Å‡∞ï‡±ã‡∞≤‡±á‡∞ï‡∞™‡±ã‡∞§‡±Å‡∞Ç‡∞¶‡∞ø. ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç "‡∞µ‡∞∞‡±ç‡∞∑‡∞æ‡∞ï‡∞æ‡∞≤‡∞Ç‡∞≤‡±ã ‡∞∏‡±å‡∞ï‡∞∞‡±ç‡∞Ø‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞® ‡∞™‡∞∞‡±Å‡∞ó‡±Å‡∞≤ ‡∞∑‡±Ç‡∞∏‡±ç" ‡∞µ‡∞Ç‡∞ü‡∞ø ‡∞∏‡∞π‡∞ú ‡∞≠‡∞æ‡∞∑‡∞æ ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡∞®‡±Å ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞µ‡∞ø‡∞µ‡∞∞‡∞£‡∞≤‡±ç‡∞≤‡±ã ‡∞Ü ‡∞ñ‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞Æ‡±à‡∞® ‡∞™‡∞¶‡∞æ‡∞≤‡±Å ‡∞≤‡±á‡∞ï‡∞™‡±ã‡∞Ø‡∞ø‡∞®‡∞æ ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡±Å‡∞≤‡∞®‡±Å ‡∞ï‡∞®‡±Å‡∞ó‡±ä‡∞®‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞Ö‡∞®‡±Å‡∞Æ‡∞§‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

‡∞Æ‡∞æ ‡∞Ö‡∞Æ‡∞≤‡±Å Azure OpenAI ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞∂‡∞ï‡±ç‡∞§‡∞ø‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞® ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‡∞∏‡±ç‚Äå‡∞®‡±Å PostgreSQL ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï pgvector ‡∞é‡∞ï‡±ç‡∞∏‡±ç‚Äå‡∞ü‡±Ü‡∞®‡±ç‡∞∑‡∞®‡±ç‚Äå‡∞§‡±ã ‡∞ï‡∞≤‡∞ø‡∞™‡∞ø, ‡∞§‡±Ü‡∞≤‡∞ø‡∞µ‡±à‡∞® ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞ï‡∞®‡±Å‡∞ó‡±ä‡∞®‡∞°‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞Æ‡±Ü‡∞∞‡±Å‡∞ó‡±Å‡∞™‡∞∞‡∞ö‡±á ‡∞Ö‡∞ß‡∞ø‡∞ï ‡∞™‡∞®‡∞ø‡∞§‡±Ä‡∞∞‡±Å, ‡∞∏‡±ç‡∞ï‡±á‡∞≤‡∞¨‡±Å‡∞≤‡±ç ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞∏‡∞ø‡∞∏‡±ç‡∞ü‡∞Æ‡±ç‚Äå‡∞®‡±Å ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

## ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±Å‡∞®‡±á ‡∞≤‡∞ï‡±ç‡∞∑‡±ç‡∞Ø‡∞æ‡∞≤‡±Å

‡∞à ‡∞≤‡±ç‡∞Ø‡∞æ‡∞¨‡±ç ‡∞Æ‡±Å‡∞ó‡∞ø‡∞∏‡∞ø‡∞® ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡±Å‡∞ó‡±Å‡∞§‡∞æ‡∞∞‡±Å:

- **‡∞á‡∞Ç‡∞ü‡∞ø‡∞ó‡±ç‡∞∞‡±á‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø** Azure OpenAI ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‡∞∏‡±ç‚Äå‡∞®‡±Å ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç  
- **‡∞Ö‡∞Æ‡∞≤‡±Å ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø** pgvector ‡∞®‡±Å ‡∞∏‡∞Æ‡∞∞‡±ç‡∞•‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞® ‡∞∏‡∞æ‡∞∞‡±Ç‡∞™‡±ç‡∞Ø‡∞§ ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞Ü‡∞™‡∞∞‡±á‡∞∑‡∞®‡±ç‡∞≤ ‡∞ï‡±ã‡∞∏‡∞Ç  
- **‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø** ‡∞∏‡∞π‡∞ú ‡∞≠‡∞æ‡∞∑ ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤ ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞ü‡±Ç‡∞≤‡±ç‡∞∏‡±ç  
- **‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø** ‡∞∏‡∞æ‡∞Ç‡∞™‡±ç‡∞∞‡∞¶‡∞æ‡∞Ø ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞ï‡∞≤‡∞ø‡∞™‡∞ø‡∞® ‡∞π‡±à‡∞¨‡±ç‡∞∞‡∞ø‡∞°‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç  
- **‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø** ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞™‡∞®‡∞ø‡∞§‡±Ä‡∞∞‡±Å ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡∞®‡±Å  
- **‡∞°‡∞ø‡∞ú‡±à‡∞®‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø** ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞∏‡∞æ‡∞∞‡±Ç‡∞™‡±ç‡∞Ø‡∞§ ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞∏‡∞ø‡∞´‡∞æ‡∞∞‡±ç‡∞∏‡±Å ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡±ç‡∞•‡∞≤‡∞®‡±Å  

## üß† ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞Ü‡∞∞‡±ç‡∞ï‡∞ø‡∞ü‡±Ü‡∞ï‡±ç‡∞ö‡∞∞‡±ç

### ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞™‡±à‡∞™‡±ç‚Äå‡∞≤‡±à‡∞®‡±ç

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                User Query                       ‚îÇ
‚îÇ         "comfortable running shoes"            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Azure OpenAI API                     ‚îÇ
‚îÇ        text-embedding-3-small                  ‚îÇ
‚îÇ        Input: Query Text                       ‚îÇ
‚îÇ        Output: 1536-dimensional vector         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              pgvector Search                   ‚îÇ
‚îÇ      Cosine Similarity: embedding <=> vector   ‚îÇ
‚îÇ      WHERE similarity > threshold              ‚îÇ
‚îÇ      ORDER BY similarity DESC                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Ranked Results                      ‚îÇ
‚îÇ    1. Nike Air Zoom (0.89 similarity)         ‚îÇ
‚îÇ    2. Adidas Ultraboost (0.85 similarity)     ‚îÇ
‚îÇ    3. New Balance Fresh Foam (0.82 similarity) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ú‡∞®‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡±ç‡∞Ø‡±Ç‡∞π‡∞Ç

```python
# mcp_server/embeddings/embedding_manager.py
"""
Comprehensive embedding management for semantic search.
"""
import asyncio
import hashlib
import json
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import numpy as np
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential
from azure.core.exceptions import HttpResponseError
import logging

logger = logging.getLogger(__name__)

class EmbeddingManager:
    """Manage text embeddings for semantic search."""
    
    def __init__(self, project_endpoint: str, deployment_name: str = "text-embedding-3-small"):
        self.project_endpoint = project_endpoint
        self.deployment_name = deployment_name
        self.credential = DefaultAzureCredential()
        self.client = None
        
        # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ï‡∞æ‡∞®‡±ç‡∞´‡∞ø‡∞ó‡∞∞‡±á‡∞∑‡∞®‡±ç
        self.embedding_dimension = 1536  # text-embedding-3-small ‡∞ï‡±ä‡∞≤‡∞§
        self.max_tokens = 8000  # ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞Ö‡∞≠‡±ç‡∞Ø‡∞∞‡±ç‡∞•‡∞®‡∞ï‡±Å ‡∞ó‡∞∞‡∞ø‡∞∑‡±ç‡∞ü ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±ç‡∞≤‡±Å
        self.batch_size = 100  # ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡∞ø‡∞Ç‡∞ó‡±ç ‡∞™‡∞∞‡∞ø‡∞Æ‡∞æ‡∞£‡∞Ç
        
        # ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ï‡∞æ‡∞®‡±ç‡∞´‡∞ø‡∞ó‡∞∞‡±á‡∞∑‡∞®‡±ç
        self.embedding_cache = {}
        self.cache_ttl = timedelta(hours=24)
        
        # ‡∞∞‡±á‡∞ü‡±ç ‡∞™‡∞∞‡∞ø‡∞Æ‡∞ø‡∞§‡∞ø
        self.rate_limit_requests = 1000  # ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞®‡∞ø‡∞Æ‡∞ø‡∞∑‡∞Ç
        self.rate_limit_tokens = 150000  # ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞®‡∞ø‡∞Æ‡∞ø‡∞∑‡∞Ç
        
    async def initialize(self):
        """Initialize the Azure AI client."""
        
        try:
            self.client = AIProjectClient(
                endpoint=self.project_endpoint,
                credential=self.credential
            )
            
            # ‡∞ï‡∞®‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞™‡∞∞‡±Ä‡∞ï‡±ç‡∞∑‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
            await self._test_connection()
            
            logger.info("Embedding manager initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize embedding manager: {e}")
            raise
    
    async def _test_connection(self):
        """Test Azure OpenAI connection."""
        
        try:
            test_embedding = await self.generate_embedding("test connection")
            if len(test_embedding) != self.embedding_dimension:
                raise ValueError(f"Unexpected embedding dimension: {len(test_embedding)}")
            
            logger.info("Azure OpenAI connection test successful")
            
        except Exception as e:
            logger.error(f"Azure OpenAI connection test failed: {e}")
            raise
    
    async def generate_embedding(self, text: str, use_cache: bool = True) -> List[float]:
        """Generate embedding for a single text."""
        
        if not text or not text.strip():
            raise ValueError("Text cannot be empty")
        
        # ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å‡∞ó‡∞æ ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞§‡∞®‡∞ø‡∞ñ‡±Ä ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
        if use_cache:
            cache_key = self._get_cache_key(text)
            cached_embedding = self._get_cached_embedding(cache_key)
            if cached_embedding:
                return cached_embedding
        
        try:
            # ‡∞ï‡±ç‡∞≤‡∞Ø‡∞ø‡∞Ç‡∞ü‡±ç ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡∞ø‡∞Ç‡∞¶‡∞®‡∞ø ‡∞®‡∞ø‡∞∞‡±ç‡∞ß‡∞æ‡∞∞‡∞ø‡∞Ç‡∞ö‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø
            if not self.client:
                await self.initialize()
            
            # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
            response = await self.client.embeddings.create(
                model=self.deployment_name,
                input=text.strip()
            )
            
            embedding = response.data[0].embedding
            
            # ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
            if use_cache:
                self._cache_embedding(cache_key, embedding)
            
            logger.debug(f"Generated embedding for text (length: {len(text)})")
            
            return embedding
            
        except HttpResponseError as e:
            logger.error(f"Azure OpenAI API error: {e}")
            raise Exception(f"Embedding generation failed: {e}")
        except Exception as e:
            logger.error(f"Embedding generation error: {e}")
            raise
    
    async def generate_embeddings_batch(
        self, 
        texts: List[str], 
        use_cache: bool = True
    ) -> List[List[float]]:
        """Generate embeddings for multiple texts efficiently."""
        
        if not texts:
            return []
        
        embeddings = []
        cache_misses = []
        cache_miss_indices = []
        
        # ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞§‡∞®‡∞ø‡∞ñ‡±Ä ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
        for i, text in enumerate(texts):
            if not text or not text.strip():
                embeddings.append([])
                continue
                
            if use_cache:
                cache_key = self._get_cache_key(text)
                cached_embedding = self._get_cached_embedding(cache_key)
                if cached_embedding:
                    embeddings.append(cached_embedding)
                    continue
            
            # ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞Æ‡∞ø‡∞∏‡±ç‡∞∏‡±Å‡∞≤‡∞®‡±Å ‡∞ü‡±ç‡∞∞‡∞æ‡∞ï‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
            embeddings.append(None)  # ‡∞™‡±ç‡∞≤‡±á‡∞∏‡±ç‚Äå‡∞π‡±ã‡∞≤‡±ç‡∞°‡∞∞‡±ç
            cache_misses.append(text.strip())
            cache_miss_indices.append(i)
        
        # ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞Æ‡∞ø‡∞∏‡±ç‡∞∏‡±Å‡∞≤ ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
        if cache_misses:
            try:
                # API ‡∞™‡∞∞‡∞ø‡∞Æ‡∞ø‡∞§‡±Å‡∞≤‡∞®‡±Å ‡∞ó‡±å‡∞∞‡∞µ‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç‚Äå‡∞≤‡∞≤‡±ã ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
                for batch_start in range(0, len(cache_misses), self.batch_size):
                    batch_end = min(batch_start + self.batch_size, len(cache_misses))
                    batch_texts = cache_misses[batch_start:batch_end]
                    
                    # ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
                    response = await self.client.embeddings.create(
                        model=self.deployment_name,
                        input=batch_texts
                    )
                    
                    # ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞≤‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
                    for j, embedding_data in enumerate(response.data):
                        actual_index = cache_miss_indices[batch_start + j]
                        embedding = embedding_data.embedding
                        embeddings[actual_index] = embedding
                        
                        # ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
                        if use_cache:
                            text = batch_texts[j]
                            cache_key = self._get_cache_key(text)
                            self._cache_embedding(cache_key, embedding)
                    
                    # ‡∞∞‡±á‡∞ü‡±ç ‡∞™‡∞∞‡∞ø‡∞Æ‡∞ø‡∞§‡∞ø - ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç‚Äå‡∞≤ ‡∞Æ‡∞ß‡±ç‡∞Ø ‡∞ö‡∞ø‡∞®‡±ç‡∞® ‡∞Ü‡∞≤‡∞∏‡±ç‡∞Ø‡∞Ç
                    if batch_end < len(cache_misses):
                        await asyncio.sleep(0.1)
                
                logger.info(f"Generated {len(cache_misses)} embeddings in batch")
                
            except Exception as e:
                logger.error(f"Batch embedding generation failed: {e}")
                raise
        
        return embeddings
    
    def _get_cache_key(self, text: str) -> str:
        """Generate cache key for text."""
        
        # ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞ï‡±Ä ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç + ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï SHA-256 ‡∞π‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
        content = f"{self.deployment_name}:{text.strip()}"
        return hashlib.sha256(content.encode()).hexdigest()
    
    def _get_cached_embedding(self, cache_key: str) -> Optional[List[float]]:
        """Get embedding from cache if not expired."""
        
        if cache_key in self.embedding_cache:
            embedding_data = self.embedding_cache[cache_key]
            
            # ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞é‡∞Ç‡∞ü‡±ç‡∞∞‡±Ä ‡∞á‡∞Ç‡∞ï‡∞æ ‡∞ö‡±Ü‡∞≤‡±ç‡∞≤‡±Å‡∞¨‡∞æ‡∞ü‡±Å ‡∞Ö‡∞Ø‡±ç‡∞Ø‡∞ø‡∞Ç‡∞¶‡±ã ‡∞§‡∞®‡∞ø‡∞ñ‡±Ä ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
            if datetime.now() - embedding_data['timestamp'] < self.cache_ttl:
                return embedding_data['embedding']
            else:
                # ‡∞ï‡∞æ‡∞≤‡∞Ç ‡∞Æ‡±Å‡∞ó‡∞ø‡∞∏‡∞ø‡∞® ‡∞é‡∞Ç‡∞ü‡±ç‡∞∞‡±Ä‡∞®‡∞ø ‡∞§‡±ä‡∞≤‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
                del self.embedding_cache[cache_key]
        
        return None
    
    def _cache_embedding(self, cache_key: str, embedding: List[float]):
        """Cache embedding with timestamp."""
        
        self.embedding_cache[cache_key] = {
            'embedding': embedding,
            'timestamp': datetime.now()
        }
        
        # ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞™‡∞∞‡∞ø‡∞Æ‡∞æ‡∞£‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞™‡∞∞‡∞ø‡∞Æ‡∞ø‡∞§‡∞Ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
        if len(self.embedding_cache) > 10000:
            # ‡∞™‡∞æ‡∞§ ‡∞é‡∞Ç‡∞ü‡±ç‡∞∞‡±Ä‡∞≤‡∞®‡±Å ‡∞§‡±ä‡∞≤‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
            oldest_keys = sorted(
                self.embedding_cache.keys(),
                key=lambda k: self.embedding_cache[k]['timestamp']
            )[:1000]
            
            for key in oldest_keys:
                del self.embedding_cache[key]
    
    async def cleanup(self):
        """Cleanup resources."""
        
        if self.client:
            await self.client.close()
        
        logger.info("Embedding manager cleanup completed")

# ‡∞ó‡±ç‡∞≤‡±ã‡∞¨‡∞≤‡±ç ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞Æ‡±á‡∞®‡±á‡∞ú‡∞∞‡±ç ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£
embedding_manager = EmbeddingManager(
    project_endpoint=os.getenv('PROJECT_ENDPOINT'),
    deployment_name=os.getenv('EMBEDDING_DEPLOYMENT_NAME', 'text-embedding-3-small')
)
```

## üîç ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ú‡∞®‡∞∞‡±á‡∞∑‡∞®‡±ç

### ‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡±Ü‡∞°‡±ç ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞™‡±à‡∞™‡±ç‚Äå‡∞≤‡±à‡∞®‡±ç

```python
# mcp_server/embeddings/product_embedder.py
"""
Product embedding generation and management.
"""
import asyncio
import asyncpg
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
from .embedding_manager import embedding_manager

logger = logging.getLogger(__name__)

class ProductEmbedder:
    """Generate and manage product embeddings for semantic search."""
    
    def __init__(self, db_provider):
        self.db_provider = db_provider
        self.embedding_manager = embedding_manager
        
        # ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡±Å‡∞≤ ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç ‡∞ï‡∞≤‡∞Ø‡∞ø‡∞ï ‡∞µ‡±ç‡∞Ø‡±Ç‡∞π‡∞Ç
        self.text_template = "{product_name} {brand} {description} {category} {tags}"
        
    async def generate_product_embeddings(
        self, 
        store_id: str,
        batch_size: int = 50,
        force_regenerate: bool = False
    ) -> Dict[str, Any]:
        """Generate embeddings for all products in a store."""
        
        async with self.db_provider.get_connection() as conn:
            try:
                # ‡∞∏‡±ç‡∞ü‡±ã‡∞∞‡±ç ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞∏‡±Ü‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
                await conn.execute("SELECT retail.set_store_context($1)", store_id)
                
                # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Æ‡±à‡∞® ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡±Å‡∞≤‡∞®‡±Å ‡∞™‡±ä‡∞Ç‡∞¶‡∞Ç‡∞°‡∞ø
                if force_regenerate:
                    products_query = """
                        SELECT 
                            p.product_id,
                            p.product_name,
                            p.product_description,
                            p.brand,
                            pc.category_name,
                            array_to_string(p.tags, ' ') as tags_text
                        FROM retail.products p
                        LEFT JOIN retail.product_categories pc ON p.category_id = pc.category_id
                        WHERE p.is_active = TRUE
                        ORDER BY p.created_at DESC
                    """
                else:
                    products_query = """
                        SELECT 
                            p.product_id,
                            p.product_name,
                            p.product_description,
                            p.brand,
                            pc.category_name,
                            array_to_string(p.tags, ' ') as tags_text
                        FROM retail.products p
                        LEFT JOIN retail.product_categories pc ON p.category_id = pc.category_id
                        LEFT JOIN retail.product_embeddings pe ON p.product_id = pe.product_id
                        WHERE p.is_active = TRUE
                          AND (pe.product_id IS NULL OR pe.updated_at < p.updated_at)
                        ORDER BY p.created_at DESC
                    """
                
                products = await conn.fetch(products_query)
                
                if not products:
                    return {
                        'success': True,
                        'message': 'No products need embedding generation',
                        'processed_count': 0,
                        'store_id': store_id
                    }
                
                logger.info(f"Generating embeddings for {len(products)} products in store {store_id}")
                
                # ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡±Å‡∞≤‡∞®‡±Å ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç‚Äå‡∞≤‡∞≤‡±ã ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
                processed_count = 0
                
                for i in range(0, len(products), batch_size):
                    batch = products[i:i + batch_size]
                    await self._process_product_batch(conn, batch, store_id)
                    processed_count += len(batch)
                    
                    logger.info(f"Processed {processed_count}/{len(products)} products")
                
                return {
                    'success': True,
                    'message': f'Successfully generated embeddings for {processed_count} products',
                    'processed_count': processed_count,
                    'store_id': store_id,
                    'total_products': len(products)
                }
                
            except Exception as e:
                logger.error(f"Product embedding generation failed: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'store_id': store_id
                }
    
    async def _process_product_batch(
        self, 
        conn: asyncpg.Connection, 
        products: List[Dict], 
        store_id: str
    ):
        """Process a batch of products for embedding generation."""
        
        # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
        texts = []
        product_ids = []
        
        for product in products:
            # ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞∂‡±ã‡∞ß‡∞ø‡∞Ç‡∞ö‡∞¶‡∞ó‡∞ø‡∞® ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞ó‡∞æ ‡∞ï‡∞≤‡∞™‡∞Ç‡∞°‡∞ø
            combined_text = self._create_product_text(product)
            texts.append(combined_text)
            product_ids.append(product['product_id'])
        
        # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
        embeddings = await self.embedding_manager.generate_embeddings_batch(texts)
        
        # ‡∞°‡±á‡∞ü‡∞æ‡∞¨‡±á‡∞∏‡±ç‚Äå‡∞≤‡±ã ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞®‡∞ø‡∞≤‡±ç‡∞µ ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
        for i, (product_id, embedding) in enumerate(zip(product_ids, embeddings)):
            if embedding:  # ‡∞µ‡∞ø‡∞´‡∞≤‡∞Æ‡±à‡∞® ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç‚Äå‡∞®‡±Å ‡∞¶‡∞æ‡∞ü‡∞µ‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
                await self._store_product_embedding(
                    conn, 
                    product_id, 
                    store_id, 
                    texts[i], 
                    embedding
                )
    
    def _create_product_text(self, product: Dict[str, Any]) -> str:
        """Create combined text for product embedding."""
        
        # None ‡∞µ‡∞ø‡∞≤‡±Å‡∞µ‡∞≤‡∞®‡±Å ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
        product_name = product.get('product_name') or ''
        brand = product.get('brand') or ''
        description = product.get('product_description') or ''
        category = product.get('category_name') or ''
        tags = product.get('tags_text') or ''
        
        # ‡∞∂‡±ã‡∞ß‡∞ø‡∞Ç‡∞ö‡∞¶‡∞ó‡∞ø‡∞® ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞ó‡∞æ ‡∞ï‡∞≤‡∞™‡∞Ç‡∞°‡∞ø
        combined_text = self.text_template.format(
            product_name=product_name,
            brand=brand,
            description=description,
            category=category,
            tags=tags
        )
        
        # ‡∞Ö‡∞¶‡∞®‡∞™‡±Å ‡∞ñ‡∞æ‡∞≥‡±Ä‡∞≤‡∞®‡±Å ‡∞∂‡±Å‡∞≠‡±ç‡∞∞‡∞™‡∞∞‡∞ö‡∞Ç‡∞°‡∞ø
        return ' '.join(combined_text.split())
    
    async def _store_product_embedding(
        self,
        conn: asyncpg.Connection,
        product_id: str,
        store_id: str,
        embedding_text: str,
        embedding: List[float]
    ):
        """Store product embedding in database."""
        
        # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‚Äå‡∞®‡±Å pgvector ‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞æ‡∞ü‡±ç‚Äå‡∞ï‡±Å ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞Ç‡∞°‡∞ø
        embedding_vector = f"[{','.join(map(str, embedding))}]"
        
        # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‚Äå‡∞®‡±Å ‡∞Ö‡∞™‡±ç‚Äå‡∞∏‡∞∞‡±ç‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
        upsert_query = """
            INSERT INTO retail.product_embeddings (
                product_id, store_id, embedding_text, embedding, embedding_model
            ) VALUES ($1, $2, $3, $4, $5)
            ON CONFLICT (product_id, embedding_model) 
            DO UPDATE SET
                store_id = EXCLUDED.store_id,
                embedding_text = EXCLUDED.embedding_text,
                embedding = EXCLUDED.embedding,
                updated_at = CURRENT_TIMESTAMP
        """
        
        await conn.execute(
            upsert_query,
            product_id,
            store_id,
            embedding_text,
            embedding_vector,
            self.embedding_manager.deployment_name
        )
    
    async def update_product_embedding(
        self, 
        product_id: str, 
        store_id: str
    ) -> Dict[str, Any]:
        """Update embedding for a single product."""
        
        async with self.db_provider.get_connection() as conn:
            try:
                # ‡∞∏‡±ç‡∞ü‡±ã‡∞∞‡±ç ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞∏‡±Ü‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
                await conn.execute("SELECT retail.set_store_context($1)", store_id)
                
                # ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞™‡±ä‡∞Ç‡∞¶‡∞Ç‡∞°‡∞ø
                product_query = """
                    SELECT 
                        p.product_id,
                        p.product_name,
                        p.product_description,
                        p.brand,
                        pc.category_name,
                        array_to_string(p.tags, ' ') as tags_text
                    FROM retail.products p
                    LEFT JOIN retail.product_categories pc ON p.category_id = pc.category_id
                    WHERE p.product_id = $1 AND p.is_active = TRUE
                """
                
                product = await conn.fetchrow(product_query, product_id)
                
                if not product:
                    return {
                        'success': False,
                        'error': f'Product {product_id} not found or inactive'
                    }
                
                # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
                combined_text = self._create_product_text(dict(product))
                embedding = await self.embedding_manager.generate_embedding(combined_text)
                
                # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞®‡∞ø‡∞≤‡±ç‡∞µ ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
                await self._store_product_embedding(
                    conn, product_id, store_id, combined_text, embedding
                )
                
                return {
                    'success': True,
                    'message': f'Successfully updated embedding for product {product_id}',
                    'product_id': product_id,
                    'store_id': store_id
                }
                
            except Exception as e:
                logger.error(f"Single product embedding update failed: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'product_id': product_id
                }

# ‡∞ó‡±ç‡∞≤‡±ã‡∞¨‡∞≤‡±ç ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞∞‡±ç ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£
product_embedder = ProductEmbedder(db_provider)
```

## üîé ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞ü‡±Ç‡∞≤‡±ç‡∞∏‡±ç

### ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞ü‡±Ç‡∞≤‡±ç

```python
# mcp_server/tools/semantic_search.py
"""
Semantic search tools for natural language product queries.
"""
from typing import Dict, Any, List, Optional
from ..tools.base import DatabaseTool, ToolResult, ToolCategory
from ..embeddings.embedding_manager import embedding_manager
import logging

logger = logging.getLogger(__name__)

class SemanticProductSearchTool(DatabaseTool):
    """Advanced semantic search tool for products using vector similarity."""
    
    def __init__(self, db_provider):
        super().__init__(
            name="semantic_search_products",
            description="Search products using natural language queries with semantic understanding",
            db_provider=db_provider
        )
        self.category = ToolCategory.DATABASE_QUERY
        self.embedding_manager = embedding_manager
    
    async def execute(self, **kwargs) -> ToolResult:
        """Execute semantic product search."""
        
        query = kwargs.get('query')
        store_id = kwargs.get('store_id')
        limit = kwargs.get('limit', 20)
        similarity_threshold = kwargs.get('similarity_threshold', 0.7)
        include_metadata = kwargs.get('include_metadata', True)
        
        if not query:
            return ToolResult(
                success=False,
                error="Search query is required"
            )
        
        if not store_id:
            return ToolResult(
                success=False,
                error="store_id is required for semantic search"
            )
        
        try:
            # ‡∞ï‡±ç‡∞µ‡±Ü‡∞∞‡±Ä ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
            query_embedding = await self.embedding_manager.generate_embedding(query)
            
            # ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∂‡±ã‡∞ß‡∞® ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
            search_results = await self._perform_semantic_search(
                query_embedding,
                store_id,
                limit,
                similarity_threshold,
                include_metadata
            )
            
            return ToolResult(
                success=True,
                data=search_results,
                row_count=len(search_results),
                metadata={
                    'query': query,
                    'store_id': store_id,
                    'similarity_threshold': similarity_threshold,
                    'search_type': 'semantic'
                }
            )
            
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return ToolResult(
                success=False,
                error=f"Semantic search failed: {str(e)}"
            )
    
    async def _perform_semantic_search(
        self,
        query_embedding: List[float],
        store_id: str,
        limit: int,
        similarity_threshold: float,
        include_metadata: bool
    ) -> List[Dict[str, Any]]:
        """Perform vector similarity search."""
        
        # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‚Äå‡∞®‡±Å PostgreSQL ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞æ‡∞ü‡±ç‚Äå‡∞ï‡±Å ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞Ç‡∞°‡∞ø
        embedding_vector = f"[{','.join(map(str, query_embedding))}]"
        
        # ‡∞∂‡±ã‡∞ß‡∞® ‡∞ï‡±ç‡∞µ‡±Ü‡∞∞‡±Ä‡∞®‡∞ø ‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
        if include_metadata:
            search_query = """
                SELECT 
                    p.product_id,
                    p.product_name,
                    p.brand,
                    p.price,
                    p.product_description,
                    p.current_stock,
                    p.rating_average,
                    p.rating_count,
                    p.tags,
                    pc.category_name,
                    pe.embedding_text,
                    1 - (pe.embedding <=> $1::vector) as similarity_score
                FROM retail.product_embeddings pe
                JOIN retail.products p ON pe.product_id = p.product_id
                LEFT JOIN retail.product_categories pc ON p.category_id = pc.category_id
                WHERE pe.store_id = $2
                  AND p.is_active = TRUE
                  AND 1 - (pe.embedding <=> $1::vector) >= $3
                ORDER BY pe.embedding <=> $1::vector
                LIMIT $4
            """
        else:
            search_query = """
                SELECT 
                    p.product_id,
                    p.product_name,
                    p.brand,
                    p.price,
                    1 - (pe.embedding <=> $1::vector) as similarity_score
                FROM retail.product_embeddings pe
                JOIN retail.products p ON pe.product_id = p.product_id
                WHERE pe.store_id = $2
                  AND p.is_active = TRUE
                  AND 1 - (pe.embedding <=> $1::vector) >= $3
                ORDER BY pe.embedding <=> $1::vector
                LIMIT $4
            """
        
        async with self.get_connection() as conn:
            # ‡∞∏‡±ç‡∞ü‡±ã‡∞∞‡±ç ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞∏‡±Ü‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
            await conn.execute("SELECT retail.set_store_context($1)", store_id)
            
            # ‡∞∂‡±ã‡∞ß‡∞®‡∞®‡±Å ‡∞Ö‡∞Æ‡∞≤‡±Å ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
            results = await conn.fetch(
                search_query,
                embedding_vector,
                store_id,
                similarity_threshold,
                limit
            )
            
            return [dict(result) for result in results]
    
    def get_input_schema(self) -> Dict[str, Any]:
        """Get input schema for semantic search tool."""
        
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Natural language search query",
                    "minLength": 1,
                    "maxLength": 500
                },
                "store_id": {
                    "type": "string",
                    "description": "Store ID for search scope",
                    "pattern": "^[a-zA-Z0-9_-]+$"
                },
                "limit": {
                    "type": "integer",
                    "description": "Maximum number of results to return",
                    "minimum": 1,
                    "maximum": 100,
                    "default": 20
                },
                "similarity_threshold": {
                    "type": "number",
                    "description": "Minimum similarity score (0.0 to 1.0)",
                    "minimum": 0.0,
                    "maximum": 1.0,
                    "default": 0.7
                },
                "include_metadata": {
                    "type": "boolean",
                    "description": "Include detailed product metadata in results",
                    "default": True
                }
            },
            "required": ["query", "store_id"],
            "additionalProperties": False
        }

class HybridSearchTool(DatabaseTool):
    """Hybrid search combining traditional keyword and semantic search."""
    
    def __init__(self, db_provider):
        super().__init__(
            name="hybrid_product_search",
            description="Hybrid search combining keyword matching and semantic similarity for optimal results",
            db_provider=db_provider
        )
        self.category = ToolCategory.DATABASE_QUERY
        self.embedding_manager = embedding_manager
    
    async def execute(self, **kwargs) -> ToolResult:
        """Execute hybrid product search."""
        
        query = kwargs.get('query')
        store_id = kwargs.get('store_id')
        limit = kwargs.get('limit', 20)
        semantic_weight = kwargs.get('semantic_weight', 0.7)
        keyword_weight = kwargs.get('keyword_weight', 0.3)
        
        if not query:
            return ToolResult(
                success=False,
                error="Search query is required"
            )
        
        if not store_id:
            return ToolResult(
                success=False,
                error="store_id is required for hybrid search"
            )
        
        try:
            # ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∂‡±ã‡∞ß‡∞® ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞ï‡±ç‡∞µ‡±Ü‡∞∞‡±Ä ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
            query_embedding = await self.embedding_manager.generate_embedding(query)
            
            # ‡∞π‡±à‡∞¨‡±ç‡∞∞‡∞ø‡∞°‡±ç ‡∞∂‡±ã‡∞ß‡∞® ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
            search_results = await self._perform_hybrid_search(
                query,
                query_embedding,
                store_id,
                limit,
                semantic_weight,
                keyword_weight
            )
            
            return ToolResult(
                success=True,
                data=search_results,
                row_count=len(search_results),
                metadata={
                    'query': query,
                    'store_id': store_id,
                    'semantic_weight': semantic_weight,
                    'keyword_weight': keyword_weight,
                    'search_type': 'hybrid'
                }
            )
            
        except Exception as e:
            logger.error(f"Hybrid search failed: {e}")
            return ToolResult(
                success=False,
                error=f"Hybrid search failed: {str(e)}"
            )
    
    async def _perform_hybrid_search(
        self,
        query: str,
        query_embedding: List[float],
        store_id: str,
        limit: int,
        semantic_weight: float,
        keyword_weight: float
    ) -> List[Dict[str, Any]]:
        """Perform hybrid search combining keyword and semantic similarity."""
        
        # ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‚Äå‡∞®‡±Å PostgreSQL ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞æ‡∞ü‡±ç‚Äå‡∞ï‡±Å ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞Ç‡∞°‡∞ø
        embedding_vector = f"[{','.join(map(str, query_embedding))}]"
        
        # ‡∞ï‡±Ä‡∞µ‡∞∞‡±ç‡∞°‡±ç ‡∞Æ‡±ç‡∞Ø‡∞æ‡∞ö‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞∂‡±ã‡∞ß‡∞® ‡∞™‡∞¶‡∞æ‡∞≤‡±Å ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
        search_terms = ' & '.join(query.lower().split())
        
        hybrid_query = """
            WITH keyword_scores AS (
                SELECT 
                    p.product_id,
                    ts_rank(
                        to_tsvector('english', 
                            p.product_name || ' ' || 
                            COALESCE(p.product_description, '') || ' ' || 
                            COALESCE(p.brand, '') || ' ' ||
                            COALESCE(array_to_string(p.tags, ' '), '')
                        ),
                        plainto_tsquery('english', $2)
                    ) as keyword_score
                FROM retail.products p
                WHERE p.is_active = TRUE
                  AND p.store_id = $3
                  AND (
                    to_tsvector('english', 
                        p.product_name || ' ' || 
                        COALESCE(p.product_description, '') || ' ' || 
                        COALESCE(p.brand, '') || ' ' ||
                        COALESCE(array_to_string(p.tags, ' '), '')
                    ) @@ plainto_tsquery('english', $2)
                    OR p.product_name ILIKE '%' || $2 || '%'
                    OR p.brand ILIKE '%' || $2 || '%'
                  )
            ),
            semantic_scores AS (
                SELECT 
                    pe.product_id,
                    1 - (pe.embedding <=> $1::vector) as semantic_score
                FROM retail.product_embeddings pe
                WHERE pe.store_id = $3
                  AND 1 - (pe.embedding <=> $1::vector) >= 0.5
            ),
            combined_scores AS (
                SELECT 
                    COALESCE(ks.product_id, ss.product_id) as product_id,
                    COALESCE(ks.keyword_score, 0) * $4 as weighted_keyword_score,
                    COALESCE(ss.semantic_score, 0) * $5 as weighted_semantic_score,
                    COALESCE(ks.keyword_score, 0) * $4 + COALESCE(ss.semantic_score, 0) * $5 as combined_score
                FROM keyword_scores ks
                FULL OUTER JOIN semantic_scores ss ON ks.product_id = ss.product_id
                WHERE COALESCE(ks.keyword_score, 0) * $4 + COALESCE(ss.semantic_score, 0) * $5 > 0
            )
            SELECT 
                p.product_id,
                p.product_name,
                p.brand,
                p.price,
                p.product_description,
                p.current_stock,
                p.rating_average,
                p.rating_count,
                p.tags,
                pc.category_name,
                cs.weighted_keyword_score,
                cs.weighted_semantic_score,
                cs.combined_score
            FROM combined_scores cs
            JOIN retail.products p ON cs.product_id = p.product_id
            LEFT JOIN retail.product_categories pc ON p.category_id = pc.category_id
            WHERE p.is_active = TRUE
            ORDER BY cs.combined_score DESC
            LIMIT $6
        """
        
        async with self.get_connection() as conn:
            # ‡∞∏‡±ç‡∞ü‡±ã‡∞∞‡±ç ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞∏‡±Ü‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
            await conn.execute("SELECT retail.set_store_context($1)", store_id)
            
            # ‡∞π‡±à‡∞¨‡±ç‡∞∞‡∞ø‡∞°‡±ç ‡∞∂‡±ã‡∞ß‡∞®‡∞®‡±Å ‡∞Ö‡∞Æ‡∞≤‡±Å ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
            results = await conn.fetch(
                hybrid_query,
                embedding_vector,  # $1
                query,            # $2
                store_id,         # $3
                keyword_weight,   # $4
                semantic_weight,  # $5
                limit            # $6
            )
            
            return [dict(result) for result in results]
    
    def get_input_schema(self) -> Dict[str, Any]:
        """Get input schema for hybrid search tool."""
        
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Search query (supports both keywords and natural language)",
                    "minLength": 1,
                    "maxLength": 500
                },
                "store_id": {
                    "type": "string",
                    "description": "Store ID for search scope",
                    "pattern": "^[a-zA-Z0-9_-]+$"
                },
                "limit": {
                    "type": "integer",
                    "description": "Maximum number of results to return",
                    "minimum": 1,
                    "maximum": 100,
                    "default": 20
                },
                "semantic_weight": {
                    "type": "number",
                    "description": "Weight for semantic similarity (0.0 to 1.0)",
                    "minimum": 0.0,
                    "maximum": 1.0,
                    "default": 0.7
                },
                "keyword_weight": {
                    "type": "number",
                    "description": "Weight for keyword matching (0.0 to 1.0)",
                    "minimum": 0.0,
                    "maximum": 1.0,
                    "default": 0.3
                }
            },
            "required": ["query", "store_id"],
            "additionalProperties": False
        }
```

## üéØ ‡∞∏‡∞ø‡∞´‡∞æ‡∞∞‡±ç‡∞∏‡±Å ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡±ç‡∞•‡∞≤‡±Å

### ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞∏‡∞ø‡∞´‡∞æ‡∞∞‡±ç‡∞∏‡±Å ‡∞á‡∞Ç‡∞ú‡∞ø‡∞®‡±ç

```python
# mcp_server/tools/recommendations.py
"""
Product recommendation system using embedding similarity.
"""
from typing import Dict, Any, List, Optional
from ..tools.base import DatabaseTool, ToolResult, ToolCategory
import logging

logger = logging.getLogger(__name__)

class ProductRecommendationTool(DatabaseTool):
    """Generate product recommendations based on similarity and user behavior."""
    
    def __init__(self, db_provider):
        super().__init__(
            name="get_product_recommendations",
            description="Generate personalized product recommendations using similarity analysis",
            db_provider=db_provider
        )
        self.category = ToolCategory.ANALYTICS
    
    async def execute(self, **kwargs) -> ToolResult:
        """Execute product recommendation generation."""
        
        recommendation_type = kwargs.get('type', 'similar_products')
        store_id = kwargs.get('store_id')
        
        if not store_id:
            return ToolResult(
                success=False,
                error="store_id is required for recommendations"
            )
        
        try:
            if recommendation_type == 'similar_products':
                return await self._get_similar_products(kwargs)
            elif recommendation_type == 'customer_based':
                return await self._get_customer_recommendations(kwargs)
            elif recommendation_type == 'trending':
                return await self._get_trending_products(kwargs)
            elif recommendation_type == 'cross_sell':
                return await self._get_cross_sell_recommendations(kwargs)
            else:
                return ToolResult(
                    success=False,
                    error=f"Unknown recommendation type: {recommendation_type}"
                )
        
        except Exception as e:
            logger.error(f"Product recommendation failed: {e}")
            return ToolResult(
                success=False,
                error=f"Recommendation generation failed: {str(e)}"
            )
    
    async def _get_similar_products(self, kwargs: Dict[str, Any]) -> ToolResult:
        """Get products similar to a given product using embedding similarity."""
        
        product_id = kwargs.get('product_id')
        store_id = kwargs['store_id']
        limit = kwargs.get('limit', 10)
        similarity_threshold = kwargs.get('similarity_threshold', 0.7)
        
        if not product_id:
            return ToolResult(
                success=False,
                error="product_id is required for similar product recommendations"
            )
        
        similar_products_query = """
            WITH target_product AS (
                SELECT embedding
                FROM retail.product_embeddings
                WHERE product_id = $1 AND store_id = $2
            )
            SELECT 
                p.product_id,
                p.product_name,
                p.brand,
                p.price,
                p.product_description,
                p.rating_average,
                p.rating_count,
                pc.category_name,
                1 - (pe.embedding <=> tp.embedding) as similarity_score
            FROM retail.product_embeddings pe
            CROSS JOIN target_product tp
            JOIN retail.products p ON pe.product_id = p.product_id
            LEFT JOIN retail.product_categories pc ON p.category_id = pc.category_id
            WHERE pe.store_id = $2
              AND pe.product_id != $1  -- Exclude the target product itself
              AND p.is_active = TRUE
              AND 1 - (pe.embedding <=> tp.embedding) >= $3
            ORDER BY pe.embedding <=> tp.embedding
            LIMIT $4
        """
        
        result = await self.execute_query(
            similar_products_query,
            (product_id, store_id, similarity_threshold, limit),
            store_id
        )
        
        if result.success:
            result.metadata = {
                'recommendation_type': 'similar_products',
                'target_product_id': product_id,
                'similarity_threshold': similarity_threshold,
                'store_id': store_id
            }
        
        return result
    
    async def _get_customer_recommendations(self, kwargs: Dict[str, Any]) -> ToolResult:
        """Get personalized recommendations based on customer purchase history."""
        
        customer_id = kwargs.get('customer_id')
        store_id = kwargs['store_id']
        limit = kwargs.get('limit', 10)
        days_back = kwargs.get('days_back', 90)
        
        if not customer_id:
            return ToolResult(
                success=False,
                error="customer_id is required for customer-based recommendations"
            )
        
        customer_recommendations_query = """
            WITH customer_purchases AS (
                -- Get products purchased by the customer
                SELECT DISTINCT p.product_id, pe.embedding
                FROM retail.sales_transactions st
                JOIN retail.sales_transaction_items sti ON st.transaction_id = sti.transaction_id
                JOIN retail.products p ON sti.product_id = p.product_id
                JOIN retail.product_embeddings pe ON p.product_id = pe.product_id
                WHERE st.customer_id = $1
                  AND st.transaction_date >= CURRENT_DATE - INTERVAL '%s days'
                  AND st.transaction_type = 'sale'
            ),
            avg_customer_embedding AS (
                -- Calculate average embedding vector for customer preferences
                SELECT 
                    (
                        SELECT ARRAY(
                            SELECT AVG(embedding_element)
                            FROM customer_purchases cp,
                                 LATERAL unnest(cp.embedding) WITH ORDINALITY AS t(embedding_element, ordinality)
                            GROUP BY ordinality
                            ORDER BY ordinality
                        )
                    )::vector as avg_embedding
                FROM (SELECT 1) dummy
                WHERE EXISTS (SELECT 1 FROM customer_purchases)
            )
            SELECT 
                p.product_id,
                p.product_name,
                p.brand,
                p.price,
                p.product_description,
                p.rating_average,
                p.rating_count,
                pc.category_name,
                1 - (pe.embedding <=> ace.avg_embedding) as preference_score
            FROM retail.product_embeddings pe
            CROSS JOIN avg_customer_embedding ace
            JOIN retail.products p ON pe.product_id = p.product_id
            LEFT JOIN retail.product_categories pc ON p.category_id = pc.category_id
            WHERE pe.store_id = $2
              AND p.is_active = TRUE
              AND pe.product_id NOT IN (SELECT product_id FROM customer_purchases)
              AND 1 - (pe.embedding <=> ace.avg_embedding) >= 0.6
            ORDER BY pe.embedding <=> ace.avg_embedding
            LIMIT $3
        """ % days_back
        
        result = await self.execute_query(
            customer_recommendations_query,
            (customer_id, store_id, limit),
            store_id
        )
        
        if result.success:
            result.metadata = {
                'recommendation_type': 'customer_based',
                'customer_id': customer_id,
                'days_back': days_back,
                'store_id': store_id
            }
        
        return result
    
    def get_input_schema(self) -> Dict[str, Any]:
        """Get input schema for recommendation tool."""
        
        return {
            "type": "object",
            "properties": {
                "type": {
                    "type": "string",
                    "enum": ["similar_products", "customer_based", "trending", "cross_sell"],
                    "description": "Type of recommendation to generate",
                    "default": "similar_products"
                },
                "store_id": {
                    "type": "string",
                    "description": "Store ID for recommendations",
                    "pattern": "^[a-zA-Z0-9_-]+$"
                },
                "product_id": {
                    "type": "string",
                    "description": "Product ID for similar product recommendations"
                },
                "customer_id": {
                    "type": "string",
                    "description": "Customer ID for personalized recommendations"
                },
                "limit": {
                    "type": "integer",
                    "description": "Maximum number of recommendations",
                    "minimum": 1,
                    "maximum": 50,
                    "default": 10
                },
                "similarity_threshold": {
                    "type": "number",
                    "description": "Minimum similarity score",
                    "minimum": 0.0,
                    "maximum": 1.0,
                    "default": 0.7
                },
                "days_back": {
                    "type": "integer",
                    "description": "Days of purchase history to consider",
                    "minimum": 1,
                    "maximum": 365,
                    "default": 90
                }
            },
            "required": ["store_id"],
            "additionalProperties": False
        }
```

## ‚ö° ‡∞™‡∞®‡∞ø‡∞§‡±Ä‡∞∞‡±Å ‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç

### ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞® ‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç

```sql
-- Optimize pgvector performance
-- Add to postgresql.conf

# Increase work_mem for vector operations
work_mem = '256MB'

# Optimize shared_buffers for vector data
shared_buffers = '512MB'

# Enable parallel query execution
max_parallel_workers_per_gather = 4
max_parallel_workers = 8

# Vector-specific optimizations
SET maintenance_work_mem = '1GB';
SET max_parallel_maintenance_workers = 4;

-- Optimize HNSW index parameters
CREATE INDEX CONCURRENTLY idx_product_embeddings_vector_optimized 
ON retail.product_embeddings 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 200);

-- Create partial indexes for active products only
CREATE INDEX CONCURRENTLY idx_product_embeddings_active
ON retail.product_embeddings 
USING hnsw (embedding vector_cosine_ops)
WHERE store_id IN (SELECT store_id FROM retail.stores WHERE is_active = TRUE);

-- Analyze vector distribution for optimization
ANALYZE retail.product_embeddings;

-- Vector search performance monitoring
CREATE OR REPLACE FUNCTION retail.analyze_vector_performance()
RETURNS TABLE (
    avg_search_time_ms NUMERIC,
    index_size TEXT,
    total_vectors BIGINT,
    cache_hit_ratio NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        (SELECT AVG(EXTRACT(MILLISECONDS FROM clock_timestamp() - query_start))
         FROM pg_stat_activity 
         WHERE query LIKE '%embedding <=> %'
         AND state = 'active') as avg_search_time_ms,
        pg_size_pretty(pg_relation_size('idx_product_embeddings_vector')) as index_size,
        COUNT(*)::BIGINT as total_vectors,
        (SELECT 100.0 * blks_hit / (blks_hit + blks_read) 
         FROM pg_stat_user_indexes 
         WHERE indexrelname = 'idx_product_embeddings_vector') as cache_hit_ratio
    FROM retail.product_embeddings;
END;
$$ LANGUAGE plpgsql;
```

### ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞µ‡±ç‡∞Ø‡±Ç‡∞π‡∞Ç

```python
# mcp_server/embeddings/cache_manager.py
"""
Advanced caching strategy for embeddings and search results.
"""
import redis.asyncio as redis
import json
import hashlib
from typing import Dict, Any, List, Optional
from datetime import timedelta
import logging

logger = logging.getLogger(__name__)

class EmbeddingCacheManager:
    """Advanced caching for embeddings and search results."""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = None
        self.redis_url = redis_url
        
        # ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç TTL ‡∞∏‡±Ü‡∞ü‡±ç‡∞ü‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç
        self.embedding_ttl = timedelta(days=7)  # 1 ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞ü‡±Å ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞ö‡±á‡∞Ø‡∞¨‡∞°‡∞ø‡∞Ç‡∞¶‡∞ø
        self.search_ttl = timedelta(hours=1)    # 1 ‡∞ó‡∞Ç‡∞ü ‡∞™‡∞æ‡∞ü‡±Å ‡∞∂‡±ã‡∞ß‡∞® ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞≤‡±Å ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞ö‡±á‡∞Ø‡∞¨‡∞°‡∞ø‡∞Ç‡∞¶‡∞ø
        self.recommendation_ttl = timedelta(hours=4)  # 4 ‡∞ó‡∞Ç‡∞ü‡∞≤‡±Å ‡∞™‡∞æ‡∞ü‡±Å ‡∞∏‡∞ø‡∞´‡∞æ‡∞∞‡±ç‡∞∏‡±Å‡∞≤‡±Å ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞ö‡±á‡∞Ø‡∞¨‡∞°‡∞ø‡∞Ç‡∞¶‡∞ø
        
        # ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞ï‡±Ä ‡∞™‡±ç‡∞∞‡∞ø‡∞´‡∞ø‡∞ï‡±ç‡∞∏‡±Å‡∞≤‡±Å
        self.EMBEDDING_PREFIX = "emb:"
        self.SEARCH_PREFIX = "search:"
        self.RECOMMENDATION_PREFIX = "rec:"
    
    async def initialize(self):
        """Initialize Redis connection."""
        
        try:
            self.redis_client = redis.from_url(self.redis_url)
            # ‡∞ï‡∞®‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞™‡∞∞‡±Ä‡∞ï‡±ç‡∞∑‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
            await self.redis_client.ping()
            logger.info("Embedding cache manager initialized")
        
        except Exception as e:
            logger.warning(f"Redis cache not available: {e}")
            self.redis_client = None
    
    async def cache_embedding(self, text: str, embedding: List[float], model: str):
        """Cache text embedding."""
        
        if not self.redis_client:
            return
        
        try:
            cache_key = self._get_embedding_key(text, model)
            cache_data = {
                'embedding': embedding,
                'model': model,
                'cached_at': str(datetime.utcnow())
            }
            
            await self.redis_client.setex(
                cache_key,
                self.embedding_ttl,
                json.dumps(cache_data)
            )
            
        except Exception as e:
            logger.warning(f"Failed to cache embedding: {e}")
    
    async def get_cached_embedding(self, text: str, model: str) -> Optional[List[float]]:
        """Get cached embedding."""
        
        if not self.redis_client:
            return None
        
        try:
            cache_key = self._get_embedding_key(text, model)
            cached_data = await self.redis_client.get(cache_key)
            
            if cached_data:
                data = json.loads(cached_data)
                return data['embedding']
        
        except Exception as e:
            logger.warning(f"Failed to retrieve cached embedding: {e}")
        
        return None
    
    async def cache_search_results(
        self, 
        query: str, 
        store_id: str, 
        results: List[Dict],
        search_params: Dict[str, Any]
    ):
        """Cache search results."""
        
        if not self.redis_client:
            return
        
        try:
            cache_key = self._get_search_key(query, store_id, search_params)
            cache_data = {
                'results': results,
                'query': query,
                'store_id': store_id,
                'params': search_params,
                'cached_at': str(datetime.utcnow())
            }
            
            await self.redis_client.setex(
                cache_key,
                self.search_ttl,
                json.dumps(cache_data, default=str)
            )
            
        except Exception as e:
            logger.warning(f"Failed to cache search results: {e}")
    
    async def get_cached_search_results(
        self, 
        query: str, 
        store_id: str, 
        search_params: Dict[str, Any]
    ) -> Optional[List[Dict]]:
        """Get cached search results."""
        
        if not self.redis_client:
            return None
        
        try:
            cache_key = self._get_search_key(query, store_id, search_params)
            cached_data = await self.redis_client.get(cache_key)
            
            if cached_data:
                data = json.loads(cached_data)
                return data['results']
        
        except Exception as e:
            logger.warning(f"Failed to retrieve cached search results: {e}")
        
        return None
    
    def _get_embedding_key(self, text: str, model: str) -> str:
        """Generate cache key for embedding."""
        
        content = f"{model}:{text.strip()}"
        hash_key = hashlib.sha256(content.encode()).hexdigest()
        return f"{self.EMBEDDING_PREFIX}{hash_key}"
    
    def _get_search_key(self, query: str, store_id: str, params: Dict[str, Any]) -> str:
        """Generate cache key for search results."""
        
        # ‡∞ï‡±ç‡∞µ‡±Ü‡∞∞‡±Ä ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞™‡∞æ‡∞∞‡∞æ‡∞Æ‡∞ø‡∞§‡±Å‡∞≤ ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞∏‡±ç‡∞•‡∞ø‡∞∞‡∞Æ‡±à‡∞® ‡∞π‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø
        content = f"{query}:{store_id}:{json.dumps(params, sort_keys=True)}"
        hash_key = hashlib.sha256(content.encode()).hexdigest()
        return f"{self.SEARCH_PREFIX}{hash_key}"
    
    async def invalidate_store_cache(self, store_id: str):
        """Invalidate all cached data for a store."""
        
        if not self.redis_client:
            return
        
        try:
            # ‡∞∏‡±ç‡∞ü‡±ã‡∞∞‡±ç‚Äå‡∞ï‡±Å ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® ‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±Ä‡∞≤‡±Å ‡∞ï‡∞®‡±Å‡∞ó‡±ä‡∞®‡∞Ç‡∞°‡∞ø
            pattern = f"*:{store_id}:*"
            keys = await self.redis_client.keys(pattern)
            
            if keys:
                await self.redis_client.delete(*keys)
                logger.info(f"Invalidated {len(keys)} cache entries for store {store_id}")
        
        except Exception as e:
            logger.warning(f"Failed to invalidate store cache: {e}")
    
    async def cleanup(self):
        """Cleanup cache resources."""
        
        if self.redis_client:
            await self.redis_client.close()

# ‡∞ó‡±ç‡∞≤‡±ã‡∞¨‡∞≤‡±ç ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡±ç ‡∞Æ‡±á‡∞®‡±á‡∞ú‡∞∞‡±ç
cache_manager = EmbeddingCacheManager()
```

## üéØ ‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø‡∞Æ‡±à‡∞® ‡∞™‡∞æ‡∞†‡∞æ‡∞≤‡±Å

‡∞à ‡∞≤‡±ç‡∞Ø‡∞æ‡∞¨‡±ç ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞ö‡±á‡∞∏‡∞ø‡∞® ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞Ç‡∞°‡∞æ‡∞≤‡∞ø:

‚úÖ **Azure OpenAI ‡∞á‡∞Ç‡∞ü‡∞ø‡∞ó‡±ç‡∞∞‡±á‡∞∑‡∞®‡±ç**: ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡∞ø‡∞Ç‡∞ó‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç‚Äå‡∞§‡±ã ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ú‡∞®‡∞∞‡±á‡∞∑‡∞®‡±ç  
‚úÖ **‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞Ö‡∞Æ‡∞≤‡±Å**: pgvector ‡∞§‡±ã ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø-‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Æ‡±à‡∞® ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç  
‚úÖ **‡∞π‡±à‡∞¨‡±ç‡∞∞‡∞ø‡∞°‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞∏‡∞æ‡∞Æ‡∞∞‡±ç‡∞•‡±ç‡∞Ø‡∞æ‡∞≤‡±Å**: ‡∞â‡∞§‡±ç‡∞§‡∞Æ ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞≤ ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞ï‡±Ä‡∞µ‡∞∞‡±ç‡∞°‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞ï‡∞≤‡∞Ø‡∞ø‡∞ï  
‚úÖ **‡∞∏‡∞ø‡∞´‡∞æ‡∞∞‡±ç‡∞∏‡±Å ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡±ç‡∞•‡∞≤‡±Å**: ‡∞∏‡∞æ‡∞∞‡±Ç‡∞™‡±ç‡∞Ø‡∞§ ‡∞Ü‡∞ß‡∞æ‡∞∞‡∞ø‡∞§ AI-‡∞∂‡∞ï‡±ç‡∞§‡∞ø‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞® ‡∞â‡∞§‡±ç‡∞™‡∞§‡±ç‡∞§‡∞ø ‡∞∏‡∞ø‡∞´‡∞æ‡∞∞‡±ç‡∞∏‡±Å‡∞≤‡±Å  
‚úÖ **‡∞™‡∞®‡∞ø‡∞§‡±Ä‡∞∞‡±Å ‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç**: ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞∏‡±Ç‡∞ö‡∞ø‡∞ï ‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞§‡±Ü‡∞≤‡∞ø‡∞µ‡±à‡∞® ‡∞ï‡±ç‡∞Ø‡∞æ‡∞∑‡∞ø‡∞Ç‡∞ó‡±ç  
‚úÖ **‡∞∏‡±ç‡∞ï‡±á‡∞≤‡∞¨‡±Å‡∞≤‡±ç ‡∞Ü‡∞∞‡±ç‡∞ï‡∞ø‡∞ü‡±Ü‡∞ï‡±ç‡∞ö‡∞∞‡±ç**: ‡∞é‡∞Ç‡∞ü‡∞∞‡±ç‡∞™‡±ç‡∞∞‡±à‡∞ú‡±ç-‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Æ‡±à‡∞® ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞Æ‡±å‡∞≤‡∞ø‡∞ï ‡∞∏‡∞¶‡±Å‡∞™‡∞æ‡∞Ø‡∞æ‡∞≤‡±Å  

## üöÄ ‡∞§‡∞¶‡±Å‡∞™‡∞∞‡∞ø ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø

**[‡∞≤‡±ç‡∞Ø‡∞æ‡∞¨‡±ç 08: ‡∞ü‡±Ü‡∞∏‡±ç‡∞ü‡∞ø‡∞Ç‡∞ó‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞°‡±Ä‡∞¨‡∞ó‡±ç‡∞ó‡∞ø‡∞Ç‡∞ó‡±ç](../08-Testing/README.md)** ‡∞§‡±ã ‡∞ï‡±ä‡∞®‡∞∏‡∞æ‡∞ó‡∞Ç‡∞°‡∞ø:

- ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞∏‡∞Æ‡∞ó‡±ç‡∞∞ ‡∞ü‡±Ü‡∞∏‡±ç‡∞ü‡∞ø‡∞Ç‡∞ó‡±ç ‡∞µ‡±ç‡∞Ø‡±Ç‡∞π‡∞æ‡∞≤‡∞®‡±Å ‡∞Ö‡∞Æ‡∞≤‡±Å ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø  
- ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞™‡∞®‡∞ø‡∞§‡±Ä‡∞∞‡±Å ‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø‡∞≤‡∞®‡±Å ‡∞°‡±Ä‡∞¨‡∞ó‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø  
- ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞®‡∞æ‡∞£‡±ç‡∞Ø‡∞§ ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§‡∞§‡∞®‡±Å ‡∞ß‡±É‡∞µ‡±Ä‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø  
- ‡∞∏‡∞ø‡∞´‡∞æ‡∞∞‡±ç‡∞∏‡±Å ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡±ç‡∞• ‡∞ñ‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞§‡±ç‡∞µ‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞™‡∞∞‡±Ä‡∞ï‡±ç‡∞∑‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø  

## üìö ‡∞Ö‡∞¶‡∞®‡∞™‡±Å ‡∞µ‡∞®‡∞∞‡±Å‡∞≤‡±Å

### Azure OpenAI
- [Azure OpenAI ‡∞∏‡∞∞‡±ç‡∞µ‡±Ä‡∞∏‡±ç ‡∞°‡∞æ‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±Ü‡∞Ç‡∞ü‡±á‡∞∑‡∞®‡±ç](https://docs.microsoft.com/azure/cognitive-services/openai/) - ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞∏‡∞∞‡±ç‡∞µ‡±Ä‡∞∏‡±ç ‡∞ó‡±à‡∞°‡±ç  
- [‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç API ‡∞∞‡∞ø‡∞´‡∞∞‡±Ü‡∞®‡±ç‡∞∏‡±ç](https://platform.openai.com/docs/api-reference/embeddings) - API ‡∞°‡∞æ‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±Ü‡∞Ç‡∞ü‡±á‡∞∑‡∞®‡±ç  
- [‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞â‡∞§‡±ç‡∞§‡∞Æ ‡∞™‡∞¶‡±ç‡∞ß‡∞§‡±Å‡∞≤‡±Å](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) - ‡∞Ö‡∞Æ‡∞≤‡±Å ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ó‡∞¶‡∞∞‡±ç‡∞∂‡∞ï‡∞æ‡∞≤‡±Å  

### ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞°‡±á‡∞ü‡∞æ‡∞¨‡±á‡∞∏‡±Å‡∞≤‡±Å
- [pgvector ‡∞°‡∞æ‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±Ü‡∞Ç‡∞ü‡±á‡∞∑‡∞®‡±ç](https://github.com/pgvector/pgvector) - PostgreSQL ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞é‡∞ï‡±ç‡∞∏‡±ç‚Äå‡∞ü‡±Ü‡∞®‡±ç‡∞∑‡∞®‡±ç  
- [‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç](https://www.pinecone.io/learn/vector-search-optimization/) - ‡∞™‡∞®‡∞ø‡∞§‡±Ä‡∞∞‡±Å ‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ç‡∞ó‡±ç  
- [HNSW ‡∞Ö‡∞≤‡±ç‡∞ó‡±ã‡∞∞‡∞ø‡∞•‡∞Ç](https://arxiv.org/abs/1603.09320) - ‡∞π‡±à‡∞Ø‡∞∞‡∞æ‡∞∞‡±ç‡∞ï‡∞ø‡∞ï‡∞≤‡±ç ‡∞®‡∞æ‡∞µ‡∞ø‡∞ó‡∞¨‡±Å‡∞≤‡±ç ‡∞∏‡±ç‡∞Æ‡∞æ‡∞≤‡±ç ‡∞µ‡∞∞‡∞≤‡±ç‡∞°‡±ç ‡∞ó‡±ç‡∞∞‡∞æ‡∞´‡±ç‡∞∏‡±ç  

### ‡∞∏‡±á‡∞Æ‡∞æ‡∞Ç‡∞ü‡∞ø‡∞ï‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç
- [‡∞á‡∞®‡±ç‡∞´‡∞∞‡±ç‡∞Æ‡±á‡∞∑‡∞®‡±ç ‡∞∞‡∞ø‡∞ü‡±ç‡∞∞‡±Ä‡∞µ‡∞≤‡±ç ‡∞´‡∞Ç‡∞°‡∞Æ‡±Ü‡∞Ç‡∞ü‡∞≤‡±ç‡∞∏‡±ç](https://nlp.stanford.edu/IR-book/) - ‡∞∏‡±ç‡∞ü‡∞æ‡∞®‡±ç‡∞´‡∞∞‡±ç‡∞°‡±ç IR ‡∞™‡∞æ‡∞†‡±ç‡∞Ø‡∞™‡±Å‡∞∏‡±ç‡∞§‡∞ï‡∞Ç  
- [‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞â‡∞§‡±ç‡∞§‡∞Æ ‡∞™‡∞¶‡±ç‡∞ß‡∞§‡±Å‡∞≤‡±Å](https://weaviate.io/blog/vector-search-best-practices) - ‡∞Ö‡∞Æ‡∞≤‡±Å ‡∞®‡∞Æ‡±Ç‡∞®‡∞æ‡∞≤‡±Å  
- [‡∞π‡±à‡∞¨‡±ç‡∞∞‡∞ø‡∞°‡±ç ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞µ‡±ç‡∞Ø‡±Ç‡∞π‡∞æ‡∞≤‡±Å](https://blog.vespa.ai/hybrid-search/) - ‡∞µ‡±á‡∞∞‡±ç‡∞µ‡±á‡∞∞‡±Å ‡∞∏‡±Ü‡∞∞‡±ç‡∞ö‡±ç ‡∞µ‡∞ø‡∞ß‡∞æ‡∞®‡∞æ‡∞≤ ‡∞ï‡∞≤‡∞Ø‡∞ø‡∞ï  

---

**‡∞Æ‡±Å‡∞®‡±Å‡∞™‡∞ü‡∞ø**: [‡∞≤‡±ç‡∞Ø‡∞æ‡∞¨‡±ç 06: ‡∞ü‡±Ç‡∞≤‡±ç ‡∞°‡±Ü‡∞µ‡∞≤‡∞™‡±ç‚Äå‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç](../06-Tools/README.md)  
**‡∞§‡∞¶‡±Å‡∞™‡∞∞‡∞ø**: [‡∞≤‡±ç‡∞Ø‡∞æ‡∞¨‡±ç 08: ‡∞ü‡±Ü‡∞∏‡±ç‡∞ü‡∞ø‡∞Ç‡∞ó‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞°‡±Ä‡∞¨‡∞ó‡±ç‡∞ó‡∞ø‡∞Ç‡∞ó‡±ç](../08-Testing/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**‡∞Ö‡∞∏‡±ç‡∞™‡∞∑‡±ç‡∞ü‡∞§**:  
‡∞à ‡∞°‡∞æ‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç‚Äå‡∞®‡±Å AI ‡∞Ö‡∞®‡±Å‡∞µ‡∞æ‡∞¶ ‡∞∏‡±á‡∞µ [Co-op Translator](https://github.com/Azure/co-op-translator) ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞®‡±Å‡∞µ‡∞¶‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡∞ø‡∞Ç‡∞¶‡∞ø. ‡∞Æ‡±á‡∞Æ‡±Å ‡∞ñ‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞§‡±ç‡∞µ‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞ø‡∞®‡∞™‡±ç‡∞™‡∞ü‡∞ø‡∞ï‡±Ä, ‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡±Ü‡∞°‡±ç ‡∞Ö‡∞®‡±Å‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±ç‡∞≤‡±ã ‡∞™‡±ä‡∞∞‡∞™‡∞æ‡∞ü‡±ç‡∞≤‡±Å ‡∞≤‡±á‡∞¶‡∞æ ‡∞§‡∞™‡±ç‡∞™‡±Å‡∞≤‡±Å ‡∞â‡∞Ç‡∞°‡∞µ‡∞ö‡±ç‡∞ö‡±Å. ‡∞Æ‡±Ç‡∞≤ ‡∞°‡∞æ‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç ‡∞¶‡∞æ‡∞®‡∞ø ‡∞∏‡±ç‡∞µ‡∞¶‡±á‡∞∂‡±Ä ‡∞≠‡∞æ‡∞∑‡∞≤‡±ã ‡∞Ö‡∞ß‡∞ø‡∞ï‡∞æ‡∞∞‡∞ø‡∞ï ‡∞Æ‡±Ç‡∞≤‡∞Ç‡∞ó‡∞æ ‡∞™‡∞∞‡∞ø‡∞ó‡∞£‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞ø. ‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø‡∞Æ‡±à‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞æ‡∞®‡∞ø‡∞ï‡∞ø, ‡∞™‡±ç‡∞∞‡±ä‡∞´‡±Ü‡∞∑‡∞®‡∞≤‡±ç ‡∞Æ‡∞æ‡∞®‡∞µ ‡∞Ö‡∞®‡±Å‡∞µ‡∞æ‡∞¶‡∞Ç ‡∞∏‡∞ø‡∞´‡∞æ‡∞∞‡±ç‡∞∏‡±Å ‡∞ö‡±á‡∞Ø‡∞¨‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø. ‡∞à ‡∞Ö‡∞®‡±Å‡∞µ‡∞æ‡∞¶‡∞Ç ‡∞µ‡∞æ‡∞°‡∞ï‡∞Ç‡∞≤‡±ã ‡∞è‡∞∞‡±ç‡∞™‡∞°‡∞ø‡∞® ‡∞è‡∞µ‡±à‡∞®‡∞æ ‡∞Ö‡∞™‡∞æ‡∞∞‡±ç‡∞•‡∞æ‡∞≤‡±Å ‡∞≤‡±á‡∞¶‡∞æ ‡∞§‡∞™‡±ç‡∞™‡±Å‡∞¶‡∞æ‡∞∞‡∞ø‡∞§‡±Ä‡∞∏‡±á ‡∞Ö‡∞∞‡±ç‡∞•‡∞æ‡∞≤‡±Å ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞Æ‡±á‡∞Æ‡±Å ‡∞¨‡∞æ‡∞ß‡±ç‡∞Ø‡∞§ ‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞Æ‡±Å.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->