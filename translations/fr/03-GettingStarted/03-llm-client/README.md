# Cr√©ation d'un client avec LLM

Jusqu'√† pr√©sent, vous avez vu comment cr√©er un serveur et un client. Le client a pu appeler le serveur explicitement pour lister ses outils, ressources et invites. Cependant, ce n'est pas une approche tr√®s pratique. Votre utilisateur vit √† l'√®re agentique et s'attend √† utiliser des invites et √† communiquer avec un LLM pour ce faire. Pour votre utilisateur, peu importe que vous utilisiez MCP ou non pour stocker vos capacit√©s, mais il s'attend √† utiliser le langage naturel pour interagir. Alors, comment r√©soudre cela ? La solution consiste √† ajouter un LLM au client.

## Vue d'ensemble

Dans cette le√ßon, nous nous concentrons sur l'ajout d'un LLM √† votre client et montrons comment cela offre une bien meilleure exp√©rience √† votre utilisateur.

## Objectifs d'apprentissage

√Ä la fin de cette le√ßon, vous serez capable de :

- Cr√©er un client avec un LLM.
- Interagir ais√©ment avec un serveur MCP en utilisant un LLM.
- Offrir une meilleure exp√©rience utilisateur finale c√¥t√© client.

## Approche

Essayons de comprendre l'approche que nous devons adopter. Ajouter un LLM semble simple, mais allons-nous r√©ellement faire cela ?

Voici comment le client interagira avec le serveur :

1. √âtablir une connexion avec le serveur.

1. Lister les capacit√©s, invites, ressources et outils, et enregistrer leur sch√©ma.

1. Ajouter un LLM et transmettre les capacit√©s enregistr√©es et leur sch√©ma dans un format que le LLM comprend.

1. G√©rer une invite utilisateur en la passant au LLM avec les outils list√©s par le client.

Parfait, maintenant que nous comprenons comment faire cela de mani√®re g√©n√©rale, essayons-le dans l'exercice ci-dessous.

## Exercice : Cr√©ation d'un client avec un LLM

Dans cet exercice, nous allons apprendre √† ajouter un LLM √† notre client.

### Authentification avec un jeton d'acc√®s personnel GitHub

Cr√©er un jeton GitHub est un processus simple. Voici comment proc√©der :

- Allez dans Param√®tres GitHub ‚Äì Cliquez sur votre photo de profil en haut √† droite et s√©lectionnez Param√®tres.
- Naviguez vers Param√®tres du d√©veloppeur ‚Äì Faites d√©filer vers le bas et cliquez sur Param√®tres du d√©veloppeur.
- S√©lectionnez Jetons d'acc√®s personnel ‚Äì Cliquez sur Jetons √† granulom√©trie fine puis G√©n√©rer un nouveau jeton.
- Configurez votre jeton ‚Äì Ajoutez une note pour r√©f√©rence, d√©finissez une date d'expiration et s√©lectionnez les port√©es n√©cessaires (autorisations). Dans ce cas, assurez-vous d'ajouter l'autorisation Models.
- G√©n√©rez et copiez le jeton ‚Äì Cliquez sur G√©n√©rer le jeton, et assurez-vous de le copier imm√©diatement, car vous ne pourrez plus le voir.

### -1- Connexion au serveur

Cr√©ons d'abord notre client :

#### TypeScript

```typescript
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";
import { Transport } from "@modelcontextprotocol/sdk/shared/transport.js";
import OpenAI from "openai";
import { z } from "zod"; // Importer zod pour la validation du sch√©ma

class MCPClient {
    private openai: OpenAI;
    private client: Client;
    constructor(){
        this.openai = new OpenAI({
            baseURL: "https://models.inference.ai.azure.com", 
            apiKey: process.env.GITHUB_TOKEN,
        });

        this.client = new Client(
            {
                name: "example-client",
                version: "1.0.0"
            },
            {
                capabilities: {
                prompts: {},
                resources: {},
                tools: {}
                }
            }
            );    
    }
}
```

Dans le code pr√©c√©dent, nous avons :

- Import√© les biblioth√®ques n√©cessaires
- Cr√©√© une classe avec deux membres, `client` et `openai` qui nous aideront respectivement √† g√©rer un client et interagir avec un LLM.
- Configur√© notre instance LLM pour utiliser les mod√®les GitHub en r√©glant `baseUrl` pour pointer vers l'API d'inf√©rence.

#### Python

```python
from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client

# Cr√©er les param√®tres du serveur pour la connexion stdio
server_params = StdioServerParameters(
    command="mcp",  # Ex√©cutable
    args=["run", "server.py"],  # Arguments optionnels de la ligne de commande
    env=None,  # Variables d'environnement optionnelles
)


async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(
            read, write
        ) as session:
            # Initialiser la connexion
            await session.initialize()


if __name__ == "__main__":
    import asyncio

    asyncio.run(run())

```

Dans le code pr√©c√©dent, nous avons :

- Import√© les biblioth√®ques n√©cessaires pour MCP
- Cr√©√© un client

#### .NET

```csharp
using Azure;
using Azure.AI.Inference;
using Azure.Identity;
using System.Text.Json;
using ModelContextProtocol.Client;
using ModelContextProtocol.Protocol.Transport;
using System.Text.Json;

var clientTransport = new StdioClientTransport(new()
{
    Name = "Demo Server",
    Command = "/workspaces/mcp-for-beginners/03-GettingStarted/02-client/solution/server/bin/Debug/net8.0/server",
    Arguments = [],
});

await using var mcpClient = await McpClientFactory.CreateAsync(clientTransport);
```

#### Java

Tout d'abord, vous devez ajouter les d√©pendances LangChain4j √† votre fichier `pom.xml`. Ajoutez ces d√©pendances pour activer l'int√©gration MCP et le support des mod√®les GitHub :

```xml
<properties>
    <langchain4j.version>1.0.0-beta3</langchain4j.version>
</properties>

<dependencies>
    <!-- LangChain4j MCP Integration -->
    <dependency>
        <groupId>dev.langchain4j</groupId>
        <artifactId>langchain4j-mcp</artifactId>
        <version>${langchain4j.version}</version>
    </dependency>
    
    <!-- OpenAI Official API Client -->
    <dependency>
        <groupId>dev.langchain4j</groupId>
        <artifactId>langchain4j-open-ai-official</artifactId>
        <version>${langchain4j.version}</version>
    </dependency>
    
    <!-- GitHub Models Support -->
    <dependency>
        <groupId>dev.langchain4j</groupId>
        <artifactId>langchain4j-github-models</artifactId>
        <version>${langchain4j.version}</version>
    </dependency>
    
    <!-- Spring Boot Starter (optional, for production apps) -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
    </dependency>
</dependencies>
```

Puis cr√©ez votre classe client Java :

```java
import dev.langchain4j.mcp.McpToolProvider;
import dev.langchain4j.mcp.client.DefaultMcpClient;
import dev.langchain4j.mcp.client.McpClient;
import dev.langchain4j.mcp.client.transport.McpTransport;
import dev.langchain4j.mcp.client.transport.http.HttpMcpTransport;
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.openaiofficial.OpenAiOfficialChatModel;
import dev.langchain4j.service.AiServices;
import dev.langchain4j.service.tool.ToolProvider;

import java.time.Duration;
import java.util.List;

public class LangChain4jClient {
    
    public static void main(String[] args) throws Exception {        // Configurer le LLM pour utiliser les mod√®les GitHub
        ChatLanguageModel model = OpenAiOfficialChatModel.builder()
                .isGitHubModels(true)
                .apiKey(System.getenv("GITHUB_TOKEN"))
                .timeout(Duration.ofSeconds(60))
                .modelName("gpt-4.1-nano")
                .build();

        // Cr√©er un transport MCP pour se connecter au serveur
        McpTransport transport = new HttpMcpTransport.Builder()
                .sseUrl("http://localhost:8080/sse")
                .timeout(Duration.ofSeconds(60))
                .logRequests(true)
                .logResponses(true)
                .build();

        // Cr√©er un client MCP
        McpClient mcpClient = new DefaultMcpClient.Builder()
                .transport(transport)
                .build();
    }
}
```

Dans le code pr√©c√©dent, nous avons :

- **Ajout√© les d√©pendances LangChain4j** : n√©cessaires pour l'int√©gration MCP, le client officiel OpenAI, et le support des mod√®les GitHub
- **Import√© les biblioth√®ques LangChain4j** : pour l'int√©gration MCP et la fonctionnalit√© du mod√®le de chat OpenAI
- **Cr√©√© un `ChatLanguageModel`** : configur√© pour utiliser les mod√®les GitHub avec votre jeton GitHub
- **Mis en place le transport HTTP** : utilisant les Server-Sent Events (SSE) pour se connecter au serveur MCP
- **Cr√©√© un client MCP** : qui va g√©rer la communication avec le serveur
- **Utilis√© le support MCP int√©gr√© de LangChain4j** : qui simplifie l'int√©gration entre les LLM et les serveurs MCP

#### Rust

Cet exemple suppose que vous avez un serveur MCP bas√© sur Rust en fonctionnement. Si vous n'en avez pas, r√©f√©rez-vous √† la le√ßon [01-first-server](../01-first-server/README.md) pour cr√©er le serveur.

Une fois votre serveur MCP Rust pr√™t, ouvrez un terminal et naviguez vers le m√™me r√©pertoire que le serveur. Puis ex√©cutez la commande suivante pour cr√©er un nouveau projet client LLM :

```bash
mkdir calculator-llmclient
cd calculator-llmclient
cargo init
```

Ajoutez les d√©pendances suivantes √† votre fichier `Cargo.toml` :

```toml
[dependencies]
async-openai = { version = "0.29.0", features = ["byot"] }
rmcp = { version = "0.5.0", features = ["client", "transport-child-process"] }
serde_json = "1.0.141"
tokio = { version = "1.46.1", features = ["rt-multi-thread"] }
```

> [!NOTE]
> Il n'existe pas de biblioth√®que Rust officielle pour OpenAI, cependant, la crate `async-openai` est une [biblioth√®que maintenue par la communaut√©](https://platform.openai.com/docs/libraries/rust#rust) qui est couramment utilis√©e.

Ouvrez le fichier `src/main.rs` et remplacez son contenu par le code suivant :

```rust
use async_openai::{Client, config::OpenAIConfig};
use rmcp::{
    RmcpError,
    model::{CallToolRequestParam, ListToolsResult},
    service::{RoleClient, RunningService, ServiceExt},
    transport::{ConfigureCommandExt, TokioChildProcess},
};
use serde_json::{Value, json};
use std::error::Error;
use tokio::process::Command;

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    // Message initial
    let mut messages = vec![json!({"role": "user", "content": "What is the sum of 3 and 2?"})];

    // Configuration du client OpenAI
    let api_key = std::env::var("OPENAI_API_KEY")?;
    let openai_client = Client::with_config(
        OpenAIConfig::new()
            .with_api_base("https://models.github.ai/inference/chat")
            .with_api_key(api_key),
    );

    // Configuration du client MCP
    let server_dir = std::path::Path::new(env!("CARGO_MANIFEST_DIR"))
        .parent()
        .unwrap()
        .join("calculator-server");

    let mcp_client = ()
        .serve(
            TokioChildProcess::new(Command::new("cargo").configure(|cmd| {
                cmd.arg("run").current_dir(server_dir);
            }))
            .map_err(RmcpError::transport_creation::<TokioChildProcess>)?,
        )
        .await?;

    // √Ä FAIRE : Obtenir la liste des outils MCP

    // √Ä FAIRE : Conversation LLM avec appels aux outils

    Ok(())
}
```

Ce code configure une application Rust de base qui se connectera √† un serveur MCP et aux mod√®les GitHub pour les interactions avec le LLM.

> [!IMPORTANT]
> Assurez-vous de d√©finir la variable d'environnement `OPENAI_API_KEY` avec votre jeton GitHub avant de lancer l'application.

Parfait, pour notre prochaine √©tape, listons les capacit√©s du serveur.

### -2- Lister les capacit√©s du serveur

Nous allons maintenant nous connecter au serveur et demander ses capacit√©s :

#### Typescript

Dans la m√™me classe, ajoutez les m√©thodes suivantes :

```typescript
async connectToServer(transport: Transport) {
     await this.client.connect(transport);
     this.run();
     console.error("MCPClient started on stdin/stdout");
}

async run() {
    console.log("Asking server for available tools");

    // outils de liste
    const toolsResult = await this.client.listTools();
}
```

Dans le code pr√©c√©dent, nous avons :

- Ajout√© du code pour se connecter au serveur, `connectToServer`.
- Cr√©√© une m√©thode `run` responsable de g√©rer le flux de notre application. Jusqu'√† pr√©sent, elle liste uniquement les outils, mais nous y ajouterons bient√¥t plus de fonctionnalit√©s.

#### Python

```python
# Lister les ressources disponibles
resources = await session.list_resources()
print("LISTING RESOURCES")
for resource in resources:
    print("Resource: ", resource)

# Lister les outils disponibles
tools = await session.list_tools()
print("LISTING TOOLS")
for tool in tools.tools:
    print("Tool: ", tool.name)
    print("Tool", tool.inputSchema["properties"])
```

Voici ce que nous avons ajout√© :

- List√© les ressources et outils et les avons affich√©s. Pour les outils, nous listons √©galement le `inputSchema` que nous utiliserons plus tard.

#### .NET

```csharp
async Task<List<ChatCompletionsToolDefinition>> GetMcpTools()
{
    Console.WriteLine("Listing tools");
    var tools = await mcpClient.ListToolsAsync();

    List<ChatCompletionsToolDefinition> toolDefinitions = new List<ChatCompletionsToolDefinition>();

    foreach (var tool in tools)
    {
        Console.WriteLine($"Connected to server with tools: {tool.Name}");
        Console.WriteLine($"Tool description: {tool.Description}");
        Console.WriteLine($"Tool parameters: {tool.JsonSchema}");

        // TODO: convert tool definition from MCP tool to LLm tool     
    }

    return toolDefinitions;
}
```

Dans le code pr√©c√©dent, nous avons :

- List√© les outils disponibles sur le serveur MCP
- Pour chaque outil, list√© nom, description et son sch√©ma. Ce dernier est quelque chose que nous utiliserons pour appeler les outils prochainement.

#### Java

```java
// Cr√©ez un fournisseur d'outils qui d√©couvre automatiquement les outils MCP
ToolProvider toolProvider = McpToolProvider.builder()
        .mcpClients(List.of(mcpClient))
        .build();

// Le fournisseur d'outils MCP g√®re automatiquement :
// - La liste des outils disponibles depuis le serveur MCP
// - La conversion des sch√©mas d'outils MCP au format LangChain4j
// - La gestion de l'ex√©cution des outils et des r√©ponses
```

Dans le code pr√©c√©dent, nous avons :

- Cr√©√© un `McpToolProvider` qui d√©couvre automatiquement et enregistre tous les outils du serveur MCP
- Le fournisseur d'outils g√®re la conversion entre les sch√©mas d'outils MCP et le format d'outil de LangChain4j en interne
- Cette approche abstrait le processus manuel de listing et de conversion des outils

#### Rust

La r√©cup√©ration des outils depuis le serveur MCP se fait via la m√©thode `list_tools`. Dans votre fonction `main`, apr√®s avoir configur√© le client MCP, ajoutez le code suivant :

```rust
// Obtenir la liste des outils MCP
let tools = mcp_client.list_tools(Default::default()).await?;
```

### -3- Convertir les capacit√©s du serveur en outils LLM

L'√©tape suivante apr√®s avoir list√© les capacit√©s du serveur est de les convertir dans un format que le LLM comprend. Une fois que cela sera fait, nous pourrons fournir ces capacit√©s comme outils √† notre LLM.

#### TypeScript

1. Ajoutez le code suivant pour convertir la r√©ponse du serveur MCP en un format d'outil que le LLM peut utiliser :

    ```typescript
    openAiToolAdapter(tool: {
        name: string;
        description?: string;
        input_schema: any;
        }) {
        // Cr√©ez un sch√©ma zod bas√© sur le input_schema
        const schema = z.object(tool.input_schema);
    
        return {
            type: "function" as const, // D√©finissez explicitement le type sur "fonction"
            function: {
            name: tool.name,
            description: tool.description,
            parameters: {
            type: "object",
            properties: tool.input_schema.properties,
            required: tool.input_schema.required,
            },
            },
        };
    }

    ```

    Le code ci-dessus prend une r√©ponse du serveur MCP et la convertit en un format de d√©finition d'outil que le LLM peut comprendre.

1. Mettons √† jour la m√©thode `run` ensuite pour lister les capacit√©s du serveur :

    ```typescript
    async run() {
        console.log("Asking server for available tools");
        const toolsResult = await this.client.listTools();
        const tools = toolsResult.tools.map((tool) => {
            return this.openAiToolAdapter({
            name: tool.name,
            description: tool.description,
            input_schema: tool.inputSchema,
            });
        });
    }
    ```

    Dans le code pr√©c√©dent, nous avons mis √† jour la m√©thode `run` pour parcourir le r√©sultat et pour chaque entr√©e, appeler `openAiToolAdapter`.

#### Python

1. D'abord, cr√©ons la fonction convertisseur suivante

    ```python
    def convert_to_llm_tool(tool):
        tool_schema = {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "type": "function",
                "parameters": {
                    "type": "object",
                    "properties": tool.inputSchema["properties"]
                }
            }
        }

        return tool_schema
    ```

    Dans la fonction `convert_to_llm_tools` ci-dessus, nous prenons une r√©ponse d'outil MCP et la convertissons dans un format que le LLM peut comprendre.

1. Ensuite, mettons √† jour notre code client pour utiliser cette fonction ainsi :

    ```python
    functions = []
    for tool in tools.tools:
        print("Tool: ", tool.name)
        print("Tool", tool.inputSchema["properties"])
        functions.append(convert_to_llm_tool(tool))
    ```

    Ici, nous ajoutons un appel √† `convert_to_llm_tool` pour convertir la r√©ponse d'outil MCP en quelque chose que nous pourrons fournir au LLM plus tard.

#### .NET

1. Ajoutons du code pour convertir la r√©ponse d'outil MCP en quelque chose que le LLM peut comprendre

```csharp
ChatCompletionsToolDefinition ConvertFrom(string name, string description, JsonElement jsonElement)
{ 
    // convert the tool to a function definition
    FunctionDefinition functionDefinition = new FunctionDefinition(name)
    {
        Description = description,
        Parameters = BinaryData.FromObjectAsJson(new
        {
            Type = "object",
            Properties = jsonElement
        },
        new JsonSerializerOptions() { PropertyNamingPolicy = JsonNamingPolicy.CamelCase })
    };

    // create a tool definition
    ChatCompletionsToolDefinition toolDefinition = new ChatCompletionsToolDefinition(functionDefinition);
    return toolDefinition;
}
```

Dans le code pr√©c√©dent, nous avons :

- Cr√©√© une fonction `ConvertFrom` qui prend nom, description et sch√©ma d'entr√©e.
- D√©fini une fonctionnalit√© qui cr√©e une `FunctionDefinition` pass√©e √† une `ChatCompletionsDefinition`. Cette derni√®re est quelque chose que le LLM peut comprendre.

1. Voyons comment mettre √† jour un code existant pour profiter de cette fonction ci-dessus :

    ```csharp
    async Task<List<ChatCompletionsToolDefinition>> GetMcpTools()
    {
        Console.WriteLine("Listing tools");
        var tools = await mcpClient.ListToolsAsync();

        List<ChatCompletionsToolDefinition> toolDefinitions = new List<ChatCompletionsToolDefinition>();

        foreach (var tool in tools)
        {
            Console.WriteLine($"Connected to server with tools: {tool.Name}");
            Console.WriteLine($"Tool description: {tool.Description}");
            Console.WriteLine($"Tool parameters: {tool.JsonSchema}");

            JsonElement propertiesElement;
            tool.JsonSchema.TryGetProperty("properties", out propertiesElement);

            var def = ConvertFrom(tool.Name, tool.Description, propertiesElement);
            Console.WriteLine($"Tool definition: {def}");
            toolDefinitions.Add(def);

            Console.WriteLine($"Properties: {propertiesElement}");        
        }

        return toolDefinitions;
    }
    ```    In the preceding code, we've:

    - Update the function to convert the MCP tool response to an LLm tool. Let's highlight the code we added:

        ```csharp
        JsonElement propertiesElement;
        tool.JsonSchema.TryGetProperty("properties", out propertiesElement);

        var def = ConvertFrom(tool.Name, tool.Description, propertiesElement);
        Console.WriteLine($"Tool definition: {def}");
        toolDefinitions.Add(def);
        ```

        The input schema is part of the tool response but on the "properties" attribute, so we need to extract. Furthermore, we now call `ConvertFrom` with the tool details. Now we've done the heavy lifting, let's see how it call comes together as we handle a user prompt next.

#### Java

```java
// Cr√©er une interface Bot pour l'interaction en langage naturel
public interface Bot {
    String chat(String prompt);
}

// Configurer le service d'IA avec les outils LLM et MCP
Bot bot = AiServices.builder(Bot.class)
        .chatLanguageModel(model)
        .toolProvider(toolProvider)
        .build();
```

Dans le code pr√©c√©dent, nous avons :

- D√©fini une interface simple `Bot` pour les interactions en langage naturel
- Utilis√© `AiServices` de LangChain4j pour lier automatiquement le LLM avec le fournisseur d'outils MCP
- Le framework g√®re automatiquement la conversion des sch√©mas d'outils et l'appel des fonctions en arri√®re-plan
- Cette approche √©limine la conversion manuelle des outils ‚Äì LangChain4j s'occupe de toute la complexit√© de conversion des outils MCP au format compatible LLM

#### Rust

Pour convertir la r√©ponse d'outil MCP en un format que le LLM peut comprendre, nous allons ajouter une fonction utilitaire qui formate la liste des outils. Ajoutez le code suivant √† votre fichier `main.rs` sous la fonction `main`. Il sera appel√© lors des requ√™tes au LLM :

```rust
async fn format_tools(tools: &ListToolsResult) -> Result<Vec<Value>, Box<dyn Error>> {
    let tools_json = serde_json::to_value(tools)?;
    let Some(tools_array) = tools_json.get("tools").and_then(|t| t.as_array()) else {
        return Ok(vec![]);
    };

    let formatted_tools = tools_array
        .iter()
        .filter_map(|tool| {
            let name = tool.get("name")?.as_str()?;
            let description = tool.get("description")?.as_str()?;
            let schema = tool.get("inputSchema")?;

            Some(json!({
                "type": "function",
                "function": {
                    "name": name,
                    "description": description,
                    "parameters": {
                        "type": "object",
                        "properties": schema.get("properties").unwrap_or(&json!({})),
                        "required": schema.get("required").unwrap_or(&json!([]))
                    }
                }
            }))
        })
        .collect();

    Ok(formatted_tools)
}
```

Parfait, nous sommes maintenant pr√™ts √† g√©rer les demandes utilisateurs, attaquons cela.

### -4- G√©rer la demande d'invite utilisateur

Dans cette partie du code, nous allons g√©rer les demandes utilisateur.

#### TypeScript

1. Ajoutez une m√©thode qui sera utilis√©e pour appeler notre LLM :

    ```typescript
    async callTools(
        tool_calls: OpenAI.Chat.Completions.ChatCompletionMessageToolCall[],
        toolResults: any[]
    ) {
        for (const tool_call of tool_calls) {
        const toolName = tool_call.function.name;
        const args = tool_call.function.arguments;

        console.log(`Calling tool ${toolName} with args ${JSON.stringify(args)}`);


        // 2. Appelez l'outil du serveur
        const toolResult = await this.client.callTool({
            name: toolName,
            arguments: JSON.parse(args),
        });

        console.log("Tool result: ", toolResult);

        // 3. Faites quelque chose avec le r√©sultat
        // √Ä FAIRE

        }
    }
    ```

    Dans le code pr√©c√©dent, nous avons :

    - Ajout√© une m√©thode `callTools`.
    - La m√©thode prend une r√©ponse du LLM et v√©rifie quels outils ont √©t√© appel√©s, le cas √©ch√©ant :

        ```typescript
        for (const tool_call of tool_calls) {
        const toolName = tool_call.function.name;
        const args = tool_call.function.arguments;

        console.log(`Calling tool ${toolName} with args ${JSON.stringify(args)}`);

        // appeler l'outil
        }
        ```

    - Appelle un outil si le LLM indique qu'il doit √™tre appel√© :

        ```typescript
        // 2. Appeler l'outil du serveur
        const toolResult = await this.client.callTool({
            name: toolName,
            arguments: JSON.parse(args),
        });

        console.log("Tool result: ", toolResult);

        // 3. Faire quelque chose avec le r√©sultat
        // √Ä FAIRE
        ```

1. Mettez √† jour la m√©thode `run` pour inclure des appels au LLM et appeler `callTools` :

    ```typescript

    // 1. Cr√©ez des messages qui sont une entr√©e pour le LLM
    const prompt = "What is the sum of 2 and 3?"

    const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
            {
                role: "user",
                content: prompt,
            },
        ];

    console.log("Querying LLM: ", messages[0].content);

    // 2. Appeler le LLM
    let response = this.openai.chat.completions.create({
        model: "gpt-4.1-mini",
        max_tokens: 1000,
        messages,
        tools: tools,
    });    

    let results: any[] = [];

    // 3. Parcourez la r√©ponse du LLM, pour chaque choix, v√©rifiez s'il contient des appels d'outil
    (await response).choices.map(async (choice: { message: any; }) => {
        const message = choice.message;
        if (message.tool_calls) {
            console.log("Making tool call")
            await this.callTools(message.tool_calls, results);
        }
    });
    ```

Parfait, listons maintenant le code complet :

```typescript
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";
import { Transport } from "@modelcontextprotocol/sdk/shared/transport.js";
import OpenAI from "openai";
import { z } from "zod"; // Importer zod pour la validation du sch√©ma

class MyClient {
    private openai: OpenAI;
    private client: Client;
    constructor(){
        this.openai = new OpenAI({
            baseURL: "https://models.inference.ai.azure.com", // pourrait devoir changer pour cette URL √† l'avenir : https://models.github.ai/inference
            apiKey: process.env.GITHUB_TOKEN,
        });

        this.client = new Client(
            {
                name: "example-client",
                version: "1.0.0"
            },
            {
                capabilities: {
                prompts: {},
                resources: {},
                tools: {}
                }
            }
            );    
    }

    async connectToServer(transport: Transport) {
        await this.client.connect(transport);
        this.run();
        console.error("MCPClient started on stdin/stdout");
    }

    openAiToolAdapter(tool: {
        name: string;
        description?: string;
        input_schema: any;
          }) {
          // Cr√©er un sch√©ma zod bas√© sur le input_schema
          const schema = z.object(tool.input_schema);
      
          return {
            type: "function" as const, // D√©finir explicitement le type √† "function"
            function: {
              name: tool.name,
              description: tool.description,
              parameters: {
              type: "object",
              properties: tool.input_schema.properties,
              required: tool.input_schema.required,
              },
            },
          };
    }
    
    async callTools(
        tool_calls: OpenAI.Chat.Completions.ChatCompletionMessageToolCall[],
        toolResults: any[]
      ) {
        for (const tool_call of tool_calls) {
          const toolName = tool_call.function.name;
          const args = tool_call.function.arguments;
    
          console.log(`Calling tool ${toolName} with args ${JSON.stringify(args)}`);
    
    
          // 2. Appeler l'outil du serveur
          const toolResult = await this.client.callTool({
            name: toolName,
            arguments: JSON.parse(args),
          });
    
          console.log("Tool result: ", toolResult);
    
          // 3. Faire quelque chose avec le r√©sultat
          // √Ä FAIRE
    
         }
    }

    async run() {
        console.log("Asking server for available tools");
        const toolsResult = await this.client.listTools();
        const tools = toolsResult.tools.map((tool) => {
            return this.openAiToolAdapter({
              name: tool.name,
              description: tool.description,
              input_schema: tool.inputSchema,
            });
        });

        const prompt = "What is the sum of 2 and 3?";
    
        const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
            {
                role: "user",
                content: prompt,
            },
        ];

        console.log("Querying LLM: ", messages[0].content);
        let response = this.openai.chat.completions.create({
            model: "gpt-4.1-mini",
            max_tokens: 1000,
            messages,
            tools: tools,
        });    

        let results: any[] = [];
    
        // 1. Parcourir la r√©ponse LLM, pour chaque choix, v√©rifier s'il y a des appels d'outil
        (await response).choices.map(async (choice: { message: any; }) => {
          const message = choice.message;
          if (message.tool_calls) {
              console.log("Making tool call")
              await this.callTools(message.tool_calls, results);
          }
        });
    }
    
}

let client = new MyClient();
 const transport = new StdioClientTransport({
            command: "node",
            args: ["./build/index.js"]
        });

client.connectToServer(transport);
```

#### Python

1. Ajoutons quelques imports n√©cessaires pour appeler un LLM

    ```python
    # llm
    import os
    from azure.ai.inference import ChatCompletionsClient
    from azure.ai.inference.models import SystemMessage, UserMessage
    from azure.core.credentials import AzureKeyCredential
    import json
    ```

1. Ensuite, ajoutons la fonction qui appellera le LLM :

    ```python
    # llm

    def call_llm(prompt, functions):
        token = os.environ["GITHUB_TOKEN"]
        endpoint = "https://models.inference.ai.azure.com"

        model_name = "gpt-4o"

        client = ChatCompletionsClient(
            endpoint=endpoint,
            credential=AzureKeyCredential(token),
        )

        print("CALLING LLM")
        response = client.complete(
            messages=[
                {
                "role": "system",
                "content": "You are a helpful assistant.",
                },
                {
                "role": "user",
                "content": prompt,
                },
            ],
            model=model_name,
            tools = functions,
            # Param√®tres optionnels
            temperature=1.,
            max_tokens=1000,
            top_p=1.    
        )

        response_message = response.choices[0].message
        
        functions_to_call = []

        if response_message.tool_calls:
            for tool_call in response_message.tool_calls:
                print("TOOL: ", tool_call)
                name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                functions_to_call.append({ "name": name, "args": args })

        return functions_to_call
    ```

    Dans le code pr√©c√©dent, nous avons :

    - Pass√© nos fonctions, trouv√©es sur le serveur MCP et converties, au LLM.
    - Puis appel√© le LLM avec ces fonctions.
    - Inspect√© le r√©sultat pour voir quelles fonctions nous devrions appeler, si besoin.
    - Enfin, nous passons un tableau de fonctions √† appeler.

1. Derni√®re √©tape, mettons √† jour notre code principal :

    ```python
    prompt = "Add 2 to 20"

    # demander au LLM quels outils utiliser, si jamais
    functions_to_call = call_llm(prompt, functions)

    # appeler les fonctions sugg√©r√©es
    for f in functions_to_call:
        result = await session.call_tool(f["name"], arguments=f["args"])
        print("TOOLS result: ", result.content)
    ```

    Voil√†, c'√©tait la derni√®re √©tape, dans le code ci-dessus nous :

    - Appelons un outil MCP via `call_tool` en utilisant une fonction que le LLM a pens√© que nous devrions appeler selon notre invite.
    - Affichons le r√©sultat de l'appel d'outil au serveur MCP.

#### .NET

1. Voici un exemple de code pour faire une requ√™te d'invite LLM :

    ```csharp
    var tools = await GetMcpTools();

    for (int i = 0; i < tools.Count; i++)
    {
        var tool = tools[i];
        Console.WriteLine($"MCP Tools def: {i}: {tool}");
    }

    // 0. Define the chat history and the user message
    var userMessage = "add 2 and 4";

    chatHistory.Add(new ChatRequestUserMessage(userMessage));

    // 1. Define tools
    ChatCompletionsToolDefinition def = CreateToolDefinition();


    // 2. Define options, including the tools
    var options = new ChatCompletionsOptions(chatHistory)
    {
        Model = "gpt-4.1-mini",
        Tools = { tools[0] }
    };

    // 3. Call the model  

    ChatCompletions? response = await client.CompleteAsync(options);
    var content = response.Content;

    ```

    Dans le code pr√©c√©dent, nous avons :

    - R√©cup√©r√© les outils du serveur MCP, `var tools = await GetMcpTools()`.
    - D√©fini une invite utilisateur `userMessage`.
    - Construit un objet options sp√©cifiant le mod√®le et les outils.
    - Effectu√© une requ√™te vers le LLM.

1. Une derni√®re √©tape, voyons si le LLM pense que nous devons appeler une fonction :

    ```csharp
    // 4. Check if the response contains a function call
    ChatCompletionsToolCall? calls = response.ToolCalls.FirstOrDefault();
    for (int i = 0; i < response.ToolCalls.Count; i++)
    {
        var call = response.ToolCalls[i];
        Console.WriteLine($"Tool call {i}: {call.Name} with arguments {call.Arguments}");
        //Tool call 0: add with arguments {"a":2,"b":4}

        var dict = JsonSerializer.Deserialize<Dictionary<string, object>>(call.Arguments);
        var result = await mcpClient.CallToolAsync(
            call.Name,
            dict!,
            cancellationToken: CancellationToken.None
        );

        Console.WriteLine(result.Content.First(c => c.Type == "text").Text);

    }
    ```

    Dans le code pr√©c√©dent, nous avons :

    - Parcouru une liste d'appels de fonctions.
    - Pour chaque appel d'outil, extrait le nom et les arguments, puis appel√© l'outil sur le serveur MCP en utilisant le client MCP. Enfin, nous affichons les r√©sultats.

Voici le code complet :

```csharp
using Azure;
using Azure.AI.Inference;
using Azure.Identity;
using System.Text.Json;
using ModelContextProtocol.Client;
using ModelContextProtocol.Protocol.Transport;
using System.Text.Json;

var endpoint = "https://models.inference.ai.azure.com";
var token = Environment.GetEnvironmentVariable("GITHUB_TOKEN"); // Your GitHub Access Token
var client = new ChatCompletionsClient(new Uri(endpoint), new AzureKeyCredential(token));
var chatHistory = new List<ChatRequestMessage>
{
    new ChatRequestSystemMessage("You are a helpful assistant that knows about AI")
};

var clientTransport = new StdioClientTransport(new()
{
    Name = "Demo Server",
    Command = "/workspaces/mcp-for-beginners/03-GettingStarted/02-client/solution/server/bin/Debug/net8.0/server",
    Arguments = [],
});

Console.WriteLine("Setting up stdio transport");

await using var mcpClient = await McpClientFactory.CreateAsync(clientTransport);

ChatCompletionsToolDefinition ConvertFrom(string name, string description, JsonElement jsonElement)
{ 
    // convert the tool to a function definition
    FunctionDefinition functionDefinition = new FunctionDefinition(name)
    {
        Description = description,
        Parameters = BinaryData.FromObjectAsJson(new
        {
            Type = "object",
            Properties = jsonElement
        },
        new JsonSerializerOptions() { PropertyNamingPolicy = JsonNamingPolicy.CamelCase })
    };

    // create a tool definition
    ChatCompletionsToolDefinition toolDefinition = new ChatCompletionsToolDefinition(functionDefinition);
    return toolDefinition;
}



async Task<List<ChatCompletionsToolDefinition>> GetMcpTools()
{
    Console.WriteLine("Listing tools");
    var tools = await mcpClient.ListToolsAsync();

    List<ChatCompletionsToolDefinition> toolDefinitions = new List<ChatCompletionsToolDefinition>();

    foreach (var tool in tools)
    {
        Console.WriteLine($"Connected to server with tools: {tool.Name}");
        Console.WriteLine($"Tool description: {tool.Description}");
        Console.WriteLine($"Tool parameters: {tool.JsonSchema}");

        JsonElement propertiesElement;
        tool.JsonSchema.TryGetProperty("properties", out propertiesElement);

        var def = ConvertFrom(tool.Name, tool.Description, propertiesElement);
        Console.WriteLine($"Tool definition: {def}");
        toolDefinitions.Add(def);

        Console.WriteLine($"Properties: {propertiesElement}");        
    }

    return toolDefinitions;
}

// 1. List tools on mcp server

var tools = await GetMcpTools();
for (int i = 0; i < tools.Count; i++)
{
    var tool = tools[i];
    Console.WriteLine($"MCP Tools def: {i}: {tool}");
}

// 2. Define the chat history and the user message
var userMessage = "add 2 and 4";

chatHistory.Add(new ChatRequestUserMessage(userMessage));


// 3. Define options, including the tools
var options = new ChatCompletionsOptions(chatHistory)
{
    Model = "gpt-4.1-mini",
    Tools = { tools[0] }
};

// 4. Call the model  

ChatCompletions? response = await client.CompleteAsync(options);
var content = response.Content;

// 5. Check if the response contains a function call
ChatCompletionsToolCall? calls = response.ToolCalls.FirstOrDefault();
for (int i = 0; i < response.ToolCalls.Count; i++)
{
    var call = response.ToolCalls[i];
    Console.WriteLine($"Tool call {i}: {call.Name} with arguments {call.Arguments}");
    //Tool call 0: add with arguments {"a":2,"b":4}

    var dict = JsonSerializer.Deserialize<Dictionary<string, object>>(call.Arguments);
    var result = await mcpClient.CallToolAsync(
        call.Name,
        dict!,
        cancellationToken: CancellationToken.None
    );

    Console.WriteLine(result.Content.First(c => c.Type == "text").Text);

}

// 5. Print the generic response
Console.WriteLine($"Assistant response: {content}");
```

#### Java

```java
try {
    // Ex√©cuter des requ√™tes en langage naturel qui utilisent automatiquement les outils MCP
    String response = bot.chat("Calculate the sum of 24.5 and 17.3 using the calculator service");
    System.out.println(response);

    response = bot.chat("What's the square root of 144?");
    System.out.println(response);

    response = bot.chat("Show me the help for the calculator service");
    System.out.println(response);
} finally {
    mcpClient.close();
}
```

Dans le code pr√©c√©dent, nous avons :

- Utilis√© des invites en langage naturel simples pour interagir avec les outils du serveur MCP
- Le framework LangChain4j g√®re automatiquement :
  - La conversion des invites utilisateur en appels d'outils lorsque n√©cessaire
  - L'appel des outils MCP appropri√©s bas√© sur la d√©cision du LLM
  - La gestion du flux de conversation entre le LLM et le serveur MCP
- La m√©thode `bot.chat()` retourne des r√©ponses en langage naturel pouvant inclure les r√©sultats d'ex√©cutions d'outils MCP
- Cette approche fournit une exp√©rience utilisateur fluide o√π les utilisateurs n'ont pas besoin de conna√Ætre l'impl√©mentation MCP sous-jacente

Exemple de code complet :

```java
public class LangChain4jClient {
    
    public static void main(String[] args) throws Exception {        ChatLanguageModel model = OpenAiOfficialChatModel.builder()
                .isGitHubModels(true)
                .apiKey(System.getenv("GITHUB_TOKEN"))
                .timeout(Duration.ofSeconds(60))
                .modelName("gpt-4.1-nano")
                .timeout(Duration.ofSeconds(60))
                .build();

        McpTransport transport = new HttpMcpTransport.Builder()
                .sseUrl("http://localhost:8080/sse")
                .timeout(Duration.ofSeconds(60))
                .logRequests(true)
                .logResponses(true)
                .build();

        McpClient mcpClient = new DefaultMcpClient.Builder()
                .transport(transport)
                .build();

        ToolProvider toolProvider = McpToolProvider.builder()
                .mcpClients(List.of(mcpClient))
                .build();

        Bot bot = AiServices.builder(Bot.class)
                .chatLanguageModel(model)
                .toolProvider(toolProvider)
                .build();

        try {
            String response = bot.chat("Calculate the sum of 24.5 and 17.3 using the calculator service");
            System.out.println(response);

            response = bot.chat("What's the square root of 144?");
            System.out.println(response);

            response = bot.chat("Show me the help for the calculator service");
            System.out.println(response);
        } finally {
            mcpClient.close();
        }
    }
}
```

#### Rust

C'est ici que la majeure partie du travail se fait. Nous appellerons le LLM avec l'invite utilisateur initiale, puis nous traiterons la r√©ponse pour voir si des outils doivent √™tre appel√©s. Le cas √©ch√©ant, nous appellerons ces outils et continuerons la conversation avec le LLM jusqu'√† ce qu'aucun appel d'outil ne soit n√©cessaire et que nous ayons une r√©ponse finale.

Nous allons faire plusieurs appels au LLM, alors d√©finissons une fonction qui g√©rera l'appel au LLM. Ajoutez la fonction suivante √† votre `main.rs` :

```rust
async fn call_llm(
    client: &Client<OpenAIConfig>,
    messages: &[Value],
    tools: &ListToolsResult,
) -> Result<Value, Box<dyn Error>> {
    let response = client
        .completions()
        .create_byot(json!({
            "messages": messages,
            "model": "openai/gpt-4.1",
            "tools": format_tools(tools).await?,
        }))
        .await?;
    Ok(response)
}
```

Cette fonction prend le client LLM, une liste de messages (y compris l'invite utilisateur), les outils du serveur MCP, et envoie une requ√™te au LLM, retournant la r√©ponse.
La r√©ponse du LLM contiendra un tableau de `choices`. Nous devrons traiter le r√©sultat pour voir si des `tool_calls` sont pr√©sents. Cela nous permet de savoir que le LLM demande qu‚Äôun outil sp√©cifique soit appel√© avec des arguments. Ajoutez le code suivant √† la fin de votre fichier `main.rs` pour d√©finir une fonction qui g√®re la r√©ponse du LLM :

```rust
async fn process_llm_response(
    llm_response: &Value,
    mcp_client: &RunningService<RoleClient, ()>,
    openai_client: &Client<OpenAIConfig>,
    mcp_tools: &ListToolsResult,
    messages: &mut Vec<Value>,
) -> Result<(), Box<dyn Error>> {
    let Some(message) = llm_response
        .get("choices")
        .and_then(|c| c.as_array())
        .and_then(|choices| choices.first())
        .and_then(|choice| choice.get("message"))
    else {
        return Ok(());
    };

    // Imprimer le contenu s'il est disponible
    if let Some(content) = message.get("content").and_then(|c| c.as_str()) {
        println!("ü§ñ {}", content);
    }

    // G√©rer les appels d'outils
    if let Some(tool_calls) = message.get("tool_calls").and_then(|tc| tc.as_array()) {
        messages.push(message.clone()); // Ajouter un message de l'assistant

        // Ex√©cuter chaque appel d'outil
        for tool_call in tool_calls {
            let (tool_id, name, args) = extract_tool_call_info(tool_call)?;
            println!("‚ö° Calling tool: {}", name);

            let result = mcp_client
                .call_tool(CallToolRequestParam {
                    name: name.into(),
                    arguments: serde_json::from_str::<Value>(&args)?.as_object().cloned(),
                })
                .await?;

            // Ajouter le r√©sultat de l'outil aux messages
            messages.push(json!({
                "role": "tool",
                "tool_call_id": tool_id,
                "content": serde_json::to_string_pretty(&result)?
            }));
        }

        // Continuer la conversation avec les r√©sultats de l'outil
        let response = call_llm(openai_client, messages, mcp_tools).await?;
        Box::pin(process_llm_response(
            &response,
            mcp_client,
            openai_client,
            mcp_tools,
            messages,
        ))
        .await?;
    }
    Ok(())
}
```

Si des `tool_calls` sont pr√©sents, il extrait les informations de l‚Äôoutil, appelle le serveur MCP avec la requ√™te de l‚Äôoutil, et ajoute les r√©sultats aux messages de la conversation. Il continue ensuite la conversation avec le LLM et les messages sont mis √† jour avec la r√©ponse de l‚Äôassistant et les r√©sultats de l‚Äôappel de l‚Äôoutil.

Pour extraire les informations d‚Äôappel des outils que le LLM retourne pour les appels MCP, nous allons ajouter une autre fonction d‚Äôaide pour extraire tout ce qui est n√©cessaire pour effectuer l‚Äôappel. Ajoutez le code suivant √† la fin de votre fichier `main.rs` :

```rust
fn extract_tool_call_info(tool_call: &Value) -> Result<(String, String, String), Box<dyn Error>> {
    let tool_id = tool_call
        .get("id")
        .and_then(|id| id.as_str())
        .unwrap_or("")
        .to_string();
    let function = tool_call.get("function").ok_or("Missing function")?;
    let name = function
        .get("name")
        .and_then(|n| n.as_str())
        .unwrap_or("")
        .to_string();
    let args = function
        .get("arguments")
        .and_then(|a| a.as_str())
        .unwrap_or("{}")
        .to_string();
    Ok((tool_id, name, args))
}
```

Avec tous les √©l√©ments en place, nous pouvons maintenant g√©rer la requ√™te initiale de l‚Äôutilisateur et appeler le LLM. Mettez √† jour votre fonction `main` pour inclure le code suivant :

```rust
// Conversation LLM avec appels d'outils
let response = call_llm(&openai_client, &messages, &tools).await?;
process_llm_response(
    &response,
    &mcp_client,
    &openai_client,
    &tools,
    &mut messages,
)
.await?;
```

Cela interrogera le LLM avec la requ√™te initiale de l‚Äôutilisateur demandant la somme de deux nombres, et traitera la r√©ponse pour g√©rer dynamiquement les appels d‚Äôoutils.

Bravo, vous l‚Äôavez fait !

## Exercice

Reprenez le code de l‚Äôexercice et d√©veloppez le serveur avec d‚Äôautres outils. Puis cr√©ez un client avec un LLM, comme dans l‚Äôexercice, et testez-le avec diff√©rentes requ√™tes pour vous assurer que tous vos outils serveur sont appel√©s dynamiquement. Cette mani√®re de construire un client permet √† l‚Äôutilisateur final d‚Äôavoir une excellente exp√©rience, car il peut utiliser des requ√™tes au lieu de commandes clients exactes, sans se rendre compte qu‚Äôun serveur MCP est appel√©.

## Solution

[Solution](/03-GettingStarted/03-llm-client/solution/README.md)

## Points Cl√©s

- Ajouter un LLM √† votre client offre une meilleure fa√ßon pour les utilisateurs d‚Äôinteragir avec les serveurs MCP.
- Vous devez convertir la r√©ponse du serveur MCP en quelque chose que le LLM peut comprendre.

## Exemples

- [Calculatrice Java](../samples/java/calculator/README.md)
- [Calculatrice .Net](../../../../03-GettingStarted/samples/csharp)
- [Calculatrice JavaScript](../samples/javascript/README.md)
- [Calculatrice TypeScript](../samples/typescript/README.md)
- [Calculatrice Python](../../../../03-GettingStarted/samples/python)
- [Calculatrice Rust](../../../../03-GettingStarted/samples/rust)

## Ressources Suppl√©mentaires

## Et Apr√®s

- Suivant : [Consommer un serveur avec Visual Studio Code](../04-vscode/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d‚Äôinterpr√©tations erron√©es r√©sultant de l‚Äôutilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->