<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad02c1223d7861292651ffce2f52bb28",
  "translation_date": "2025-12-11T14:31:15+00:00",
  "source_file": "11-MCPServerHandsOnLabs/08-Testing/README.md",
  "language_code": "kn"
}
-->
# ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç

## üéØ ‡≤à ‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ó‡≤∂‡≤æ‡≤≤‡≥Ü ‡≤è‡≤®‡≥Å ‡≤í‡≤≥‡≤ó‡≥ä‡≤Ç‡≤°‡≤ø‡≤¶‡≥Ü

‡≤à ‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ó‡≤∂‡≤æ‡≤≤‡≥Ü ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≤æ ‡≤™‡≤∞‡≤ø‡≤∏‡≤∞‡≤ó‡≤≥‡≤≤‡≥ç‡≤≤‡≤ø MCP ‡≤∏‡≤∞‡≥ç‡≤µ‡≤∞‡≥ç‚Äå‡≤ó‡≤≥ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤ï‡≥Å‡≤∞‡≤ø‡≤§‡≥Å ‡≤∏‡≤Æ‡≤ó‡≥ç‡≤∞ ‡≤Æ‡≤æ‡≤∞‡≥ç‡≤ó‡≤¶‡≤∞‡≥ç‡≤∂‡≤®‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤í‡≤¶‡≤ó‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü. ‡≤®‡≥Ä‡≤µ‡≥Å ‡≤¨‡≤≤‡≤µ‡≤æ‡≤¶ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤§‡≤Ç‡≤§‡≥ç‡≤∞‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤ú‡≤æ‡≤∞‡≤ø‡≤ó‡≥Ü ‡≤§‡≤∞‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å, ‡≤∏‡≤Ç‡≤ï‡≥Ä‡≤∞‡≥ç‡≤£ ‡≤∏‡≤Æ‡≤∏‡≥ç‡≤Ø‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤°‡≤ø‡≤¨‡≤ó‡≥ç ‡≤Æ‡≤æ‡≤°‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤®‡≤ø‡≤Æ‡≥ç‡≤Æ MCP ‡≤∏‡≤∞‡≥ç‡≤µ‡≤∞‡≥ç ‡≤µ‡≤ø‡≤µ‡≤ø‡≤ß ‡≤™‡≤∞‡≤ø‡≤∏‡≥ç‡≤•‡≤ø‡≤§‡≤ø‡≤ó‡≤≥‡≤≤‡≥ç‡≤≤‡≤ø ‡≤µ‡≤ø‡≤∂‡≥ç‡≤µ‡≤æ‡≤∏‡≤æ‡≤∞‡≥ç‡≤π‡≤µ‡≤æ‡≤ó‡≤ø ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤ø‡≤∏‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å ‡≤ï‡≤≤‡≤ø‡≤Ø‡≥Å‡≤§‡≥ç‡≤§‡≥Ä‡≤∞‡≤ø.

## ‡≤Ö‡≤µ‡≤≤‡≥ã‡≤ï‡≤®

MCP ‡≤∏‡≤∞‡≥ç‡≤µ‡≤∞‡≥ç‚Äå‡≤ó‡≤≥ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü‡≤ó‡≥Ü ‡≤ò‡≤ü‡≤ï ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü‡≤ó‡≤≥‡≥Å, ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü‡≤ó‡≤≥‡≥Å, ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤Æ‡≤æ‡≤®‡≥ç‡≤Ø‡≤§‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤®‡≥à‡≤ú ‡≤ú‡≤ó‡≤§‡≥ç‡≤§‡≤ø‡≤® ‡≤¶‡≥É‡≤∂‡≥ç‡≤Ø‡≤™‡≤ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤í‡≤≥‡≤ó‡≥ä‡≤Ç‡≤° ‡≤¨‡≤π‡≥Å‡≤Æ‡≤ü‡≥ç‡≤ü‡≤¶ ‡≤µ‡≤ø‡≤ß‡≤æ‡≤® ‡≤Ö‡≤ó‡≤§‡≥ç‡≤Ø‡≤µ‡≤ø‡≤¶‡≥Ü. ‡≤à ‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ó‡≤∂‡≤æ‡≤≤‡≥Ü ‡≤Ö‡≤≠‡≤ø‡≤µ‡≥É‡≤¶‡≥ç‡≤ß‡≤ø‡≤Ø‡≤ø‡≤Ç‡≤¶ ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≤æ ‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤£‡≥Ü‡≤Ø‡≤µ‡≤∞‡≥Ü‡≤ó‡≥Ü ‡≤∏‡≤Ç‡≤™‡≥Ç‡≤∞‡≥ç‡≤£ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤ú‡≥Ä‡≤µ‡≤®‡≤ö‡≤ï‡≥ç‡≤∞‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤í‡≤≥‡≤ó‡≥ä‡≤Ç‡≤°‡≤ø‡≤¶‡≥Ü.

‡≤®‡≤Æ‡≥ç‡≤Æ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤§‡≤Ç‡≤§‡≥ç‡≤∞‡≤µ‡≥Å ‡≤µ‡≤ø‡≤∂‡≥ç‡≤µ‡≤æ‡≤∏‡≤æ‡≤∞‡≥ç‡≤π‡≤§‡≥Ü, ‡≤≠‡≤¶‡≥ç‡≤∞‡≤§‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤í‡≤§‡≥ç‡≤§‡≤æ‡≤Ø‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü, ‡≤®‡≤ø‡≤Æ‡≥ç‡≤Æ MCP ‡≤∏‡≤∞‡≥ç‡≤µ‡≤∞‡≥ç ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≤æ ‡≤ï‡≥Ü‡≤≤‡≤∏‡≤≠‡≤æ‡≤∞‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤ø‡≤∏‡≥Å‡≤µ‡≤æ‡≤ó ‡≤°‡≥á‡≤ü‡≤æ ‡≤Ö‡≤ñ‡≤Ç‡≤°‡≤§‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤¶‡≤æ‡≤∞ ‡≤Ö‡≤®‡≥Å‡≤≠‡≤µ ‡≤ó‡≥Å‡≤£‡≤Æ‡≤ü‡≥ç‡≤ü‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤ï‡≤æ‡≤™‡≤æ‡≤°‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü.

## ‡≤ï‡≤≤‡≤ø‡≤ï‡≥Ü‡≤Ø ‡≤â‡≤¶‡≥ç‡≤¶‡≥á‡≤∂‡≤ó‡≤≥‡≥Å

‡≤à ‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ó‡≤∂‡≤æ‡≤≤‡≥Ü‡≤Ø ‡≤Ö‡≤Ç‡≤§‡≥ç‡≤Ø‡≤ï‡≥ç‡≤ï‡≥Ü, ‡≤®‡≥Ä‡≤µ‡≥Å ‡≤∏‡≤æ‡≤ß‡≥ç‡≤Ø‡≤µ‡≤æ‡≤ó‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü:

- **‡≤∏‡≤Ç‡≤™‡≥Ç‡≤∞‡≥ç‡≤£ ‡≤ò‡≤ü‡≤ï ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤∏‡≥ç‡≤Ø‡≥Ç‡≤ü‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤ú‡≤æ‡≤∞‡≤ø‡≤ó‡≥Ü ‡≤§‡≤∞‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å**  
- **MCP ‡≤â‡≤™‡≤ï‡≤∞‡≤£‡≤ó‡≤≥‡≥Å ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤æ‡≤ö‡≤∞‡≤£‡≥Ü‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü ‡≤™‡≤∞‡≤ø‡≤£‡≤æ‡≤Æ‡≤ï‡≤æ‡≤∞‡≤ø ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤§‡≤Ç‡≤§‡≥ç‡≤∞‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤µ‡≤ø‡≤®‡≥ç‡≤Ø‡≤æ‡≤∏‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å**  
- **‡≤Ö‡≤§‡≥ç‡≤Ø‡≤æ‡≤ß‡≥Å‡≤®‡≤ø‡≤ï ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤§‡≤Ç‡≤§‡≥ç‡≤∞‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤¨‡≤≥‡≤∏‡≤ø ‡≤∏‡≤Ç‡≤ï‡≥Ä‡≤∞‡≥ç‡≤£ ‡≤∏‡≤Æ‡≤∏‡≥ç‡≤Ø‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤°‡≤ø‡≤¨‡≤ó‡≥ç ‡≤Æ‡≤æ‡≤°‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å**  
- **‡≤®‡≥à‡≤ú ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤¶‡≥É‡≤∂‡≥ç‡≤Ø‡≤™‡≤ü‡≤ó‡≤≥‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤≤‡≥ã‡≤°‡≥ç ‡≤Ö‡≤°‡≤ø‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≤æ‡≤®‡≥ç‡≤Ø‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å**  
- **‡≤™‡≥ç‡≤∞‡≤≠‡≤æ‡≤µ‡≤∂‡≥Ä‡≤≤ ‡≤é‡≤ö‡≥ç‡≤ö‡≤∞‡≤ø‡≤ï‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤Ö‡≤µ‡≤≤‡≥ã‡≤ï‡≤®‡≤¶‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≤æ ‡≤µ‡≥ç‡≤Ø‡≤µ‡≤∏‡≥ç‡≤•‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤ø‡≤∏‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å**  
- **‡≤®‡≤ø‡≤∞‡≤Ç‡≤§‡≤∞ ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü‡≤ó‡≤æ‡≤ó‡≤ø ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤™‡≥ç‡≤∞‡≤µ‡≤æ‡≤π‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥ç‡≤µ‡≤Ø‡≤Ç‡≤ö‡≤æ‡≤≤‡≤ø‡≤§‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å**

## üß™ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤µ‡≤æ‡≤∏‡≥ç‡≤§‡≥Å‡≤∂‡≤ø‡≤≤‡≥ç‡≤™

### ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤§‡≤Ç‡≤§‡≥ç‡≤∞ ‡≤Ö‡≤µ‡≤≤‡≥ã‡≤ï‡≤®

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Unit Tests                       ‚îÇ
‚îÇ   ‚Ä¢ Tool execution logic                       ‚îÇ
‚îÇ   ‚Ä¢ Database query validation                  ‚îÇ
‚îÇ   ‚Ä¢ Authentication/authorization               ‚îÇ
‚îÇ   ‚Ä¢ Embedding generation                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             Integration Tests                   ‚îÇ
‚îÇ   ‚Ä¢ End-to-end MCP workflows                  ‚îÇ
‚îÇ   ‚Ä¢ Database schema validation                 ‚îÇ
‚îÇ   ‚Ä¢ API endpoint testing                       ‚îÇ
‚îÇ   ‚Ä¢ Multi-tool interactions                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Performance Tests                    ‚îÇ
‚îÇ   ‚Ä¢ Load testing under realistic conditions    ‚îÇ
‚îÇ   ‚Ä¢ Database performance validation            ‚îÇ
‚îÇ   ‚Ä¢ Memory and resource usage                  ‚îÇ
‚îÇ   ‚Ä¢ Embedding generation performance           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              E2E Tests                         ‚îÇ
‚îÇ   ‚Ä¢ Complete user workflows                    ‚îÇ
‚îÇ   ‚Ä¢ VS Code integration testing               ‚îÇ
‚îÇ   ‚Ä¢ Real-world scenario validation            ‚îÇ
‚îÇ   ‚Ä¢ Cross-browser compatibility               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤™‡≤∞‡≤ø‡≤∏‡≤∞ ‡≤∏‡≥ç‡≤•‡≤æ‡≤™‡≤®‡≥Ü

```python
# tests/conftest.py
"""
Pytest configuration and shared fixtures for MCP server testing.
"""
import pytest
import asyncio
import asyncpg
import os
from typing import AsyncGenerator, Dict, Any
from unittest.mock import AsyncMock, Mock
import tempfile
import shutil
from datetime import datetime

# ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü
TEST_DATABASE_URL = "postgresql://test_user:test_pass@localhost:5432/test_retail_db"
TEST_STORE_IDS = ['test_seattle', 'test_redmond', 'test_bellevue']

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="session")
async def test_database():
    """Set up test database with schema and sample data."""
    
    # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
    sys_conn = await asyncpg.connect(
        "postgresql://postgres:password@localhost:5432/postgres"
    )
    
    try:
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        await sys_conn.execute("DROP DATABASE IF EXISTS test_retail_db")
        await sys_conn.execute("CREATE DATABASE test_retail_db")
    finally:
        await sys_conn.close()
    
    # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç‚Äå‡≤ó‡≥Ü ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï‡≤ø‡≤∏‡≤ø ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤∏‡≥ç‡≤ï‡≥Ä‡≤Æ‡≤æ ‡≤∏‡≤ø‡≤¶‡≥ç‡≤ß‡≤™‡≤°‡≤ø‡≤∏‡≤ø
    test_conn = await asyncpg.connect(TEST_DATABASE_URL)
    
    try:
        # ‡≤∏‡≥ç‡≤ï‡≥Ä‡≤Æ‡≤æ ‡≤≤‡≥ã‡≤°‡≥ç ‡≤Æ‡≤æ‡≤°‡≤ø
        schema_sql = await load_sql_file("../scripts/create_schema.sql")
        await test_conn.execute(schema_sql)
        
        # ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø ‡≤°‡≥á‡≤ü‡≤æ ‡≤≤‡≥ã‡≤°‡≥ç ‡≤Æ‡≤æ‡≤°‡≤ø
        sample_data_sql = await load_sql_file("../scripts/sample_data.sql")
        await test_conn.execute(sample_data_sql)
        
        yield test_conn
        
    finally:
        await test_conn.close()
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥ç‡≤µ‡≤ö‡≥ç‡≤õ‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤ø
        sys_conn = await asyncpg.connect(
            "postgresql://postgres:password@localhost:5432/postgres"
        )
        try:
            await sys_conn.execute("DROP DATABASE IF EXISTS test_retail_db")
        finally:
            await sys_conn.close()

@pytest.fixture
async def db_connection(test_database):
    """Provide a clean database connection for each test."""
    
    conn = await asyncpg.connect(TEST_DATABASE_URL)
    
    # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤µ‡≤ø‡≤≠‡≤ú‡≤®‡≥Ü‡≤ó‡≤æ‡≤ó‡≤ø ‡≤µ‡≥ç‡≤Ø‡≤µ‡≤π‡≤æ‡≤∞ ‡≤™‡≥ç‡≤∞‡≤æ‡≤∞‡≤Ç‡≤≠‡≤ø‡≤∏‡≤ø
    tx = conn.transaction()
    await tx.start()
    
    try:
        yield conn
    finally:
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤µ‡≤ø‡≤≠‡≤ú‡≤®‡≥Ü ‡≤ï‡≤æ‡≤Ø‡≥ç‡≤¶‡≥Å‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≤≤‡≥Å ‡≤µ‡≥ç‡≤Ø‡≤µ‡≤π‡≤æ‡≤∞‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤¶‡≥ç‡≤¶‡≥Å‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤ø
        await tx.rollback()
        await conn.close()

@pytest.fixture
async def mock_embedding_manager():
    """Mock embedding manager for testing without Azure OpenAI calls."""
    
    mock_manager = AsyncMock()
    
    # ‡≤é‡§Æ‡•ç‡§¨‡≥Ü‡≤°‡≥ç‡≤°‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ï‡≤≤‡≤ø ‡≤Æ‡≤æ‡≤°‡≤ø
    mock_manager.generate_embedding.return_value = [0.1] * 1536  # ‡≤é‡§Æ‡•ç‡§¨‡≥Ü‡≤°‡≥ç‡≤°‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤®‡≤ï‡≤≤‡≤ø ‡≤Æ‡≤æ‡≤°‡≤ø
    mock_manager.generate_embeddings_batch.return_value = [[0.1] * 1536] * 10
    
    # ‡≤™‡≥ç‡≤∞‡≤æ‡≤∞‡≤Ç‡≤≠‡≤ø‡≤ï‡≤∞‡≤£ ‡≤®‡≤ï‡≤≤‡≤ø ‡≤Æ‡≤æ‡≤°‡≤ø
    mock_manager.initialize.return_value = None
    mock_manager.cleanup.return_value = None
    
    return mock_manager

@pytest.fixture
async def test_mcp_server(db_connection, mock_embedding_manager):
    """Set up test MCP server instance."""
    
    from mcp_server.server import MCPServer
    from mcp_server.database import DatabaseProvider
    from mcp_server.config import Config
    
    # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
    config = Config()
    config.database.connection_string = TEST_DATABASE_URL
    config.server.enable_debug = True
    
    # ‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç ‡≤™‡≥Ç‡≤∞‡≥à‡≤ï‡≥Ü‡≤¶‡≤æ‡≤∞ ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
    db_provider = DatabaseProvider(config.database.connection_string)
    await db_provider.initialize()
    
    # MCP ‡≤∏‡≤∞‡≥ç‡≤µ‡≤∞‡≥ç ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
    server = MCPServer(config, db_provider)
    server.embedding_manager = mock_embedding_manager
    
    await server.initialize()
    
    yield server
    
    await server.cleanup()

@pytest.fixture
def sample_products():
    """Sample product data for testing."""
    
    return [
        {
            'product_id': 'test-product-1',
            'product_name': 'Test Running Shoes',
            'brand': 'TestBrand',
            'price': 99.99,
            'product_description': 'Comfortable running shoes for daily training',
            'category_name': 'Electronics',
            'current_stock': 50
        },
        {
            'product_id': 'test-product-2',
            'product_name': 'Test Laptop',
            'brand': 'TestTech',
            'price': 1299.99,
            'product_description': 'High-performance laptop for professional use',
            'category_name': 'Electronics',
            'current_stock': 25
        }
    ]

async def load_sql_file(file_path: str) -> str:
    """Load SQL file content."""
    
    with open(file_path, 'r') as file:
        return file.read()

# ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤°‡≥á‡≤ü‡≤æ ‡≤∏‡≤π‡≤æ‡≤Ø‡≤ï‡≤∞‡≥Å
class TestDataHelper:
    """Helper class for managing test data."""
    
    @staticmethod
    async def create_test_store(conn: asyncpg.Connection, store_id: str) -> Dict[str, Any]:
        """Create a test store."""
        
        store_data = {
            'store_id': store_id,
            'store_name': f'Test Store {store_id}',
            'store_location': 'Test Location',
            'store_type': 'test',
            'region': 'test'
        }
        
        await conn.execute("""
            INSERT INTO retail.stores (store_id, store_name, store_location, store_type, region)
            VALUES ($1, $2, $3, $4, $5)
            ON CONFLICT (store_id) DO NOTHING
        """, *store_data.values())
        
        return store_data
    
    @staticmethod
    async def create_test_customer(conn: asyncpg.Connection, store_id: str) -> str:
        """Create a test customer."""
        
        customer_id = await conn.fetchval("""
            INSERT INTO retail.customers (
                store_id, first_name, last_name, email, loyalty_tier
            ) VALUES ($1, $2, $3, $4, $5)
            RETURNING customer_id
        """, store_id, 'Test', 'Customer', 'test@example.com', 'bronze')
        
        return customer_id
    
    @staticmethod
    async def create_test_product(
        conn: asyncpg.Connection, 
        store_id: str, 
        product_data: Dict[str, Any]
    ) -> str:
        """Create a test product."""
        
        product_id = await conn.fetchval("""
            INSERT INTO retail.products (
                store_id, sku, product_name, brand, price, product_description, current_stock
            ) VALUES ($1, $2, $3, $4, $5, $6, $7)
            RETURNING product_id
        """, 
            store_id, 
            f"TEST-{product_data['product_name'][:10]}",
            product_data['product_name'],
            product_data['brand'],
            product_data['price'],
            product_data['product_description'],
            product_data['current_stock']
        )
        
        return product_id
```

## üîß ‡≤ò‡≤ü‡≤ï ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü

### ‡≤â‡≤™‡≤ï‡≤∞‡≤£ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤´‡≥ç‡≤∞‡≥á‡≤Æ‡≥ç‡≤µ‡≤∞‡≥ç‡≤ï‡≥ç

```python
# tests/test_tools.py
"""
Comprehensive unit tests for MCP tools.
"""
import pytest
import asyncio
from unittest.mock import AsyncMock, patch
from datetime import datetime, timedelta

from mcp_server.tools.sales_analysis import SalesAnalysisTool
from mcp_server.tools.semantic_search import SemanticProductSearchTool
from mcp_server.tools.schema_introspection import SchemaIntrospectionTool
from tests.conftest import TestDataHelper

class TestSalesAnalysisTool:
    """Test sales analysis tool functionality."""
    
    @pytest.fixture
    async def sales_tool(self, test_mcp_server):
        """Create sales analysis tool for testing."""
        return SalesAnalysisTool(test_mcp_server.db_provider)
    
    async def test_daily_sales_query(self, sales_tool, db_connection):
        """Test daily sales analysis query."""
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤°‡≥á‡≤ü‡≤æ‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≤ú‡≥ç‡≤ú‡≥Å‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤ø
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        customer_id = await TestDataHelper.create_test_customer(db_connection, store_id)
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤µ‡≥ç‡≤Ø‡≤µ‡≤π‡≤æ‡≤∞‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        await db_connection.execute("""
            INSERT INTO retail.sales_transactions (
                store_id, customer_id, transaction_date, total_amount, transaction_type
            ) VALUES ($1, $2, $3, $4, $5)
        """, store_id, customer_id, datetime.now(), 150.00, 'sale')
        
        # ‡≤∏‡≤æ‡≤ß‡≤®‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ó‡≤§‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤ø
        result = await sales_tool.execute(
            query_type='daily_sales',
            store_id=store_id,
            start_date=(datetime.now() - timedelta(days=7)).date(),
            end_date=datetime.now().date()
        )
        
        # ‡≤´‡≤≤‡≤ø‡≤§‡≤æ‡≤Ç‡≤∂‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø
        assert result.success is True
        assert len(result.data) > 0
        assert 'total_revenue' in result.data[0]
        assert result.metadata['query_type'] == 'daily_sales'
    
    async def test_custom_query_validation(self, sales_tool, db_connection):
        """Test custom query validation."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤Æ‡≤æ‡≤®‡≥ç‡≤Ø‡≤µ‡≤æ‡≤¶ ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        valid_query = "SELECT COUNT(*) as customer_count FROM retail.customers"
        result = await sales_tool.execute(
            query_type='custom',
            store_id=store_id,
            query=valid_query
        )
        
        assert result.success is True
        
        # ‡≤Ö‡≤Æ‡≤æ‡≤®‡≥ç‡≤Ø‡≤µ‡≤æ‡≤¶ ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø (‡≤®‡≤ø‡≤∞‡≥ã‡≤ß‡≤ø‡≤∏‡≤¨‡≥á‡≤ï‡≥Å)
        invalid_query = "DROP TABLE retail.customers"
        result = await sales_tool.execute(
            query_type='custom',
            store_id=store_id,
            query=invalid_query
        )
        
        assert result.success is False
        assert 'validation failed' in result.error.lower()
    
    async def test_store_isolation(self, sales_tool, db_connection):
        """Test that store isolation works correctly."""
        
        # ‡≤é‡≤∞‡≤°‡≥Å ‡≤µ‡≤ø‡≤≠‡≤ø‡≤®‡≥ç‡≤® ‡≤Ö‡≤Ç‡≤ó‡≤°‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        store1 = 'test_store1'
        store2 = 'test_store2'
        
        await TestDataHelper.create_test_store(db_connection, store1)
        await TestDataHelper.create_test_store(db_connection, store2)
        
        # ‡≤µ‡≤ø‡≤≠‡≤ø‡≤®‡≥ç‡≤® ‡≤Ö‡≤Ç‡≤ó‡≤°‡≤ø‡≤ó‡≤≥‡≤≤‡≥ç‡≤≤‡≤ø ‡≤ó‡≥ç‡≤∞‡≤æ‡≤π‡≤ï‡≤∞‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        customer1 = await TestDataHelper.create_test_customer(db_connection, store1)
        customer2 = await TestDataHelper.create_test_customer(db_connection, store2)
        
        # ‡≤Ö‡≤Ç‡≤ó‡≤°‡≤ø1 ‡≤®‡≤ø‡≤Ç‡≤¶ ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü ‡≤Ö‡≤Ç‡≤ó‡≤°‡≤ø1 ‡≤°‡≥á‡≤ü‡≤æ‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≤æ‡≤§‡≥ç‡≤∞ ‡≤®‡≥ã‡≤°‡≤¨‡≥á‡≤ï‡≥Å
        result1 = await sales_tool.execute(
            query_type='custom',
            store_id=store1,
            query="SELECT COUNT(*) as count FROM retail.customers"
        )
        
        # ‡≤Ö‡≤Ç‡≤ó‡≤°‡≤ø2 ‡≤®‡≤ø‡≤Ç‡≤¶ ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü ‡≤Ö‡≤Ç‡≤ó‡≤°‡≤ø2 ‡≤°‡≥á‡≤ü‡≤æ‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≤æ‡≤§‡≥ç‡≤∞ ‡≤®‡≥ã‡≤°‡≤¨‡≥á‡≤ï‡≥Å
        result2 = await sales_tool.execute(
            query_type='custom',
            store_id=store2,
            query="SELECT COUNT(*) as count FROM retail.customers"
        )
        
        assert result1.success is True
        assert result2.success is True
        assert result1.data[0]['count'] == 1
        assert result2.data[0]['count'] == 1

class TestSemanticSearchTool:
    """Test semantic search tool functionality."""
    
    @pytest.fixture
    async def search_tool(self, test_mcp_server):
        """Create semantic search tool for testing."""
        return SemanticProductSearchTool(test_mcp_server.db_provider)
    
    async def test_semantic_search_execution(self, search_tool, db_connection, sample_products):
        """Test semantic search with mock embeddings."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤â‡≤§‡≥ç‡≤™‡≤®‡≥ç‡≤®‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        for product_data in sample_products:
            product_id = await TestDataHelper.create_test_product(
                db_connection, store_id, product_data
            )
            
            # ‡≤®‡≤ï‡≤≤‡≤ø ‡≤é‡§Æ‡•ç‡§¨‡≥Ü‡≤°‡≥ç‡≤°‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
            await db_connection.execute("""
                INSERT INTO retail.product_embeddings (
                    product_id, store_id, embedding_text, embedding
                ) VALUES ($1, $2, $3, $4)
            """, 
                product_id, store_id, 
                f"{product_data['product_name']} {product_data['brand']}", 
                '[0.1,0.2,0.3]'  # ‡≤®‡≤ï‡≤≤‡≤ø ‡≤é‡§Æ‡•ç‡§¨‡≥Ü‡≤°‡≥ç‡≤°‡≤ø‡≤Ç‡≤ó‡≥ç
            )
        
        # ‡≤π‡≥Å‡≤°‡≥Å‡≤ï‡≤æ‡≤ü‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ó‡≤§‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤ø
        result = await search_tool.execute(
            query='running shoes',
            store_id=store_id,
            limit=10,
            similarity_threshold=0.0
        )
        
        # ‡≤´‡≤≤‡≤ø‡≤§‡≤æ‡≤Ç‡≤∂‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø
        assert result.success is True
        assert len(result.data) > 0
        assert 'similarity_score' in result.data[0]
        assert result.metadata['search_type'] == 'semantic'
    
    async def test_search_parameter_validation(self, search_tool):
        """Test search parameter validation."""
        
        # ‡≤ï‡≤≥‡≥Ü‡≤¶‡≥Å‡≤π‡≥ã‡≤ó‡≤ø‡≤¶ ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        result = await search_tool.execute(store_id='test_store')
        assert result.success is False
        assert 'query is required' in result.error.lower()
        
        # ‡≤ï‡≤≥‡≥Ü‡≤¶‡≥Å‡≤π‡≥ã‡≤ó‡≤ø‡≤¶ store_id ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        result = await search_tool.execute(query='test query')
        assert result.success is False
        assert 'store_id is required' in result.error.lower()

class TestSchemaIntrospectionTool:
    """Test schema introspection tool."""
    
    @pytest.fixture
    async def schema_tool(self, test_mcp_server):
        """Create schema introspection tool for testing."""
        return SchemaIntrospectionTool(test_mcp_server.db_provider)
    
    async def test_single_table_schema(self, schema_tool, db_connection):
        """Test getting schema for a single table."""
        
        result = await schema_tool.execute(
            table_name='customers',
            include_constraints=True,
            include_indexes=True
        )
        
        assert result.success is True
        assert result.data['table_name'] == 'customers'
        assert len(result.data['columns']) > 0
        assert 'customer_id' in [col['column_name'] for col in result.data['columns']]
    
    async def test_all_tables_schema(self, schema_tool, db_connection):
        """Test getting schema for all tables."""
        
        result = await schema_tool.execute()
        
        assert result.success is True
        assert result.data['schema_name'] == 'retail'
        assert len(result.data['tables']) > 0
        
        table_names = [table['table_name'] for table in result.data['tables']]
        expected_tables = ['customers', 'products', 'sales_transactions']
        
        for expected_table in expected_tables:
            assert expected_table in table_names
```

### ‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü

```python
# tests/test_database.py
"""
Database layer testing including RLS and security.
"""
import pytest
import asyncpg
from datetime import datetime

from mcp_server.database import DatabaseProvider
from tests.conftest import TestDataHelper

class TestRowLevelSecurity:
    """Test Row Level Security implementation."""
    
    async def test_store_context_setting(self, db_connection):
        """Test that store context is set correctly."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤∏‡≥ç‡≤ü‡≥ã‡≤∞‡≥ç ‡≤∏‡≤Ç‡≤¶‡≤∞‡≥ç‡≤≠‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥Ü‡≤ü‡≥ç ‡≤Æ‡≤æ‡≤°‡≤ø
        await db_connection.execute("SELECT retail.set_store_context($1)", store_id)
        
        # ‡≤∏‡≤Ç‡≤¶‡≤∞‡≥ç‡≤≠‡≤µ‡≥Å ‡≤∏‡≥Ü‡≤ü‡≥ç ‡≤Ü‡≤ó‡≤ø‡≤¶‡≥Ü‡≤Ø‡≥á ‡≤é‡≤Ç‡≤¶‡≥Å ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø
        current_store = await db_connection.fetchval(
            "SELECT current_setting('app.current_store_id', true)"
        )
        
        assert current_store == store_id
    
    async def test_customer_isolation(self, db_connection):
        """Test that customers are properly isolated by store."""
        
        # ‡≤é‡≤∞‡≤°‡≥Å ‡≤∏‡≥ç‡≤ü‡≥ã‡≤∞‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        store1 = 'test_store1'
        store2 = 'test_store2'
        
        await TestDataHelper.create_test_store(db_connection, store1)
        await TestDataHelper.create_test_store(db_connection, store2)
        
        # ‡≤µ‡≤ø‡≤≠‡≤ø‡≤®‡≥ç‡≤® ‡≤∏‡≥ç‡≤ü‡≥ã‡≤∞‡≥ç‚Äå‡≤ó‡≤≥‡≤≤‡≥ç‡≤≤‡≤ø ‡≤ó‡≥ç‡≤∞‡≤æ‡≤π‡≤ï‡≤∞‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        await TestDataHelper.create_test_customer(db_connection, store1)
        await TestDataHelper.create_test_customer(db_connection, store2)
        
        # ‡≤∏‡≤Ç‡≤¶‡≤∞‡≥ç‡≤≠‡≤µ‡≤®‡≥ç‡≤®‡≥Å store1 ‡≤ó‡≥Ü ‡≤∏‡≥Ü‡≤ü‡≥ç ‡≤Æ‡≤æ‡≤°‡≤ø ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤ó‡≥ç‡≤∞‡≤æ‡≤π‡≤ï‡≤∞‡≤®‡≥ç‡≤®‡≥Å ‡≤é‡≤£‡≤ø‡≤∏‡≤ø
        await db_connection.execute("SELECT retail.set_store_context($1)", store1)
        store1_count = await db_connection.fetchval("SELECT COUNT(*) FROM retail.customers")
        
        # ‡≤∏‡≤Ç‡≤¶‡≤∞‡≥ç‡≤≠‡≤µ‡≤®‡≥ç‡≤®‡≥Å store2 ‡≤ó‡≥Ü ‡≤∏‡≥Ü‡≤ü‡≥ç ‡≤Æ‡≤æ‡≤°‡≤ø ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤ó‡≥ç‡≤∞‡≤æ‡≤π‡≤ï‡≤∞‡≤®‡≥ç‡≤®‡≥Å ‡≤é‡≤£‡≤ø‡≤∏‡≤ø
        await db_connection.execute("SELECT retail.set_store_context($1)", store2)
        store2_count = await db_connection.fetchval("SELECT COUNT(*) FROM retail.customers")
        
        # ‡≤™‡≥ç‡≤∞‡≤§‡≤ø ‡≤∏‡≥ç‡≤ü‡≥ã‡≤∞‡≥ç ‡≤§‡≤®‡≥ç‡≤®‡≤¶‡≥á ‡≤ó‡≥ç‡≤∞‡≤æ‡≤π‡≤ï‡≤∞‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≤æ‡≤§‡≥ç‡≤∞ ‡≤®‡≥ã‡≤°‡≤¨‡≥á‡≤ï‡≥Å
        assert store1_count == 1
        assert store2_count == 1
    
    async def test_invalid_store_context(self, db_connection):
        """Test that invalid store context raises error."""
        
        with pytest.raises(Exception) as exc_info:
            await db_connection.execute("SELECT retail.set_store_context($1)", 'invalid_store')
        
        assert "Store not found" in str(exc_info.value)
    
    async def test_cross_store_data_insertion_blocked(self, db_connection):
        """Test that users cannot insert data for other stores."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤∏‡≥ç‡≤ü‡≥ã‡≤∞‡≥ç ‡≤∏‡≤Ç‡≤¶‡≤∞‡≥ç‡≤≠‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥Ü‡≤ü‡≥ç ‡≤Æ‡≤æ‡≤°‡≤ø
        await db_connection.execute("SELECT retail.set_store_context($1)", store_id)
        
        # ‡≤µ‡≤ø‡≤≠‡≤ø‡≤®‡≥ç‡≤® ‡≤∏‡≥ç‡≤ü‡≥ã‡≤∞‡≥ç‚Äå‡≤ó‡≥Ü ‡≤ó‡≥ç‡≤∞‡≤æ‡≤π‡≤ï‡≤∞‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥á‡≤∞‡≤ø‡≤∏‡≤≤‡≥Å ‡≤™‡≥ç‡≤∞‡≤Ø‡≤§‡≥ç‡≤®‡≤ø‡≤∏‡≤ø (‡≤µ‡≤ø‡≤´‡≤≤‡≤µ‡≤æ‡≤ó‡≤¨‡≥á‡≤ï‡≥Å)
        with pytest.raises(Exception):
            await db_connection.execute("""
                INSERT INTO retail.customers (store_id, first_name, last_name, email)
                VALUES ($1, $2, $3, $4)
            """, 'different_store', 'Test', 'Customer', 'test@example.com')

class TestDatabaseProvider:
    """Test database provider functionality."""
    
    @pytest.fixture
    async def db_provider(self):
        """Create database provider for testing."""
        
        provider = DatabaseProvider(TEST_DATABASE_URL)
        await provider.initialize()
        yield provider
        await provider.cleanup()
    
    async def test_connection_pooling(self, db_provider):
        """Test connection pool functionality."""
        
        # ‡≤¨‡≤π‡≥Å ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤°‡≥Ü‡≤Ø‡≤ø‡≤∞‡≤ø
        conn1 = await db_provider.get_connection()
        conn2 = await db_provider.get_connection()
        
        assert conn1 is not None
        assert conn2 is not None
        assert conn1 != conn2  # ‡≤µ‡≤ø‡≤≠‡≤ø‡≤®‡≥ç‡≤® ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï ‡≤µ‡≤∏‡≥ç‡≤§‡≥Å‡≤ó‡≤≥‡≤æ‡≤ó‡≤ø‡≤∞‡≤¨‡≥á‡≤ï‡≥Å
        
        # ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤¨‡≤ø‡≤°‡≥Å‡≤ó‡≤°‡≥Ü ‡≤Æ‡≤æ‡≤°‡≤ø
        await db_provider.release_connection(conn1)
        await db_provider.release_connection(conn2)
    
    async def test_health_check(self, db_provider):
        """Test database health check."""
        
        health_status = await db_provider.health_check()
        
        assert health_status['status'] == 'healthy'
        assert 'connection_pool_size' in health_status
        assert 'database_version' in health_status
    
    async def test_connection_recovery(self, db_provider):
        """Test connection recovery after database issues."""
        
        # ‡≤á‡≤¶‡≥Å ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï ‡≤™‡≥Å‡≤®‡≤∞‡≥Å‡≤¶‡≥ç‡≤ß‡≤æ‡≤∞ ‡≤™‡≤∞‡≤ø‡≤∏‡≥ç‡≤•‡≤ø‡≤§‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü
        # ‡≤®‡≤ø‡≤ú‡≤µ‡≤æ‡≤¶ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø, ‡≤®‡≥Ä‡≤µ‡≥Å ‡≤§‡≤æ‡≤§‡≥ç‡≤ï‡≤æ‡≤≤‡≤ø‡≤ï‡≤µ‡≤æ‡≤ó‡≤ø ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≥Å‡≤∞‡≤ø‡≤Ø‡≤¨‡≤π‡≥Å‡≤¶‡≥Å
        # ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤™‡≥Ç‡≤≤‡≥ç ‡≤™‡≥Å‡≤®‡≤∞‡≥Å‡≤¶‡≥ç‡≤ß‡≤æ‡≤∞‡≤µ‡≤æ‡≤ó‡≥Å‡≤µ‡≥Å‡≤¶‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø
        
        # ‡≤à‡≤ó‡≤ø‡≤ó‡≥Ü, ‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤®‡≥Ü ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü ‡≤é‡≤Ç‡≤¶‡≥Å ‡≤Æ‡≤æ‡≤§‡≥ç‡≤∞ ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø
        health_status = await db_provider.health_check()
        assert health_status['status'] == 'healthy'
```

## üöÄ ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü

### ‡≤Ö‡≤Ç‡≤§‡≥ç‡≤Ø‡≤¶‡≤ø‡≤Ç‡≤¶ ‡≤Ö‡≤Ç‡≤§‡≥ç‡≤Ø‡≤µ‡≤∞‡≥Ü‡≤ó‡≥Ü ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤™‡≥ç‡≤∞‡≤µ‡≤æ‡≤π ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü

```python
# tests/test_integration.py
"""
Integration tests for complete MCP workflows.
"""
import pytest
import json
from datetime import datetime, timedelta

from mcp_server.server import MCPServer
from tests.conftest import TestDataHelper

class TestMCPWorkflows:
    """Test complete MCP server workflows."""
    
    async def test_product_search_workflow(self, test_mcp_server, db_connection, sample_products):
        """Test complete product search workflow."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤é‡§Æ‡•ç‡§¨‡≥Ü‡≤°‡≥ç‡≤°‡≤ø‡≤Ç‡≤ó‡≥ç‚Äå‡≤ó‡≤≥‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤â‡≤§‡≥ç‡≤™‡≤®‡≥ç‡≤®‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        for product_data in sample_products:
            product_id = await TestDataHelper.create_test_product(
                db_connection, store_id, product_data
            )
            
            # ‡≤â‡≤§‡≥ç‡≤™‡≤®‡≥ç‡≤®‡≤ï‡≥ç‡≤ï‡≥Ü ‡≤é‡§Æ‡•ç‡§¨‡≥Ü‡≤°‡≥ç‡≤°‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
            await db_connection.execute("""
                INSERT INTO retail.product_embeddings (
                    product_id, store_id, embedding_text, embedding
                ) VALUES ($1, $2, $3, $4)
            """, 
                product_id, store_id, 
                f"{product_data['product_name']} {product_data['brand']}", 
                '[' + ','.join(['0.1'] * 1536) + ']'  # ‡≤®‡≤ï‡≤≤‡≤ø ‡≤é‡§Æ‡•ç‡§¨‡≥Ü‡≤°‡≥ç‡≤°‡≤ø‡≤Ç‡≤ó‡≥ç
            )
        
        # ‡≤Ö‡≤∞‡≥ç‡≤•‡≤™‡≥Ç‡≤∞‡≥ç‡≤£ ‡≤π‡≥Å‡≤°‡≥Å‡≤ï‡≤æ‡≤ü‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        search_result = await test_mcp_server.execute_tool(
            'semantic_search_products',
            {
                'query': 'running shoes',
                'store_id': store_id,
                'limit': 10
            }
        )
        
        assert search_result['success'] is True
        assert len(search_result['data']) > 0
        
        # ‡≤∏‡≥ç‡≤ï‡≥Ä‡≤Æ‡≤æ ‡≤á‡≤Ç‡≤ü‡≥ç‡≤∞‡≥ã‡≤∏‡≥ç‡≤™‡≥Ü‡≤ï‡≥ç‡≤∑‡≤®‡≥ç ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        schema_result = await test_mcp_server.execute_tool(
            'get_table_schema',
            {'table_name': 'products'}
        )
        
        assert schema_result['success'] is True
        assert schema_result['data']['table_name'] == 'products'
    
    async def test_sales_analysis_workflow(self, test_mcp_server, db_connection):
        """Test sales analysis workflow."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤ó‡≥ç‡≤∞‡≤æ‡≤π‡≤ï ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤â‡≤§‡≥ç‡≤™‡≤®‡≥ç‡≤®‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        customer_id = await TestDataHelper.create_test_customer(db_connection, store_id)
        product_id = await TestDataHelper.create_test_product(
            db_connection, store_id, {
                'product_name': 'Test Product',
                'brand': 'TestBrand',
                'price': 99.99,
                'product_description': 'Test product description',
                'current_stock': 50
            }
        )
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤µ‡≥ç‡≤Ø‡≤µ‡≤π‡≤æ‡≤∞‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        transaction_id = await db_connection.fetchval("""
            INSERT INTO retail.sales_transactions (
                store_id, customer_id, transaction_date, total_amount, 
                subtotal, tax_amount, transaction_type
            ) VALUES ($1, $2, $3, $4, $5, $6, $7)
            RETURNING transaction_id
        """, store_id, customer_id, datetime.now(), 107.99, 99.99, 8.00, 'sale')
        
        # ‡≤µ‡≥ç‡≤Ø‡≤µ‡≤π‡≤æ‡≤∞ ‡≤ê‡≤ü‡≤Ç ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        await db_connection.execute("""
            INSERT INTO retail.sales_transaction_items (
                transaction_id, product_id, quantity, unit_price, total_price
            ) VALUES ($1, $2, $3, $4, $5)
        """, transaction_id, product_id, 1, 99.99, 99.99)
        
        # ‡≤¶‡≥à‡≤®‡≤Ç‡≤¶‡≤ø‡≤® ‡≤Æ‡≤æ‡≤∞‡≤æ‡≤ü ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤£‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        sales_result = await test_mcp_server.execute_tool(
            'execute_sales_query',
            {
                'query_type': 'daily_sales',
                'store_id': store_id,
                'start_date': (datetime.now() - timedelta(days=1)).date().isoformat(),
                'end_date': datetime.now().date().isoformat()
            }
        )
        
        assert sales_result['success'] is True
        assert len(sales_result['data']) > 0
        assert sales_result['data'][0]['total_revenue'] == 107.99
    
    async def test_multi_store_workflow(self, test_mcp_server, db_connection):
        """Test workflows across multiple stores."""
        
        # ‡≤π‡≤≤‡≤µ‡≤æ‡≤∞‡≥Å ‡≤Ö‡≤Ç‡≤ó‡≤°‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        stores = ['test_seattle', 'test_redmond', 'test_bellevue']
        
        for store_id in stores:
            await TestDataHelper.create_test_store(db_connection, store_id)
            
            # ‡≤™‡≥ç‡≤∞‡≤§‡≤ø ‡≤Ö‡≤Ç‡≤ó‡≤°‡≤ø‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø ‡≤ó‡≥ç‡≤∞‡≤æ‡≤π‡≤ï ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
            await TestDataHelper.create_test_customer(db_connection, store_id)
        
        # ‡≤™‡≥ç‡≤∞‡≤§‡≤ø ‡≤Ö‡≤Ç‡≤ó‡≤°‡≤ø ‡≤§‡≤®‡≥ç‡≤®‡≤¶‡≥á ‡≤°‡≥á‡≤ü‡≤æ‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≤æ‡≤§‡≥ç‡≤∞ ‡≤®‡≥ã‡≤°‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü ‡≤é‡≤Ç‡≤¶‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        for store_id in stores:
            schema_result = await test_mcp_server.execute_tool(
                'execute_sales_query',
                {
                    'query_type': 'custom',
                    'store_id': store_id,
                    'query': 'SELECT COUNT(*) as customer_count FROM retail.customers'
                }
            )
            
            assert schema_result['success'] is True
            assert schema_result['data'][0]['customer_count'] == 1

class TestErrorHandling:
    """Test error handling and edge cases."""
    
    async def test_database_connection_failure(self, test_mcp_server):
        """Test behavior when database connection fails."""
        
        # ‡≤Ö‡≤Æ‡≤æ‡≤®‡≥ç‡≤Ø ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤¨‡≤≥‡≤∏‡≤ø ‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç ‡≤µ‡≥à‡≤´‡≤≤‡≥ç‡≤Ø‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤®‡≥Å‡≤ï‡≤∞‡≤ø‡≤∏‡≤ø
        with patch.object(test_mcp_server.db_provider, 'get_connection') as mock_conn:
            mock_conn.side_effect = Exception("Database connection failed")
            
            result = await test_mcp_server.execute_tool(
                'get_table_schema',
                {'table_name': 'customers'}
            )
            
            assert result['success'] is False
            assert 'connection failed' in result['error'].lower()
    
    async def test_invalid_tool_parameters(self, test_mcp_server):
        """Test handling of invalid tool parameters."""
        
        # ‡≤Ö‡≤ó‡≤§‡≥ç‡≤Ø‡≤µ‡≤æ‡≤¶ ‡≤™‡≤∞‡≤ø‡≤Æ‡≤æ‡≤£ ‡≤ï‡≤≥‡≥Ü‡≤¶‡≥Å‡≤π‡≥ã‡≤ó‡≤ø‡≤¶‡≥Ü
        result = await test_mcp_server.execute_tool(
            'semantic_search_products',
            {'query': 'test query'}  # store_id ‡≤ï‡≤≥‡≥Ü‡≤¶‡≥Å‡≤π‡≥ã‡≤ó‡≤ø‡≤¶‡≥Ü
        )
        
        assert result['success'] is False
        assert 'store_id is required' in result['error'].lower()
        
        # ‡≤Ö‡≤Æ‡≤æ‡≤®‡≥ç‡≤Ø ‡≤™‡≤∞‡≤ø‡≤Æ‡≤æ‡≤£ ‡≤™‡≥ç‡≤∞‡≤ï‡≤æ‡≤∞
        result = await test_mcp_server.execute_tool(
            'semantic_search_products',
            {
                'query': 'test query',
                'store_id': 'test_store',
                'limit': 'invalid'  # ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤æ‡≤Ç‡≤ï‡≤µ‡≤æ‡≤ó‡≤ø‡≤∞‡≤¨‡≥á‡≤ï‡≥Å
            }
        )
        
        assert result['success'] is False
    
    async def test_sql_injection_prevention(self, test_mcp_server, db_connection):
        """Test that SQL injection attempts are blocked."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # SQL ‡≤á‡≤Ç‡≤ú‡≥Ü‡≤ï‡≥ç‡≤∑‡≤®‡≥ç ‡≤™‡≥ç‡≤∞‡≤Ø‡≤§‡≥ç‡≤®‡≤ø‡≤∏‡≤ø
        malicious_query = "SELECT * FROM retail.customers; DROP TABLE retail.customers; --"
        
        result = await test_mcp_server.execute_tool(
            'execute_sales_query',
            {
                'query_type': 'custom',
                'store_id': store_id,
                'query': malicious_query
            }
        )
        
        assert result['success'] is False
        assert 'validation failed' in result['error'].lower()
```

## üìä ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü

### ‡≤≤‡≥ã‡≤°‡≥ç ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤´‡≥ç‡≤∞‡≥á‡≤Æ‡≥ç‡≤µ‡≤∞‡≥ç‡≤ï‡≥ç

```python
# tests/test_performance.py
"""
Performance and load testing for MCP server.
"""
import pytest
import asyncio
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any

class TestPerformance:
    """Performance testing for MCP server operations."""
    
    async def test_concurrent_tool_execution(self, test_mcp_server, db_connection):
        """Test performance under concurrent tool execution."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤°‡≥á‡≤ü‡≤æ‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        for i in range(100):
            await TestDataHelper.create_test_customer(db_connection, store_id)
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤¶‡≥É‡≤∂‡≥ç‡≤Ø‡≤æ‡≤µ‡≤≥‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ø‡≤∞‡≥ç‡≤ß‡≤∞‡≤ø‡≤∏‡≤ø
        async def execute_tool_scenario():
            """Execute a tool and measure performance."""
            start_time = time.time()
            
            result = await test_mcp_server.execute_tool(
                'execute_sales_query',
                {
                    'query_type': 'custom',
                    'store_id': store_id,
                    'query': 'SELECT COUNT(*) as count FROM retail.customers'
                }
            )
            
            execution_time = time.time() - start_time
            return {
                'success': result['success'],
                'execution_time': execution_time
            }
        
        # ‡≤∏‡≤Æ‡≤ï‡≤æ‡≤≤‡≥Ä‡≤® ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤æ‡≤ö‡≤∞‡≤£‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤°‡≥Ü‡≤∏‡≤ø
        concurrent_tasks = 20
        tasks = [execute_tool_scenario() for _ in range(concurrent_tasks)]
        
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        
        # ‡≤´‡≤≤‡≤ø‡≤§‡≤æ‡≤Ç‡≤∂‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤ø‡≤∏‡≤ø
        successful_executions = [r for r in results if r['success']]
        execution_times = [r['execution_time'] for r in successful_executions]
        
        assert len(successful_executions) == concurrent_tasks
        assert statistics.mean(execution_times) < 1.0  # ‡≤∏‡≤∞‡≤æ‡≤∏‡≤∞‡≤ø 1 ‡≤∏‡≥Ü‡≤ï‡≥Ü‡≤Ç‡≤°‡≤ø‡≤®‡≥ä‡≤≥‡≤ó‡≥Ü
        assert max(execution_times) < 5.0  # 5 ‡≤∏‡≥Ü‡≤ï‡≥Ü‡≤Ç‡≤°‡≥Å‡≤ó‡≤≥‡≤ø‡≤ó‡≤ø‡≤Ç‡≤§ ‡≤π‡≥Ü‡≤ö‡≥ç‡≤ö‡≥Å ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤æ‡≤ö‡≤∞‡≤£‡≥Ü ‡≤á‡≤≤‡≥ç‡≤≤
        assert total_time < 10.0  # ‡≤é‡≤≤‡≥ç‡≤≤‡≤æ ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤æ‡≤ö‡≤∞‡≤£‡≥Ü‡≤ó‡≤≥‡≥Å 10 ‡≤∏‡≥Ü‡≤ï‡≥Ü‡≤Ç‡≤°‡≥Å‡≤ó‡≤≥‡≥ä‡≤≥‡≤ó‡≥Ü
        
        print(f"Concurrent execution stats:")
        print(f"  Total time: {total_time:.2f}s")
        print(f"  Average execution time: {statistics.mean(execution_times):.3f}s")
        print(f"  Max execution time: {max(execution_times):.3f}s")
        print(f"  Min execution time: {min(execution_times):.3f}s")
    
    async def test_database_query_performance(self, test_mcp_server, db_connection):
        """Test database query performance with large datasets."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤¶‡≥ä‡≤°‡≥ç‡≤° ‡≤°‡≥á‡≤ü‡≤æ‡≤∏‡≥Ü‡≤ü‡≥ç ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        print("Creating test dataset...")
        for i in range(1000):
            await TestDataHelper.create_test_customer(db_connection, store_id)
        
        # ‡≤µ‡≤ø‡≤µ‡≤ø‡≤ß ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≤æ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        query_tests = [
            {
                'name': 'Simple COUNT',
                'query': 'SELECT COUNT(*) FROM retail.customers',
                'expected_max_time': 0.1
            },
            {
                'name': 'Filtered SELECT',
                'query': "SELECT * FROM retail.customers WHERE loyalty_tier = 'bronze' LIMIT 100",
                'expected_max_time': 0.5
            },
            {
                'name': 'Aggregation',
                'query': 'SELECT loyalty_tier, COUNT(*) FROM retail.customers GROUP BY loyalty_tier',
                'expected_max_time': 0.5
            }
        ]
        
        for test_case in query_tests:
            start_time = time.time()
            
            result = await test_mcp_server.execute_tool(
                'execute_sales_query',
                {
                    'query_type': 'custom',
                    'store_id': store_id,
                    'query': test_case['query']
                }
            )
            
            execution_time = time.time() - start_time
            
            assert result['success'] is True
            assert execution_time < test_case['expected_max_time']
            
            print(f"Query '{test_case['name']}': {execution_time:.3f}s")
    
    async def test_embedding_generation_performance(self, test_mcp_server):
        """Test embedding generation performance."""
        
        from mcp_server.embeddings.product_embedder import ProductEmbedder
        
        # ‡≤®‡≤ï‡≤≤‡≤ø ‡≤é‡§Æ‡•ç‡§¨‡≥Ü‡≤°‡≥ç‡≤°‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤Æ‡≥ç‡≤Ø‡≤æ‡≤®‡≥á‡≤ú‡≤∞‡≥ç‚Äå‡≤®‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø (‡≤Ø‡≤•‡≤æ‡≤∞‡≥ç‡≤• API ‡≤ï‡≤∞‡≥Ü‡≤ó‡≤≥‡≤ø‡≤≤‡≥ç‡≤≤)
        embedder = ProductEmbedder(test_mcp_server.db_provider)
        embedder.embedding_manager = test_mcp_server.embedding_manager
        
        # ‡≤¨‡≥ç‡≤Ø‡≤æ‡≤ö‡≥ç ‡≤é‡§Æ‡•ç‡§¨‡≥Ü‡≤°‡≥ç‡≤°‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        test_texts = [f"Test product {i} description" for i in range(100)]
        
        start_time = time.time()
        embeddings = await embedder.embedding_manager.generate_embeddings_batch(test_texts)
        batch_time = time.time() - start_time
        
        assert len(embeddings) == 100
        assert batch_time < 5.0  # ‡≤®‡≤ï‡≤≤‡≤ø‡≤ó‡≤≥‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü 5 ‡≤∏‡≥Ü‡≤ï‡≥Ü‡≤Ç‡≤°‡≥Å‡≤ó‡≤≥‡≥ä‡≤≥‡≤ó‡≥Ü ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤≥‡≥ç‡≤≥‡≤¨‡≥á‡≤ï‡≥Å
        
        print(f"Batch embedding generation (100 items): {batch_time:.3f}s")
        print(f"Average per embedding: {batch_time/100:.4f}s")
    
    @pytest.mark.slow
    async def test_memory_usage(self, test_mcp_server, db_connection):
        """Test memory usage under load."""
        
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤∏‡≤æ‡≤ï‡≤∑‡≥ç‡≤ü‡≥Å ‡≤°‡≥á‡≤ü‡≤æ‡≤∏‡≥Ü‡≤ü‡≥ç ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        for i in range(500):
            await TestDataHelper.create_test_customer(db_connection, store_id)
        
        # ‡≤Ö‡≤®‡≥á‡≤ï ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤æ‡≤ö‡≤∞‡≤£‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤ø‡≤∏‡≤ø
        for i in range(50):
            await test_mcp_server.execute_tool(
                'execute_sales_query',
                {
                    'query_type': 'custom',
                    'store_id': store_id,
                    'query': 'SELECT * FROM retail.customers LIMIT 100'
                }
            )
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # ‡≤Æ‡≥Ü‡≤Æ‡≥ä‡≤∞‡≤ø ‡≤µ‡≥É‡≤¶‡≥ç‡≤ß‡≤ø ‡≤Ø‡≥Å‡≤ï‡≥ç‡≤§‡≤µ‡≤æ‡≤ó‡≤ø‡≤∞‡≤¨‡≥á‡≤ï‡≥Å (‡≤à ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü‡≤ó‡≥Ü 100MB ‡≤ï‡≥ç‡≤ï‡≤ø‡≤Ç‡≤§ ‡≤ï‡≤°‡≤ø‡≤Æ‡≥Ü)
        assert memory_increase < 100
        
        print(f"Memory usage:")
        print(f"  Initial: {initial_memory:.1f} MB")
        print(f"  Final: {final_memory:.1f} MB")
        print(f"  Increase: {memory_increase:.1f} MB")

class TestScalability:
    """Test scalability characteristics."""
    
    async def test_response_time_scaling(self, test_mcp_server, db_connection):
        """Test how response time scales with data size."""
        
        store_id = 'test_seattle'
        await TestDataHelper.create_test_store(db_connection, store_id)
        
        # ‡≤µ‡≤ø‡≤≠‡≤ø‡≤®‡≥ç‡≤® ‡≤°‡≥á‡≤ü‡≤æ ‡≤ó‡≤æ‡≤§‡≥ç‡≤∞‡≤ó‡≤≥‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
        data_sizes = [100, 500, 1000, 2000]
        response_times = []
        
        for size in data_sizes:
            # ‡≤á‡≤§‡≥ç‡≤§‡≥Ä‡≤ö‡≤ø‡≤® ‡≤°‡≥á‡≤ü‡≤æ‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤§‡≥Ü‡≤∞‡≤µ‡≥Å‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤ø
            await db_connection.execute("DELETE FROM retail.customers WHERE store_id = $1", store_id)
            
            # ‡≤®‡≤ø‡≤∞‡≥ç‡≤¶‡≤ø‡≤∑‡≥ç‡≤ü ‡≤ó‡≤æ‡≤§‡≥ç‡≤∞‡≤¶ ‡≤°‡≥á‡≤ü‡≤æ‡≤∏‡≥Ü‡≤ü‡≥ç ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
            for i in range(size):
                await TestDataHelper.create_test_customer(db_connection, store_id)
            
            # ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü ‡≤∏‡≤Æ‡≤Ø‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤≥‡≥Ü‡≤Ø‡≤ø‡≤∞‡≤ø
            start_time = time.time()
            result = await test_mcp_server.execute_tool(
                'execute_sales_query',
                {
                    'query_type': 'custom',
                    'store_id': store_id,
                    'query': 'SELECT COUNT(*) FROM retail.customers'
                }
            )
            execution_time = time.time() - start_time
            
            assert result['success'] is True
            response_times.append(execution_time)
            
            print(f"Data size {size}: {execution_time:.3f}s")
        
        # ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü ‡≤∏‡≤Æ‡≤Ø ‡≤Ø‡≥Å‡≤ï‡≥ç‡≤§‡≤µ‡≤æ‡≤ó‡≤ø ‡≤µ‡≥É‡≤¶‡≥ç‡≤ß‡≤ø‡≤Ø‡≤æ‡≤ó‡≤¨‡≥á‡≤ï‡≥Å (‡≤µ‡≥ç‡≤Ø‡≤µ‡≤∏‡≥ç‡≤•‡≤ø‡≤§‡≤µ‡≤æ‡≤ó‡≤ø ‡≤Ö‡≤≤‡≥ç‡≤≤)
        # ‡≤∏‡≤∞‡≤≥ ‡≤é‡≤£‡≤ø‡≤ï‡≥Ü ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü‡≤ó‡≤≥‡≥Å ‡≤¶‡≥ä‡≤°‡≥ç‡≤° ‡≤°‡≥á‡≤ü‡≤æ‡≤∏‡≥Ü‡≤ü‡≥ç‚Äå‡≤ó‡≤≥‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤ï‡≥Ç‡≤° ‡≤µ‡≥á‡≤ó‡≤µ‡≤æ‡≤ó‡≤ø ‡≤á‡≤∞‡≤¨‡≥á‡≤ï‡≥Å
        for time_val in response_times:
            assert time_val < 1.0  # ‡≤é‡≤≤‡≥ç‡≤≤‡≤æ ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü‡≤ó‡≤≥‡≥Å 1 ‡≤∏‡≥Ü‡≤ï‡≥Ü‡≤Ç‡≤°‡≤ø‡≤®‡≥ä‡≤≥‡≤ó‡≥Ü
```

## üîç ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤â‡≤™‡≤ï‡≤∞‡≤£‡≤ó‡≤≥‡≥Å

### ‡≤Ö‡≤§‡≥ç‡≤Ø‡≤æ‡≤ß‡≥Å‡≤®‡≤ø‡≤ï ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤´‡≥ç‡≤∞‡≥á‡≤Æ‡≥ç‡≤µ‡≤∞‡≥ç‡≤ï‡≥ç

```python
# mcp_server/debugging/debug_tools.py
"""
Advanced debugging tools for MCP server troubleshooting.
"""
import asyncio
import json
import time
import traceback
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)

class MCPDebugger:
    """Comprehensive debugging utilities for MCP server."""
    
    def __init__(self, server_instance):
        self.server = server_instance
        self.debug_logs = []
        self.performance_metrics = {}
        self.active_traces = {}
        
    @asynccontextmanager
    async def trace_execution(self, operation_name: str, context: Dict[str, Any] = None):
        """Trace operation execution with detailed logging."""
        
        trace_id = f"{operation_name}_{int(time.time() * 1000)}"
        start_time = time.time()
        
        trace_info = {
            'trace_id': trace_id,
            'operation': operation_name,
            'start_time': start_time,
            'context': context or {},
            'status': 'running'
        }
        
        self.active_traces[trace_id] = trace_info
        
        logger.debug(f"Starting trace: {trace_id} - {operation_name}")
        
        try:
            yield trace_info
            
            # ‡≤Ø‡≤∂‡≤∏‡≥ç‡≤∏‡≥Å
            execution_time = time.time() - start_time
            trace_info.update({
                'status': 'completed',
                'execution_time': execution_time
            })
            
            logger.debug(f"Completed trace: {trace_id} in {execution_time:.3f}s")
            
        except Exception as e:
            # ‡≤¶‡≥ã‡≤∑
            execution_time = time.time() - start_time
            trace_info.update({
                'status': 'error',
                'execution_time': execution_time,
                'error': str(e),
                'traceback': traceback.format_exc()
            })
            
            logger.error(f"Error in trace: {trace_id} - {str(e)}")
            raise
            
        finally:
            # ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤Ç‡≤° ‡≤ü‡≥ç‡≤∞‡≥á‡≤∏‡≥ç ‡≤∏‡≤Ç‡≤ó‡≥ç‡≤∞‡≤π‡≤ø‡≤∏‡≤ø
            self.debug_logs.append(trace_info.copy())
            del self.active_traces[trace_id]
            
            # ‡≤°‡≤ø‡≤¨‡≤ó‡≥ç ‡≤≤‡≤æ‡≤ó‡≥ç ‡≤ó‡≤æ‡≤§‡≥ç‡≤∞‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≤ø‡≤§‡≤ø‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤ø
            if len(self.debug_logs) > 1000:
                self.debug_logs = self.debug_logs[-500:]
    
    async def debug_tool_execution(self, tool_name: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Debug tool execution with comprehensive logging."""
        
        async with self.trace_execution(f"tool_execution_{tool_name}", {'parameters': parameters}) as trace:
            
            # ‡≤™‡≥Ç‡≤∞‡≥ç‡≤µ-‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≤æ ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤®‡≥Ü
            validation_result = await self._validate_tool_parameters(tool_name, parameters)
            trace['validation'] = validation_result
            
            if not validation_result['valid']:
                return {
                    'success': False,
                    'error': f"Parameter validation failed: {validation_result['errors']}",
                    'debug_info': trace
                }
            
            # ‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤®‡≥Ü
            db_health = await self._check_database_health()
            trace['database_health'] = db_health
            
            # ‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤£‡≥Ü‡≤Ø‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤∏‡≤æ‡≤ß‡≤®‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ó‡≤§‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤ø
            try:
                tool_instance = self.server.get_tool(tool_name)
                if not tool_instance:
                    return {
                        'success': False,
                        'error': f"Tool '{tool_name}' not found",
                        'debug_info': trace
                    }
                
                # ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü‡≤Ø ‡≤∏‡≤Æ‡≤Ø‡≤¶‡≤≤‡≥ç‡≤≤‡≤ø ‡≤∏‡≤Ç‡≤™‡≤®‡≥ç‡≤Æ‡≥Ç‡≤≤ ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤£‡≥Ü ‡≤Æ‡≤æ‡≤°‡≤ø
                start_memory = await self._get_memory_usage()
                
                result = await tool_instance.call(**parameters)
                
                end_memory = await self._get_memory_usage()
                
                trace.update({
                    'memory_start_mb': start_memory,
                    'memory_end_mb': end_memory,
                    'memory_used_mb': end_memory - start_memory,
                    'result_success': result.success,
                    'result_row_count': result.row_count
                })
                
                return {
                    'success': result.success,
                    'data': result.data,
                    'error': result.error,
                    'metadata': result.metadata,
                    'debug_info': trace
                }
                
            except Exception as e:
                trace['exception'] = {
                    'type': type(e).__name__,
                    'message': str(e),
                    'traceback': traceback.format_exc()
                }
                
                return {
                    'success': False,
                    'error': f"Tool execution failed: {str(e)}",
                    'debug_info': trace
                }
    
    async def analyze_performance_bottlenecks(self) -> Dict[str, Any]:
        """Analyze performance bottlenecks from debug logs."""
        
        if not self.debug_logs:
            return {'message': 'No debug data available'}
        
        # ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≤æ ‡≤∏‡≤Æ‡≤Ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤ø‡≤∏‡≤ø
        execution_times = {}
        error_rates = {}
        memory_usage = {}
        
        for log_entry in self.debug_logs[-100:]:  # ‡≤ï‡≥ä‡≤®‡≥Ü‡≤Ø 100 ‡≤¶‡≤æ‡≤ñ‡≤≤‡≥Ü‡≤ó‡≤≥‡≥Å
            operation = log_entry['operation']
            
            # ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≤æ ‡≤∏‡≤Æ‡≤Ø ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤£‡≥Ü
            if 'execution_time' in log_entry:
                if operation not in execution_times:
                    execution_times[operation] = []
                execution_times[operation].append(log_entry['execution_time'])
            
            # ‡≤¶‡≥ã‡≤∑ ‡≤¶‡≤∞ ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤£‡≥Ü
            if operation not in error_rates:
                error_rates[operation] = {'total': 0, 'errors': 0}
            
            error_rates[operation]['total'] += 1
            if log_entry['status'] == 'error':
                error_rates[operation]['errors'] += 1
            
            # ‡≤Æ‡≥Ü‡≤Æ‡≥ä‡≤∞‡≤ø ‡≤¨‡≤≥‡≤ï‡≥Ü ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤£‡≥Ü
            if 'memory_used_mb' in log_entry:
                if operation not in memory_usage:
                    memory_usage[operation] = []
                memory_usage[operation].append(log_entry['memory_used_mb'])
        
        # ‡≤Ö‡≤Ç‡≤ï‡≤ø‡≤Ö‡≤Ç‡≤∂‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤≤‡≥Ü‡≤ï‡≥ç‡≤ï‡≤π‡≤æ‡≤ï‡≤ø
        performance_stats = {}
        
        for operation, times in execution_times.items():
            if times:
                performance_stats[operation] = {
                    'avg_execution_time': sum(times) / len(times),
                    'max_execution_time': max(times),
                    'min_execution_time': min(times),
                    'execution_count': len(times),
                    'error_rate': (error_rates[operation]['errors'] / 
                                 error_rates[operation]['total'] * 100),
                    'avg_memory_usage': (sum(memory_usage.get(operation, [0])) / 
                                       len(memory_usage.get(operation, [1])))
                }
        
        # ‡≤Ö‡≤°‡≥ç‡≤°‡≤ø ‡≤ó‡≥Å‡≤∞‡≥Å‡≤§‡≤ø‡≤∏‡≤ø
        bottlenecks = []
        
        for operation, stats in performance_stats.items():
            if stats['avg_execution_time'] > 2.0:  # ‡≤®‡≤ø‡≤ß‡≤æ‡≤® ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ó‡≤≥‡≥Å
                bottlenecks.append({
                    'type': 'slow_execution',
                    'operation': operation,
                    'avg_time': stats['avg_execution_time']
                })
            
            if stats['error_rate'] > 5.0:  # ‡≤π‡≥Ü‡≤ö‡≥ç‡≤ö‡≤ø‡≤® ‡≤¶‡≥ã‡≤∑ ‡≤¶‡≤∞
                bottlenecks.append({
                    'type': 'high_error_rate',
                    'operation': operation,
                    'error_rate': stats['error_rate']
                })
            
            if stats['avg_memory_usage'] > 100:  # ‡≤π‡≥Ü‡≤ö‡≥ç‡≤ö‡≤ø‡≤® ‡≤Æ‡≥Ü‡≤Æ‡≥ä‡≤∞‡≤ø ‡≤¨‡≤≥‡≤ï‡≥Ü
                bottlenecks.append({
                    'type': 'high_memory_usage',
                    'operation': operation,
                    'memory_mb': stats['avg_memory_usage']
                })
        
        return {
            'performance_stats': performance_stats,
            'bottlenecks': bottlenecks,
            'total_operations': len(self.debug_logs),
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    async def _validate_tool_parameters(self, tool_name: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Validate tool parameters against schema."""
        
        try:
            tool_instance = self.server.get_tool(tool_name)
            if not tool_instance:
                return {
                    'valid': False,
                    'errors': [f"Tool '{tool_name}' not found"]
                }
            
            schema = tool_instance.get_input_schema()
            
            # ‡≤Æ‡≥Ç‡≤≤‡≤≠‡≥Ç‡≤§ ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤®‡≥Ü (‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≥Ü‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø, jsonschema ‡≤ó‡≥ç‡≤∞‡≤Ç‡≤•‡≤æ‡≤≤‡≤Ø‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤¨‡≤≥‡≤∏‡≤ø)
            errors = []
            required_props = schema.get('required', [])
            
            for prop in required_props:
                if prop not in parameters:
                    errors.append(f"Missing required parameter: {prop}")
            
            return {
                'valid': len(errors) == 0,
                'errors': errors,
                'schema': schema
            }
            
        except Exception as e:
            return {
                'valid': False,
                'errors': [f"Validation error: {str(e)}"]
            }
    
    async def _check_database_health(self) -> Dict[str, Any]:
        """Check database health and connectivity."""
        
        try:
            health_status = await self.server.db_provider.health_check()
            return {
                'healthy': health_status.get('status') == 'healthy',
                'details': health_status
            }
        except Exception as e:
            return {
                'healthy': False,
                'error': str(e)
            }
    
    async def _get_memory_usage(self) -> float:
        """Get current memory usage in MB."""
        
        try:
            import psutil
            import os
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024 / 1024
        except:
            return 0.0
    
    def get_debug_summary(self) -> Dict[str, Any]:
        """Get summary of debug information."""
        
        recent_logs = self.debug_logs[-50:] if self.debug_logs else []
        
        return {
            'total_operations': len(self.debug_logs),
            'active_traces': len(self.active_traces),
            'recent_operations': [
                {
                    'operation': log['operation'],
                    'status': log['status'],
                    'execution_time': log.get('execution_time', 0),
                    'timestamp': log.get('start_time', 0)
                }
                for log in recent_logs
            ],
            'current_traces': list(self.active_traces.keys())
        }

# ‡≤®‡≥á‡≤∞ ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤ó‡≥Ü ‡≤°‡≤ø‡≤¨‡≤ó‡≥ç ‡≤∏‡≤æ‡≤ß‡≤®
class DebugTool:
    """Interactive debugging tool for MCP server."""
    
    def __init__(self, server_instance):
        self.debugger = MCPDebugger(server_instance)
    
    async def debug_query(self, query: str, store_id: str) -> Dict[str, Any]:
        """Debug a specific database query."""
        
        return await self.debugger.debug_tool_execution(
            'execute_sales_query',
            {
                'query_type': 'custom',
                'store_id': store_id,
                'query': query
            }
        )
    
    async def debug_search(self, query: str, store_id: str) -> Dict[str, Any]:
        """Debug a semantic search query."""
        
        return await self.debugger.debug_tool_execution(
            'semantic_search_products',
            {
                'query': query,
                'store_id': store_id,
                'limit': 10
            }
        )
    
    async def get_performance_report(self) -> Dict[str, Any]:
        """Get comprehensive performance report."""
        
        return await self.debugger.analyze_performance_bottlenecks()
```

## üéØ ‡≤™‡≥ç‡≤∞‡≤Æ‡≥Å‡≤ñ ‡≤™‡≤æ‡≤†‡≤ó‡≤≥‡≥Å

‡≤à ‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ó‡≤∂‡≤æ‡≤≤‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤ø‡≤¶ ‡≤®‡≤Ç‡≤§‡≤∞, ‡≤®‡≥Ä‡≤µ‡≥Å ‡≤π‡≥ä‡≤Ç‡≤¶‡≤ø‡≤∞‡≤¨‡≥á‡≤ï‡≥Å:

‚úÖ **‡≤∏‡≤Ç‡≤™‡≥Ç‡≤∞‡≥ç‡≤£ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤´‡≥ç‡≤∞‡≥á‡≤Æ‡≥ç‡≤µ‡≤∞‡≥ç‡≤ï‡≥ç**: ‡≤é‡≤≤‡≥ç‡≤≤‡≤æ ‡≤ò‡≤ü‡≤ï‡≤ó‡≤≥‡≥Å, ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü‡≤ó‡≤≥‡≥Å  
‚úÖ **‡≤Ö‡≤§‡≥ç‡≤Ø‡≤æ‡≤ß‡≥Å‡≤®‡≤ø‡≤ï ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤â‡≤™‡≤ï‡≤∞‡≤£‡≤ó‡≤≥‡≥Å**: ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≤æ ‡≤ü‡≥ç‡≤∞‡≥á‡≤∏‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤π‡≥ä‡≤Ç‡≤¶‡≤ø‡≤∞‡≥Å‡≤µ ‡≤∏‡≥Å‡≤ß‡≤æ‡≤∞‡≤ø‡≤§ ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤â‡≤™‡≤ï‡≤∞‡≤£‡≤ó‡≤≥‡≥Å  
‚úÖ **‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤Æ‡≤æ‡≤®‡≥ç‡≤Ø‡≤§‡≥Ü**: ‡≤≤‡≥ã‡≤°‡≥ç ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤µ‡≤ø‡≤∏‡≥ç‡≤§‡≤∞‡≤£‡≤æ ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤£‡≥Ü ‡≤∏‡≤æ‡≤Æ‡≤∞‡≥ç‡≤•‡≥ç‡≤Ø‡≤ó‡≤≥‡≥Å  
‚úÖ **‡≤≠‡≤¶‡≥ç‡≤∞‡≤§‡≤æ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü**: SQL ‡≤á‡≤Ç‡≤ú‡≥Ü‡≤ï‡≥ç‡≤∑‡≤®‡≥ç ‡≤§‡≤°‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å RLS ‡≤Æ‡≤æ‡≤®‡≥ç‡≤Ø‡≤§‡≥Ü  
‚úÖ **‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤£‡≥Ü‡≤Ø ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü**: ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤Ö‡≤Ç‡≤∂‡≤ó‡≤≥‡≥Å ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤¨‡≤æ‡≤ü‡≤≤‡≥ç‚Äå‡≤®‡≥Ü‡≤ï‡≥ç ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤£‡≥Ü  
‚úÖ **CI/CD ‡≤∏‡≤ø‡≤¶‡≥ç‡≤ß‡≤§‡≥Ü**: ‡≤®‡≤ø‡≤∞‡≤Ç‡≤§‡≤∞ ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü‡≤ó‡≤æ‡≤ó‡≤ø ‡≤∏‡≥ç‡≤µ‡≤Ø‡≤Ç‡≤ö‡≤æ‡≤≤‡≤ø‡≤§ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤™‡≥ç‡≤∞‡≤µ‡≤æ‡≤π‡≤ó‡≤≥‡≥Å  

## üöÄ ‡≤Æ‡≥Å‡≤Ç‡≤¶‡≤ø‡≤®‡≤¶‡≥Å ‡≤è‡≤®‡≥Å

**[‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ó‡≤∂‡≤æ‡≤≤‡≥Ü 09: VS ‡≤ï‡≥ã‡≤°‡≥ç ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü](../09-VS-Code/README.md)** ‡≤ú‡≥ä‡≤§‡≥Ü‡≤ó‡≥Ü ‡≤Æ‡≥Å‡≤Ç‡≤¶‡≥Å‡≤µ‡≤∞‡≤ø‡≤Ø‡≤ø‡≤∞‡≤ø:

- MCP ‡≤∏‡≤∞‡≥ç‡≤µ‡≤∞‡≥ç ‡≤Ö‡≤≠‡≤ø‡≤µ‡≥É‡≤¶‡≥ç‡≤ß‡≤ø‡≤ó‡≤æ‡≤ó‡≤ø VS ‡≤ï‡≥ã‡≤°‡≥ç ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤ø‡≤∏‡≥Å‡≤µ‡≥Å‡≤¶‡≥Å  
- VS ‡≤ï‡≥ã‡≤°‡≥ç‚Äå‡≤®‡≤≤‡≥ç‡≤≤‡≤ø ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤™‡≤∞‡≤ø‡≤∏‡≤∞‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥ç‡≤•‡≤æ‡≤™‡≤ø‡≤∏‡≥Å‡≤µ‡≥Å‡≤¶‡≥Å  
- MCP ‡≤∏‡≤∞‡≥ç‡≤µ‡≤∞‡≥ç ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å VS ‡≤ï‡≥ã‡≤°‡≥ç ‡≤ö‡≤æ‡≤ü‡≥ç ‡≤ú‡≥ä‡≤§‡≥Ü‡≤ó‡≥Ü ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤ø‡≤∏‡≥Å‡≤µ‡≥Å‡≤¶‡≥Å  
- ‡≤∏‡≤Ç‡≤™‡≥Ç‡≤∞‡≥ç‡≤£ VS ‡≤ï‡≥ã‡≤°‡≥ç ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤™‡≥ç‡≤∞‡≤µ‡≤æ‡≤π‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≥Å‡≤µ‡≥Å‡≤¶‡≥Å  

## üìö ‡≤π‡≥Ü‡≤ö‡≥ç‡≤ö‡≥Å‡≤µ‡≤∞‡≤ø ‡≤∏‡≤Ç‡≤™‡≤®‡≥ç‡≤Æ‡≥Ç‡≤≤‡≤ó‡≤≥‡≥Å

### ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤´‡≥ç‡≤∞‡≥á‡≤Æ‡≥ç‡≤µ‡≤∞‡≥ç‡≤ï‡≥ç‚Äå‡≤ó‡≤≥‡≥Å
- [pytest ‡≤°‡≤æ‡≤ï‡≥ç‡≤Ø‡≥Å‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥á‡≤∂‡≤®‡≥ç](https://docs.pytest.org/) - ‡≤™‡≥à‡≤•‡≤æ‡≤®‡≥ç ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤´‡≥ç‡≤∞‡≥á‡≤Æ‡≥ç‡≤µ‡≤∞‡≥ç‡≤ï‡≥ç  
- [AsyncPG ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü](https://magicstack.github.io/asyncpg/current/index.html) - ‡≤Ö‡≤∏‡≤ø‡≤Ç‡≤ï‡≥ç ‡≤™‡≥ã‡≤∏‡≥ç‡≤ü‡≥ç‚Äå‡≤ó‡≥ç‡≤∞‡≥ÜSQL ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü  
- [FastAPI ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü](https://fastapi.tiangolo.com/tutorial/testing/) - API ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤æ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å  

### ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü
- [‡≤≤‡≥ã‡≤°‡≥ç ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü‡≤Ø ‡≤â‡≤§‡≥ç‡≤§‡≤Æ ‡≤Ö‡≤≠‡≥ç‡≤Ø‡≤æ‡≤∏‡≤ó‡≤≥‡≥Å](https://docs.python.org/3/library/asyncio.html) - ‡≤Ö‡≤∏‡≤ø‡≤Ç‡≤ï‡≥ç ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü  
- [‡≤°‡≥á‡≤ü‡≤æ‡≤¨‡≥á‡≤∏‡≥ç ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü](https://www.postgresql.org/docs/current/performance-tips.html) - ‡≤™‡≥ã‡≤∏‡≥ç‡≤ü‡≥ç‚Äå‡≤ó‡≥ç‡≤∞‡≥ÜSQL ‡≤Ü‡≤™‡≥ç‡≤ü‡≤ø‡≤Æ‡≥à‡≤ú‡≥Ü‡≤∑‡≤®‡≥ç  
- [‡≤Æ‡≥Ü‡≤Æ‡≥ä‡≤∞‡≤ø ‡≤™‡≥ç‡≤∞‡≥ä‡≤´‡≥à‡≤≤‡≤ø‡≤Ç‡≤ó‡≥ç](https://docs.python.org/3/library/tracemalloc.html) - ‡≤™‡≥à‡≤•‡≤æ‡≤®‡≥ç ‡≤Æ‡≥Ü‡≤Æ‡≥ä‡≤∞‡≤ø ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤£‡≥Ü  

### ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤â‡≤™‡≤ï‡≤∞‡≤£‡≤ó‡≤≥‡≥Å
- [‡≤™‡≥à‡≤•‡≤æ‡≤®‡≥ç ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç](https://docs.python.org/3/library/pdb.html) - ‡≤™‡≥à‡≤•‡≤æ‡≤®‡≥ç ‡≤°‡≤ø‡≤¨‡≤ó‡≤∞‡≥ç  
- [‡≤Ö‡≤∏‡≤ø‡≤Ç‡≤ï‡≥ç ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç](https://docs.python.org/3/library/asyncio-dev.html) - ‡≤Ö‡≤∏‡≤ø‡≤Ç‡≤ï‡≥ç ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç  
- [SQL ‡≤°‡≤ø‡≤¨‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç](https://www.postgresql.org/docs/current/runtime-config-logging.html) - ‡≤™‡≥ã‡≤∏‡≥ç‡≤ü‡≥ç‚Äå‡≤ó‡≥ç‡≤∞‡≥ÜSQL ‡≤≤‡≤æ‡≤ó‡≤ø‡≤Ç‡≤ó‡≥ç  

---

**‡≤π‡≤ø‡≤Ç‡≤¶‡≤ø‡≤®‡≤¶‡≥Å**: [‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ó‡≤∂‡≤æ‡≤≤‡≥Ü 07: ‡≤∏‡≥Ü‡≤Æ‡≥ç‡≤Ø‡≤æ‡≤Ç‡≤ü‡≤ø‡≤ï‡≥ç ‡≤∏‡≤∞‡≥ç‡≤ö‡≥ç ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü](../07-Semantic-Search/README.md)  
**‡≤Æ‡≥Å‡≤Ç‡≤¶‡≤ø‡≤®‡≤¶‡≥Å**: [‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ó‡≤∂‡≤æ‡≤≤‡≥Ü 09: VS ‡≤ï‡≥ã‡≤°‡≥ç ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü](../09-VS-Code/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**‡≤Ö‡≤∏‡≥ç‡≤µ‡≥Ä‡≤ï‡≤∞‡≤£**:  
‡≤à ‡≤¶‡≤∏‡≥ç‡≤§‡≤æ‡≤µ‡≥á‡≤ú‡≥Å AI ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶ ‡≤∏‡≥á‡≤µ‡≥Ü [Co-op Translator](https://github.com/Azure/co-op-translator) ‡≤¨‡≤≥‡≤∏‡≤ø ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶‡≤ø‡≤∏‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü. ‡≤®‡≤æ‡≤µ‡≥Å ‡≤®‡≤ø‡≤ñ‡≤∞‡≤§‡≥Ü‡≤Ø‡≤ø‡≤ó‡≤æ‡≤ó‡≤ø ‡≤™‡≥ç‡≤∞‡≤Ø‡≤§‡≥ç‡≤®‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤ø‡≤¶‡≥ç‡≤¶‡≤∞‡≥Ç, ‡≤∏‡≥ç‡≤µ‡≤Ø‡≤Ç‡≤ö‡≤æ‡≤≤‡≤ø‡≤§ ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶‡≤ó‡≤≥‡≤≤‡≥ç‡≤≤‡≤ø ‡≤¶‡≥ã‡≤∑‡≤ó‡≤≥‡≥Å ‡≤Ö‡≤•‡≤µ‡≤æ ‡≤Ö‡≤∏‡≤§‡≥ç‡≤Ø‡≤§‡≥Ü‡≤ó‡≤≥‡≥Å ‡≤á‡≤∞‡≤¨‡≤π‡≥Å‡≤¶‡≥Å ‡≤é‡≤Ç‡≤¶‡≥Å ‡≤¶‡≤Ø‡≤µ‡≤ø‡≤ü‡≥ç‡≤ü‡≥Å ‡≤ó‡≤Æ‡≤®‡≤ø‡≤∏‡≤ø. ‡≤Æ‡≥Ç‡≤≤ ‡≤≠‡≤æ‡≤∑‡≥Ü‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø‡≤∞‡≥Å‡≤µ ‡≤Æ‡≥Ç‡≤≤ ‡≤¶‡≤∏‡≥ç‡≤§‡≤æ‡≤µ‡≥á‡≤ú‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤ß‡≤ø‡≤ï‡≥É‡≤§ ‡≤Æ‡≥Ç‡≤≤‡≤µ‡≤æ‡≤ó‡≤ø ‡≤™‡≤∞‡≤ø‡≤ó‡≤£‡≤ø‡≤∏‡≤¨‡≥á‡≤ï‡≥Å. ‡≤Æ‡≤π‡≤§‡≥ç‡≤µ‡≤¶ ‡≤Æ‡≤æ‡≤π‡≤ø‡≤§‡≤ø‡≤ó‡≤æ‡≤ó‡≤ø, ‡≤µ‡≥É‡≤§‡≥ç‡≤§‡≤ø‡≤™‡≤∞ ‡≤Æ‡≤æ‡≤®‡≤µ ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∂‡≤ø‡≤´‡≤æ‡≤∞‡≤∏‡≥Å ‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü. ‡≤à ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶ ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤Ø‡≤ø‡≤Ç‡≤¶ ‡≤â‡≤Ç‡≤ü‡≤æ‡≤ó‡≥Å‡≤µ ‡≤Ø‡≤æ‡≤µ‡≥Å‡≤¶‡≥á ‡≤§‡≤™‡≥ç‡≤™‡≥Å ‡≤Ö‡≤∞‡≥ç‡≤•‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≥Ü‡≥Ç‡≤≥‡≥ç‡≤≥‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü ‡≤Ö‡≤•‡≤µ‡≤æ ‡≤§‡≤™‡≥ç‡≤™‡≥Å ‡≤µ‡≤ø‡≤µ‡≤∞‡≤£‡≥Ü‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü ‡≤®‡≤æ‡≤µ‡≥Å ‡≤π‡≥ä‡≤£‡≥Ü‡≤ó‡≤æ‡≤∞‡≤∞‡≤æ‡≤ó‡≥Å‡≤µ‡≥Å‡≤¶‡≤ø‡≤≤‡≥ç‡≤≤.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->